{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests organisés dans le cadre du Projet avec + d'options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A PARTIR D ICI, NOUVELLE SYNTAXE (pool_frequency + pool_frequency_change et modif  filters_double)\n",
    "\n",
    "#Pool frequency permet de définir toutes les combiens de couches on va pool\n",
    "\n",
    "#Pool_frequency_change va modifier la frequence UNE FOIS dans le programme \n",
    "#  se note sous la forme (index_pool_changement, modif) => (2, -1) -> après avoir fait 2 pools, \n",
    "#  on fait pool_frequency = pool_frequency -1 => a la place de (conv2D * 2) + pool, on aura \n",
    "#  jusqu'à la fin du programme (conv2D) + pool\n",
    "\n",
    "#filiters_double, quand il est init à -1, va faire doubler les filters des convs apres CHAQUE pooling\n",
    "\n",
    "#Real Test 4 -> variations des résultats selon le nombre de couches de pooling\n",
    "\n",
    "# CONSTANTES : nb_layers = 8, batch_size = 50, epochs =30, lr = 0.001, momentum = 0.9, optimizer Adam, \n",
    "#  padding = same, maxpool, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0, pas de regularization, MLP 128,\n",
    "#  pool_frequency_change = (0,0)\n",
    "# \n",
    "\n",
    "# * 1 convnet du type (1 conv2D) -> 1 (pool)   (8 pools au total)\n",
    "\n",
    "# * 1 convnet du type (2 conv2D) -> 1 (pool)   (4 pools au total)\n",
    "\n",
    "# * 1 convnet du type (4 conv2D) -> 1 (pool)   (2 pools au total)\n",
    "\n",
    "# * 1 convnet du type (8 conv2D) -> 1 (pool)   (1 pool au total)\n",
    "\n",
    "# * 1 convnet du type (8 conv2D) -> 0 (pool)   (0 pool au total)\n",
    "\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5']\n",
    "list_epochs = [30, 30, 30, 30, 30]\n",
    "list_batch_size = [50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max']\n",
    "list_pool_frequency = [1, 2, 4, 8, 0]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0), (0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 5 -> variations des résultats selon le batch_size\n",
    "\n",
    "# CONSTANTES : nb_layers = 8, epochs = 20, lr = 0.001, momentum = 0.9, optimizer Adam, \n",
    "#  padding = same, maxpool, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0, pas de regularization, MLP 128,\n",
    "#  pool_frequency = 2, pool_frequency_change = (0,0)\n",
    "# \n",
    "\n",
    "# * 1 convnet du type batch_size = 50\n",
    "\n",
    "# * 1 convnet du type batch_size = 100\n",
    "\n",
    "# * 1 convnet du type batch_size = 150\n",
    "\n",
    "# * 1 convnet du type batch_size = 200\n",
    "\n",
    "# * 1 convnet du type batch_size = 500\n",
    "\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5']\n",
    "list_epochs = [20, 20, 20, 20, 20]\n",
    "list_batch_size = [50, 100, 150, 200, 500]\n",
    "list_nb_layers = [8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max']\n",
    "list_pool_frequency = [2, 2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0), (0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 6 -> variations des résultats selon le learning_rate\n",
    "\n",
    "# CONSTANTES : nb_layers = 8, epochs = 30, batch_size=100, momentum = 0.9, optimizer Adam, \n",
    "#  padding = same, maxpool, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0, pas de regularization, MLP 128,\n",
    "#  pool_frequency = 2, pool_frequency_change = (0,0)\n",
    "# \n",
    "#8 tests\n",
    "\n",
    "# * 1 convnet de avec lr à 0.5\n",
    "\n",
    "# * 1 convnet de avec lr à 0.1\n",
    "\n",
    "# * 1 convnet de avec lr à 0.05\n",
    "\n",
    "# * 1 convnet de avec lr à 0.01\n",
    "\n",
    "# * 1 convnet de avec lr à 0.005\n",
    "\n",
    "# * 1 convnet de avec lr à 0.001\n",
    "\n",
    "# * 1 convnet de avec lr à 0.0005\n",
    "\n",
    "# * 1 convnet de avec lr à 0.0001\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6', '7', '8']\n",
    "list_epochs = [30, 30, 30, 30, 30, 30, 30, 30]\n",
    "list_batch_size = [100, 100, 100, 100, 100, 100, 100, 100]\n",
    "list_nb_layers = [8,8,8,8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu', 'relu', 'relu', 'relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max','max','max']\n",
    "list_pool_frequency = [2, 2, 2, 2, 2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0),(0,0),(0,0),(0,0),(0,0)]\n",
    "list_learning_r = [0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests organisés dans le cadre du Projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 1 -> importance des layers (refaire le test 5 mais sans la regularization)\n",
    "# CONSTANTES : filters = 64, batch_size = 50, epochs = 30 (30 car avec beaucoup de layers ça va être long),\n",
    "#lr = 0.001, momentum = 0.9, optimizer Adam, padding = same, maxpool, relu, kernel = (3,3), \n",
    "#avec regu L1L2 à 0 et dropout à 0 (pas de dropout), (sans Batchnorm, sans MLP final, sans filters double)\n",
    "\n",
    "#6 NB layers fixes -> 8, 16, 32, 64, 96, 128\n",
    "\n",
    "# LEXIQUE PARAM : \n",
    "# * filters_double permet de savoir toutes les combien de couche on double les filtres, si 0 on double pas\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6']\n",
    "list_epochs = [, 30, 30, 30, 30, 30]\n",
    "list_batch_size = [50, 50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,16,32,64,96,128]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [0, 0, 0, 0, 0, 0]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max']\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 2 -> importance des filters\n",
    "# CONSTANTES : nb_layers = 8, batch_size = 50, epochs = 50, lr = 0.001, momentum = 0.9, optimizer Adam, \n",
    "#padding = same, maxpool, relu, kernel = (3,3), MLP avec 128 neuronnes, pas de regularization\n",
    "# \n",
    "# * 5 convnets avec filters fixes\n",
    "#   - 1 sans filters double avec filters (16)\n",
    "#   - 1 sans filters double avec filters (32)\n",
    "#   - 1 sans filters double avec filters (64)\n",
    "#   - 1 sans filters double avec filters (96)\n",
    "#   - 1 sans filters double avec filters (128)\n",
    "\n",
    "# * 5 convnets avec filters evolutifs (filter double sur mes blocs de 2 couches de convolution)\n",
    "#   - 1 filters double (2) avec filters (16)  (va faire 16, 32, 64, 128)\n",
    "#   - 1 filters double (2) avec filters (32)  (va faire 32, 64, 128, 256)\n",
    "#   - 1 filters double (4) avec filters (16)  (va faire 16, 16, 32, 32)\n",
    "#   - 1 filters double (4) avec filters (32)  (va faire 32, 32, 64, 64)\n",
    "#   - 1 filters double (4) avec filters (64)  (va faire 64, 64, 128, 128)\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "list_epochs = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
    "list_batch_size = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,8,8,8,8,8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [16, 32, 64, 96, 128, 16, 32, 16, 32, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 2, 2, 4, 4, 4]\n",
    "list_MLP_end = [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max','max','max','max','max']\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 3 -> importance du MLP avant la softmax\n",
    "# CONSTANTES : nb_layers = 8, batch_size = 50, epochs = 100, lr = 0.001, momentum = 0.9, optimizer Adam, \n",
    "#padding = same, maxpool, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0, pas de regularization\n",
    "# \n",
    "\n",
    "# * 1 convent sans MLP\n",
    "\n",
    "# * 5 convnets avec MLP\n",
    "#   - 1 avec (MLP 32)\n",
    "#   - 1 avec (MLP 64)\n",
    "#   - 1 avec (MLP 128)\n",
    "#   - 1 avec (MLP 256)\n",
    "#   - 1 avec (MLP 512)\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6']\n",
    "list_epochs = [100, 100, 100, 100, 100, 100]\n",
    "list_batch_size = [50, 50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [0, 32, 64, 128, 256, 512]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max']\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premiers tests avec résultats cohérents (lr à 0.001, à tester avec autres hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quatrième test -> test 1 avec lr à 0.001 (et sans le bug, pour les bons résultats!)\n",
    "# CONSTANTES : nb_layers = 8, batch_size = 50, epochs 100, lr = 0.001, momentum = 0.9, optimizer Adam, \n",
    "#padding = same, maxpool, relu, kernel = (3,3)\n",
    "# \n",
    "# * 3 convnets sans regularization et MLP à 128\n",
    "#   - 2 filters double(2) avec filters (16, 32) \n",
    "#   - 1 sans filters double avec filters (32)\n",
    "\n",
    "# 2 convnets sans regu ou on test le MLP_end et filters 64\n",
    "#  * 1 sans double, filter 64 avec MLP_end(128)\n",
    "#  * 1 sans double, filter 64 avec MLP_end(0)\n",
    "\n",
    "# 5 convnets avec regu (+ MLP à 128) et filters 32 sans double\n",
    "#  * 1 convnet avec l1 à 0.01\n",
    "#  * 1 convnet avec l1 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec l2 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec L1 et L2 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec L1 et L2 à 0.01 + batchnorm + dropout à 0.2\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "list_epochs = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
    "list_batch_size = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,8,8,8,8,8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0, 0, 0.01, 0.01, 0.01]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2]\n",
    "list_filters_per_layers = [16, 32, 32, 64, 64, 32, 32, 32, 32, 32]\n",
    "list_filters_double = [2, 2, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 0, 128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max','max','max','max','max']\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "main_directory =(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\CONVNETS_\"\n",
    "                 +datetime.datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cinquième test-> Inlfuence des layers (sans bug affi et avec lr à 0.001):\n",
    "# CONSTANTES : filters = 64, batch_size = 50, epochs 30, lr = 0.001, momentum = 0.9, optimizer Adam, \n",
    "#padding = same, maxpool, relu, kernel = (3,3), avec regu L1L2 à 0.01 et dropout à 0.2 \n",
    "#(sans Batchnorm, sans MLP final, sans filters double)\n",
    "\n",
    "\n",
    "#6 NB layers fixes -> 8, 16, 32, 64, 96, 128\n",
    "\n",
    "# LEXIQUE PARAM : \n",
    "# * filters_double permet de savoir toutes les combien de couche on double les filtres, si 0 on double pas\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6']\n",
    "list_epochs = [1, 1, 1, 1, 1, 1]\n",
    "list_batch_size = [50, 50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,16,32,64,96,128]\n",
    "list_l1 = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "list_l2 = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [0, 0, 0, 0, 0, 0]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max']\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A partir d'ici, premiers tests avec lr trop haut (0.01) ?? et problème d'affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Premier Test (avec bug affi et lr trop haut):\n",
    "# CONSTANTES : nb_layers = 8, batch_size = 50, epochs 100, lr = 0.01, momentum = 0.9, optimizer Adam, \n",
    "#padding = same, maxpool, relu, kernel = (3,3)\n",
    "# \n",
    "# * 3 convnets sans regularization et MLP à 128\n",
    "#   - 2 filters double(2) avec filters (16, 32) \n",
    "#   - 1 sans filters double avec filters (32)\n",
    "\n",
    "# 2 convnets sans regu ou on test le MLP_end et filters 64\n",
    "#  * 1 sans double, filter 64 avec MLP_end(128)\n",
    "#  * 1 sans double, filter 64 avec MLP_end(0)\n",
    "\n",
    "# 5 convnets avec regu (+ MLP à 128) et filters 32 sans double\n",
    "#  * 1 convnet avec l1 à 0.01\n",
    "#  * 1 convnet avec l1 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec l2 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec L1 et L2 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec L1 et L2 à 0.01 + batchnorm + dropout à 0.2\n",
    "\n",
    "# LEXIQUE PARAM : \n",
    "# * filters_double permet de savoir toutes les combien de couche on double les filtres, si 0 on double pas\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "list_epochs = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
    "list_batch_size = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,8,8,8,8,8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0, 0, 0.01, 0.01, 0.01]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2]\n",
    "list_filters_per_layers = [16, 32, 32, 64, 64, 32, 32, 32, 32, 32]\n",
    "list_filters_double = [2, 2, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 0, 128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max','max','max','max','max']\n",
    "list_learning_r = [0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "main_directory =(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\CONVNETS_\"\n",
    "                 +datetime.datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(current)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deuxième test (avec bug affi et lr trop haut)-> Inlfuence des layers :\n",
    "# CONSTANTES : filters = 64, batch_size = 50, epochs 100, lr = 0.01, momentum = 0.9, optimizer Adam, \n",
    "#padding = same, maxpool, relu, kernel = (3,3), avec regu L1L2 à 0.01 et dropout à 0.2 \n",
    "#(sans Batchnorm, sans MLP final, sans filters double)\n",
    "\n",
    "\n",
    "#6 NB layers fixes ->8, 16, 32, 64, 96, 128\n",
    "\n",
    "# LEXIQUE PARAM : \n",
    "# * filters_double permet de savoir toutes les combien de couche on double les filtres, si 0 on double pas\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6']\n",
    "list_epochs = [1, 1, 1, 1, 1, 1]\n",
    "list_batch_size = [50, 50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,16,32,64,96,128]\n",
    "list_l1 = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "list_l2 = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [0, 0, 0, 0, 0, 0]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max']\n",
    "list_learning_r = [0.01,0.01,0.01,0.01,0.01,0.01]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Troisième test (progression à taton pour debug) -> Inlfuence des layers sur plus d'epochs et avec PMC 128 :\n",
    "# CONSTANTES : filters = 64, batch_size = 50, epochs 50, lr = 0.01, momentum = 0.9, optimizer Adam, \n",
    "#padding = same, maxpool, relu, kernel = (3,3), avec regu L1L2 à 0.01 et dropout à 0.2 \n",
    "#(sans Batchnorm, sans MLP final, sans filters double)\n",
    "\n",
    "\n",
    "#4 NB layers fixes -> 8, 16, 32, 64\n",
    "\n",
    "# LEXIQUE PARAM : \n",
    "# * filters_double permet de savoir toutes les combien de couche on double les filtres, si 0 on double pas\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4']\n",
    "list_epochs = [50, 50, 50, 50]\n",
    "list_batch_size = [50, 50, 50, 50]\n",
    "list_nb_layers = [8,16,32,64]\n",
    "list_l1 = [0.01, 0.01, 0.01, 0.01]\n",
    "list_l2 = [0.01, 0.01, 0.01, 0.01]\n",
    "list_batch_norm = [0, 0, 0, 0]\n",
    "list_dropout = [0.2, 0.2, 0.2, 0.2]\n",
    "list_filters_per_layers = [64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max']\n",
    "list_learning_r = [0.01,0.01,0.01,0.01]\n",
    "list_momentum = [0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Troisième test BIS (taton debug) -> Ne pas obtenir des résultats inutiles (0.1 en accuracy et 2.304 en loss) en faisant bouger très peu\n",
    "#de paramètres pour essayer de trouver ce qui empêche l'évolution\n",
    "\n",
    "#IDEES : \n",
    "# à cause de l'input shape fxé partout ?\n",
    "# Sans evolution des filters / batchnorm --> résultat dég ?\n",
    "# ...\n",
    "\n",
    "\n",
    "#4 NB layers fixes -> 8 pour l'instant\n",
    "\n",
    "#sous-test 1 => rien de modif pour id 1, batchnorm pour id 2 -> 0.1 vs 0.31\n",
    "#sous-test-2 => rien de modif pour id 1, evolu filters de 16 à 128 (*2 tous les 2 layers) -> pas de changement (pk ?)\n",
    "#sous-test-3 => rien de modif pour id 1, evolu filters de 32 à 256 (*2 tous les 2 layers) -> pas de changement (bizarre)\n",
    "#sous-test-4 => faire le même test que pour le 2eme indivi du \"premier test convnet\" mais avec 50 epochs\n",
    "# differences avec sous-test précédent = pas de l1 et L2, pas de dropout \n",
    "# -> AUCUN CHANGEMENT (PAREIL SI ON REFAIT SUR L ORIGINAL)\n",
    "#sous-test-5 => inspi VGG BLOCK 1 trouve sur net => Ok \n",
    "\n",
    "#IL FALLAIT METTRE EPOCHS - 1 DANS LES VAR QUI EXPLOITENT HISTORY,\n",
    "#REFAIRE DES TESTS\n",
    "\n",
    "#refaire sous-test 4 sans bug avec lr à 0.001 et batch size 64=> on voit tout \n",
    "# de suite une progression, descendre le lr a été très utile => 0.75 val_accuracy !\n",
    "\n",
    "# LEXIQUE PARAM : \n",
    "# * filters_double permet de savoir toutes les combien de couche on double les filtres, si 0 on double pas\n",
    "'''\n",
    "list_indiv_id = ['2']\n",
    "list_epochs = [10]\n",
    "list_batch_size = [64]\n",
    "list_nb_layers = [2]\n",
    "list_l1 = [0]\n",
    "list_l2 = [0]\n",
    "list_batch_norm = [0]\n",
    "list_dropout = [0]\n",
    "list_filters_per_layers = [32]\n",
    "list_filters_double = [0]\n",
    "list_MLP_end = [128]\n",
    "list_activation = ['relu']\n",
    "list_kernel = [(3,3)]\n",
    "list_padding = ['same']\n",
    "list_max_or_avg_pool = ['max']\n",
    "list_learning_r = [0.001]\n",
    "list_momentum = [0.9]\n",
    "list_optimizer = ['SGD']\n",
    "'''\n",
    "\n",
    "'''\n",
    "list_indiv_id = ['2']\n",
    "list_epochs = [50]\n",
    "list_batch_size = [64]\n",
    "list_nb_layers = [8]\n",
    "list_l1 = [0]\n",
    "list_l2 = [0]\n",
    "list_batch_norm = [0]\n",
    "list_dropout = [0]\n",
    "list_filters_per_layers = [32]\n",
    "list_filters_double = [2]\n",
    "list_MLP_end = [128]\n",
    "list_activation = ['relu']\n",
    "list_kernel = [(3,3)]\n",
    "list_padding = ['same']\n",
    "list_max_or_avg_pool = ['max']\n",
    "list_learning_r = [0.001]\n",
    "list_momentum = [0.9]\n",
    "list_optimizer = ['Adam']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
