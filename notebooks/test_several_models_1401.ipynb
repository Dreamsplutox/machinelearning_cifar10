{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7R0yO1XdBNh",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Démarrage de tensorboard et imports principaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rCCbrXXRuvi"
   },
   "outputs": [],
   "source": [
    "# Démarrage de tensorboard pour notebook\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Les imports majeurs, certains ne sont peut être plus utiles -> à re-vérifier \n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import shutil  \n",
    "\n",
    "from tensorflow.python.framework import ops #pour tenter de reset tensorboard, sans grand succès\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIIDw6pHdJbK"
   },
   "source": [
    "### Fonctions pour préparer le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tSU6Hl2Ruv0"
   },
   "outputs": [],
   "source": [
    "# Chargement des datasets de train et de validation + one hot encoding\n",
    "def load_dataset():\n",
    "    # Chargement des données cifar10\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode encoding sur les labels\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "# Normalisation pour accroître la vitesse du modèle (en redimensionnant les pixels)\n",
    "def prep_pixels(train, test):\n",
    "    # Convertion des int en float\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # Normalisation pour avoir des nombres entre 0 et 1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # Retourner les images normalisées\n",
    "    return train_norm, test_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTdXP2D-c2D5"
   },
   "source": [
    "### Classe Python pour définir les individus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jP0CrTpRuv8"
   },
   "outputs": [],
   "source": [
    "# Classe pour les convnets\n",
    "class IndividuConvnets:\n",
    "    def __init__(self, indiv_id='1', epochs=10, nb_layers=10, l1=0, l2=0, dropout=0, filters_per_layers=64, activation='relu', kernel=(3,3), padding='same', max_pool=0):\n",
    "        \n",
    "        # Initialisation de nos directory\n",
    "        #self.main_directory = main_directory\n",
    "        #self.log = main_directory #s'adaptera a\n",
    "        \n",
    "        #Pour l'instant, est utilisé ->  indiv_id (A SUPPRIMER POUR UN DATETIME ?)\n",
    "        #self.indiv_id = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "        # Mettre des conditions pour empêcher les mauvaises saisies si on le désire\n",
    "        #self.log_folder = \"CONVNETS_\" + datetime.datetime.now().strfbtime(\"%Y%m%d-%H%M\")\n",
    "        \n",
    "        if nb_layers <= 2:\n",
    "            nb_layers = 2\n",
    "            \n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.indiv_id = indiv_id\n",
    "        self.epochs = epochs\n",
    "        self.nb_layers = nb_layers\n",
    "\n",
    "        if (l1 == 'false' and l2 == 'false'): #conditon originale -> != --> Pourquoi ?\n",
    "            self.l1 = 0\n",
    "            self.l2 = 0\n",
    "        else:\n",
    "            self.l1 = l1\n",
    "            self.l2 = l2\n",
    "\n",
    "        if(dropout == 'false'):\n",
    "            self.dropout = 0\n",
    "        else:\n",
    "            self.dropout = dropout\n",
    "        self.filters_per_layers = filters_per_layers\n",
    "        self.activation = activation\n",
    "        self.kernel = kernel\n",
    "        self.padding = padding\n",
    "        self.max_pool = max_pool\n",
    "    \n",
    "    # TO STRING (retourne une liste avec tous les attributs utiles de la classe)\n",
    "    def __str__(self):\n",
    "        ma_liste = []\n",
    "        ma_liste.append(\"indiv_id:{},\\n \".format(self.indiv_id))\n",
    "        ma_liste.append(\"epochs:{},\\n \".format(self.epochs))\n",
    "        ma_liste.append(\"nb_layers:{},\\n \".format(self.nb_layers))\n",
    "        ma_liste.append(\"l1:{},\\n \".format(self.l1))\n",
    "        ma_liste.append(\"l2:{},\\n \".format(self.l2))\n",
    "        ma_liste.append(\"dropout:{},\\n \".format(self.dropout))\n",
    "        ma_liste.append(\"filters_per_layers:{},\\n \".format(self.filters_per_layers))\n",
    "        ma_liste.append(\"activation:{},\\n \".format(self.activation))\n",
    "        ma_liste.append(\"kernel:\\n \")\n",
    "        ma_liste.append(\"{},\\n \".format(self.kernel))\n",
    "        ma_liste.append(\"padding:{},\\n \".format(self.padding))\n",
    "        ma_liste.append(\"maxpool:{}\\n\".format(self.max_pool))\n",
    "            \n",
    "        return ma_liste\n",
    "    \n",
    "    def create_and_train_model(self, trainX, trainY, testX, testY, main_directory):\n",
    "        #update indiv_id pour avoir un vrai ID unique\n",
    "        self.indiv_id = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        # Choix d'un emplacement pour les logs\n",
    "        log_dir=main_directory+\"\\\\logs_\"+self.indiv_id+\"\\\\tensorboard_data\\\\\"\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        \n",
    "        print(\"log dir = \",log_dir)\n",
    "        \n",
    "        # Definir notre modèle basique\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(self.filters_per_layers, self.kernel, activation=self.activation, kernel_initializer='he_uniform', padding=self.padding, input_shape=(32, 32, 3)))\n",
    "        model.add(Conv2D(self.filters_per_layers, self.kernel, activation=self.activation, kernel_initializer='he_uniform', padding=self.padding))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        \n",
    "        # Faire toutes les convs nécessaires\n",
    "        if self.nb_layers > 2:\n",
    "            for i in range(2, self.nb_layers):\n",
    "                if self.nb_layers - i != 1:\n",
    "                    print(\"i = \", i)\n",
    "                    # 2 conv + pool\n",
    "                    model.add(Conv2D(self.filters_per_layers, self.kernel, activation=self.activation, kernel_initializer='he_uniform', padding=self.padding))\n",
    "                    model.add(Conv2D(self.filters_per_layers, self.kernel, activation=self.activation, kernel_initializer='he_uniform', padding=self.padding))\n",
    "                    model.add(MaxPooling2D((2, 2)))\n",
    "                else:\n",
    "                    #print(\"endo\")\n",
    "                    # 1 conv + pool si nombre impair de couches (nb_layers)\n",
    "                    model.add(Conv2D(self.filters_per_layers, self.kernel, activation=self.activation, kernel_initializer='he_uniform', padding=self.padding))\n",
    "                    model.add(MaxPooling2D((2, 2)))\n",
    "        \n",
    "        \n",
    "        # Fin des convs -> neural network classique (je n'utilise pas self.activation car ce n'est pas relié a ce neural net)\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "        # Compiler le modele\n",
    "        opt = SGD(lr=0.001, momentum=0.9)\n",
    "        model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        #save model png\n",
    "        plot_model(model, \"model.png\")\n",
    "        #shutil.move(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\model.png\", \"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\\"+self.log_folder+\"\\\\logs\"+str(self.indiv_id)+\"\\\\model.png\")  \n",
    "        #plot_model(model, to_file=\"C:/Users/arnau/Desktop/quatrième_année/Deep_Learning/Projet_cifar-10/\"+self.log_folder+\"/logs\"+str(self.indiv_id)+\"/model.png\")\n",
    "        \n",
    "        # Entrainer le modele\n",
    "        history = model.fit(trainX, trainY, epochs=self.epochs, batch_size=64, validation_data=(testX, testY), verbose=0, callbacks=[tensorboard_callback])\n",
    "        \n",
    "        # Deplacement modele au bon endroit\n",
    "        shutil.move(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\model.png\", main_directory+\"\\\\logs_\"+self.indiv_id+\"\\\\model.png\")\n",
    "        #shutil.move(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\model.png\", \"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\\"+self.log_folder+\"\\\\logs\"+str(self.indiv_id)+\"\\\\model.png\")\n",
    "        return history\n",
    "    \n",
    "        \n",
    "    \n",
    "    def save_model(self, history, main_directory):\n",
    "        # Sauvegarder le modele\n",
    "        #plot_model(history, \"model.png\")\n",
    "        #shutil.move(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\model.png\", \"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\\"+self.log_folder+\"\\\\logs\"+str(self.indiv_id)+\"\\\\model.png\")\n",
    "        \n",
    "        # Afficher nos résultats dans un graphique matplotlib sauvegardé\n",
    "        pyplot.gcf().subplots_adjust(hspace = 0.5)\n",
    "\n",
    "        # Afficher la loss\n",
    "        pyplot.subplot(211)\n",
    "        pyplot.title('Cross Entropy Loss')\n",
    "        pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "        pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "        # Afficher l'accuracy\n",
    "        pyplot.subplot(212)\n",
    "        pyplot.title('Classification Accuracy')\n",
    "        pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "        pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "        # Sauvegarde\n",
    "        filename = main_directory+\"\\\\logs_\"+self.indiv_id+\"\\\\\"\n",
    "        print(\"filename = \",filename)\n",
    "        pyplot.savefig(filename + 'plot.png')\n",
    "        pyplot.close()\n",
    "       \n",
    "        print(\"LOSS : \", np.round(history.history['loss'],3))\n",
    "        print(\"VAL_LOSS : \", np.round(history.history['val_loss'],3))\n",
    "        print(\"ACCURACY : \", np.round(history.history['accuracy'],3))\n",
    "        print(\"VAL_ACCURACY : \", np.round(history.history['val_accuracy'],3))\n",
    "    \n",
    "        # Créer un dataframe pandas que l'on va save en csv pour conserver les hyperparamètres du modèle\n",
    "        df = pd.DataFrame({'indiv_id': [self.indiv_id],\n",
    "                           'epochs': [self.epochs],\n",
    "                           'nb_layers': [self.nb_layers],\n",
    "                           'l1': [self.l1],\n",
    "                           'l2': [self.l2],\n",
    "                           'dropout': [self.dropout],\n",
    "                           'filters_per_layers': [self.filters_per_layers],\n",
    "                           'activation': [self.activation],\n",
    "                           'kernel': [self.kernel],\n",
    "                           'padding': [self.padding],\n",
    "                           'max_pool': [self.max_pool],\n",
    "                           'loss': np.round(history.history['loss'],3),\n",
    "                           'val_loss': np.round(history.history['val_loss'],3),\n",
    "                           'accuracy': np.round(history.history['accuracy'],3),\n",
    "                           'val_accuracy': np.round(history.history['val_accuracy'],3)\n",
    "                          })\n",
    "        df.to_csv(path_or_buf=filename+\"recap.csv\",index=False)\n",
    "        \n",
    "        return \"WAZA\" #sans commentaire\n",
    "    \n",
    "    # Lance toutes les étapes\n",
    "    def exec_indiv(self, main_directory):\n",
    "        # Charger les données\n",
    "        trainX, trainY, testX, testY = load_dataset()\n",
    "        # Normaliser les données\n",
    "        trainX, testX = prep_pixels(trainX, testX)\n",
    "        # Créer et entrainer le modele\n",
    "        model = self.create_and_train_model(trainX, trainY, testX, testY, main_directory)\n",
    "        save = self.save_model(model, main_directory)\n",
    "        # Evaluate model ou pas ? -> un peu long je trouve\n",
    "        #_, acc = model.evaluate(testX, testY, verbose=0)\n",
    "        #print('> %.3f' % (acc * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe Python qui va démarrer les tests des neural nets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe générale qui va nous servir à effectuer des actions sur des listes d'individus\n",
    "class MyTraining:\n",
    "    # Prends un ID et une liste d'individus \n",
    "    def __init__(self, id_train, indiv_list):\n",
    "        self.id_train = id_train\n",
    "        self.indiv_list = indiv_list\n",
    "    \n",
    "    def train(self, main_directory):\n",
    "        print(\"Start training\\n\")\n",
    "        for idx in range(len(self.indiv_list)):\n",
    "            print(\"indiv \", self.indiv_list[idx].indiv_id)\n",
    "            self.indiv_list[idx].exec_indiv(main_directory)\n",
    "            print(\"-----------------------------------------------------------------\\n\")\n",
    "    \n",
    "    def all_indiv(self):\n",
    "        # Affiche les caractéristiques de l'ensemble des individus\n",
    "        for indiv in self.indiv_list:\n",
    "            print('\\n'.join(indiv.__str__()))\n",
    "            for tir in range(80): print('-', end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AF9xVI2bdzuT"
   },
   "source": [
    "### Hyper paramètres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XJkowDJ4d3yH"
   },
   "outputs": [],
   "source": [
    "list_indiv_id = ['1', '2']\n",
    "list_epochs = [1, 1]\n",
    "list_nb_layers = [3, 0]\n",
    "list_l1 = ['false', 'false']\n",
    "list_l2 = ['false', 'false']\n",
    "list_dropout = ['false', 'false']\n",
    "list_filters_per_layers = [64, 32]\n",
    "list_activation = ['relu', 'relu']\n",
    "list_kernel = [(3,3), (3,3)]\n",
    "list_padding = ['same', 'same']\n",
    "list_max_pool = [0, 0]\n",
    "\n",
    "main_directory =(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\CONVNETS_\"\n",
    "                 +datetime.datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "\n",
    "# Afficher ici tous nos hyper paramètres dans un beau tableau ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIGBnz_4bj74"
   },
   "source": [
    "### Traitement général (train de l'ensemble des modèles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjIV_jF6RuwF",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indiv_id:1,\n",
      " \n",
      "epochs:1,\n",
      " \n",
      "nb_layers:3,\n",
      " \n",
      "l1:0,\n",
      " \n",
      "l2:0,\n",
      " \n",
      "dropout:0,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "maxpool:0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "indiv_id:2,\n",
      " \n",
      "epochs:1,\n",
      " \n",
      "nb_layers:2,\n",
      " \n",
      "l1:0,\n",
      " \n",
      "l2:0,\n",
      " \n",
      "dropout:0,\n",
      " \n",
      "filters_per_layers:32,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "maxpool:0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start training\n",
      "\n",
      "indiv  1\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\CONVNETS_20200113-2315\\logs_20200113-231554\\tensorboard_data\\\n",
      "filename =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\CONVNETS_20200113-2315\\logs_20200113-231554\\\n",
      "LOSS :  [1.755]\n",
      "VAL_LOSS :  [1.537]\n",
      "ACCURACY :  [0.373]\n",
      "VAL_ACCURACY :  [0.45]\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "indiv  2\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\CONVNETS_20200113-2315\\logs_20200113-231613\\tensorboard_data\\\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.135140). Check your callbacks.\n",
      "filename =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\CONVNETS_20200113-2315\\logs_20200113-231613\\\n",
      "LOSS :  [1.785]\n",
      "VAL_LOSS :  [1.533]\n",
      "ACCURACY :  [0.368]\n",
      "VAL_ACCURACY :  [0.457]\n",
      "-----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Création des individus (des neurals nets, ici convnet)\n",
    "list_indiv = []\n",
    "for num in range(len(list_indiv_id)):\n",
    "    list_indiv.append(IndividuConvnets(\n",
    "        list_indiv_id[num],\n",
    "          list_epochs[num],\n",
    "          list_nb_layers[num],\n",
    "          list_l1[num],\n",
    "          list_l2[num],\n",
    "          list_dropout[num],\n",
    "          list_filters_per_layers[num],\n",
    "          list_activation[num],\n",
    "          list_kernel[num],\n",
    "          list_padding[num],\n",
    "          list_max_pool[num]\n",
    "        )\n",
    "    )\n",
    "#ancienne méthode\n",
    "#list_indiv.append(IndividuConvnets(1, 1, 3, 'false', 'false', 'false', 64, 'relu', (3,3), 'same', 0))\n",
    "#list_indiv.append(IndividuConvnets(2, 1, 0, 'false', 'false', 'false', 32, 'relu', (3,3), 'same', 0))\n",
    "\n",
    "# Chargement de la classe training, affichage des individus et train de tous les convnets\n",
    "training_1 = MyTraining(1, list_indiv)\n",
    "training_1.all_indiv()\n",
    "training_1.train(main_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxpbXOw1RuwM"
   },
   "source": [
    "### Partie tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jm6vpk3MRuwQ"
   },
   "outputs": [],
   "source": [
    "# Procedure pour utiliser tensorboard ( j ai cherché longtemps pour trouver un truc aussi nul ;( )\n",
    "#  1 load la première cell\n",
    "#  2 utiliser la derniere cell avec --logdir (précisez bien votre répertoire, plus sur que ça\n",
    "#    fonctionne avec une string \"mon_path\"\n",
    "#  3 Vous NE POURREZ PLUS update tensorboard sur ce port et il y aura des bugs, pour éviter ça\n",
    "#    quand vous voulez faire une update, fermez jupyter notebook (shutdown total) et réouvrez le \n",
    "#    OU, faites kernel->interrupt et changez de port + de folder de log\n",
    "#PS : Oui, c'est de la merde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Lj738x3Ruwa",
    "outputId": "153978fb-3dbf-4108-ecd0-ab7a38e878b6"
   },
   "outputs": [],
   "source": [
    "# Liste des ports utilisés par tensorboard, attention ça se remplit vite et il faut kill jupyter pour clean\n",
    "from tensorboard import notebook\n",
    "notebook.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rY1I5889Ruwk"
   },
   "outputs": [],
   "source": [
    "# Tuez des processus si vous voulez, moi ça fonctionne po :(\n",
    "import os\n",
    "os.system(\"taskkill /im tensorboard.exe /f\") #kill tous les processus qui utilisent tensorboard\n",
    "#os.system('!kill 18776') #kill le processus X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seEJieCtRuwq",
    "outputId": "70d52149-06ea-4d1b-b56d-a8fbccf9ed01"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir \"CONVNETS_120120/logs1/tensorboard_data\" --port=6063\n",
    "# Code pour démarrer tensorboard dans le dossier souhaité [PRECISEZ BIEN LE DOSSIER ICI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upoC4cmYRuwz",
    "outputId": "a3104ad7-ef6e-48d9-8cd6-65370eddbd61"
   },
   "outputs": [],
   "source": [
    "# Si vous avez la folie des grandeurs\n",
    "notebook.display(port=6063, height=1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLuEIeDvRuw7",
    "outputId": "a2d1ab5c-53e8-4664-dc2c-ea1bf576a22d",
    "scrolled": true
   },
   "source": [
    "### Fichier CSV Recap + Graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indiv_id</th>\n",
       "      <th>epochs</th>\n",
       "      <th>nb_layers</th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>dropout</th>\n",
       "      <th>filters_per_layers</th>\n",
       "      <th>activation</th>\n",
       "      <th>kernel</th>\n",
       "      <th>padding</th>\n",
       "      <th>max_pool</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200113-231554</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>relu</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>same</td>\n",
       "      <td>0</td>\n",
       "      <td>1.755</td>\n",
       "      <td>1.537</td>\n",
       "      <td>0.373</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          indiv_id  epochs  nb_layers  l1  l2  dropout  filters_per_layers  \\\n",
       "0  20200113-231554       1          3   0   0        0                  64   \n",
       "\n",
       "  activation  kernel padding  max_pool   loss  val_loss  accuracy  \\\n",
       "0       relu  (3, 3)    same         0  1.755     1.537     0.373   \n",
       "\n",
       "   val_accuracy  \n",
       "0          0.45  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv = pd.read_csv(main_directory+\"\\\\logs_20200113-231554\\\\recap.csv\")\n",
    "data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGXNwkMCRuxE"
   },
   "outputs": [],
   "source": [
    "image = pyplot.imread(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\CONVNETS_20200113-1951\\\\logs1\\\\indiv1_plot.png\")\n",
    "pyplot.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "colab": {
   "collapsed_sections": [],
   "name": "test_several_models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
