{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derniers tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verif batch_size sur 50 epochs entre batch_size 200 et 500\n",
    "\n",
    "# CONSTANTES : nb_layers = 8, epochs = 50, MLP à 128, batch_size=100, lr à 0.001, momentum = 0.9, optimizer Adam, avg pooling\n",
    "#  padding = same, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0,\n",
    "#full regu (batchnorm, dropout à 0.4, l1l2 à 0.0001),\n",
    "#  pool_frequency = 2, pool_frequency_change = (0,0)\n",
    "# \n",
    "#1 test -> meilleur modèle à 300 epochs\n",
    "\n",
    "list_indiv_id = ['1', '2']\n",
    "list_epochs = [50, 50]\n",
    "list_batch_size = [200, 500]\n",
    "list_nb_layers = [8,8]\n",
    "list_l1 = [0.0001, 0.0001]\n",
    "list_l2 = [0.0001, 0.0001]\n",
    "list_batch_norm = [1, 1]\n",
    "list_dropout = [0.4, 0.4]\n",
    "list_filters_per_layers = [64, 64]\n",
    "list_filters_double = [0, 0]\n",
    "list_MLP_end = [128, 128]\n",
    "list_activation = ['relu', 'relu']\n",
    "list_kernel = [(3,3), (3,3)]\n",
    "list_padding = ['same', 'same']\n",
    "list_max_or_avg_pool = ['avg', 'avg']\n",
    "list_pool_frequency = [2, 2]\n",
    "list_pool_frequency_change = [(0,0), (0,0)]\n",
    "list_learning_r = [0.001, 0.001]\n",
    "list_momentum = [0.9, 0.9]\n",
    "list_optimizer = ['Adam', 'Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests avec de la regu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 11 -> variations des résultats selon les normes l1 et l2\n",
    "\n",
    "# CONSTANTES : epochs = 50, batch_size=100, nb_layers = 8,\n",
    "# pas de batchnorm, pas de dropout, filters_per_layers à 64, pas de filters_double\n",
    "# MLP_end à 128, activation relu, kernel = (3,3), padding = same, avg pool, pool_frequency = 2,\n",
    "# pool_frequency_change à (0,0) (pas activée), learning_r à 0.001, momentum à 0.9, optimizer Adam\n",
    "\n",
    "#12 tests\n",
    "\n",
    "# 4 l1 différents\n",
    "\n",
    "# * avec l1 = 0.1\n",
    "# * avec l1 = 0.01\n",
    "# * avec l1 = 0.001\n",
    "# * avec l1 = 0.0001\n",
    "\n",
    "# 4 L2 différents\n",
    "\n",
    "# * avec l2 = 0.1\n",
    "# * avec l2 = 0.01\n",
    "# * avec l2 = 0.001\n",
    "# * avec l2 = 0.0001\n",
    "\n",
    "# 4 L1L2 différents\n",
    "\n",
    "# * avec l1L2 = 0.1\n",
    "# * avec l1L2 = 0.01\n",
    "# * avec l1L2 = 0.001\n",
    "# * avec l1L2 = 0.0001\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']\n",
    "list_epochs = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
    "list_batch_size = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
    "list_nb_layers = [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
    "list_l1 = [0.1, 0.01, 0.001, 0.0001, 0, 0, 0, 0, 0.1, 0.01, 0.001, 0.0001]\n",
    "list_l2 = [0, 0, 0, 0, 0.1, 0.01, 0.001, 0.0001, 0.1, 0.01, 0.001, 0.0001]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu','relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3)]\n",
    "list_padding = ['same','same','same','same','same','same','same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['avg','avg','avg','avg','avg','avg', 'avg', 'avg', 'avg', 'avg', 'avg', 'avg']\n",
    "list_pool_frequency = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0)]\n",
    "list_learning_r = [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
    "list_momentum = [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 12 -> variations des résultats selon le dropout\n",
    "\n",
    "# CONSTANTES : epochs = 50, batch_size=100, nb_layers = 8,\n",
    "# L1 ET L2 à 0, pas de batchnorm, filters_per_layers à 64, pas de filters_double\n",
    "# MLP_end à 128, activation relu, kernel = (3,3), padding = same, avg pool, pool_frequency = 2,\n",
    "# pool_frequency_change à (0,0) (pas activée), learning_r à 0.001, momentum à 0.9, optimizer Adam\n",
    "#\n",
    "#6 tests\n",
    "\n",
    "# * 1 dropout à 0.1\n",
    "# * 1 dropout à 0.2\n",
    "# * 1 dropout à 0.3\n",
    "# * 1 dropout à 0.4\n",
    "# * 1 dropout à 0.5\n",
    "# * 1 dropout à 0.6\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6']\n",
    "list_epochs = [50, 50, 50, 50, 50, 50]\n",
    "list_batch_size = [100, 100, 100, 100, 100, 100]\n",
    "list_nb_layers = [8,8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu', 'relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['avg','avg','avg','avg','avg','avg']\n",
    "list_pool_frequency = [2, 2, 2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0),(0,0),(0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 13 -> variations des résultats selon batchnorm\n",
    "\n",
    "# CONSTANTES : batch_size=100, nb_layers = 8,\n",
    "# L1 ET L2 à 0, batchnorm = 1, pas de dropout, filters_per_layers à 64, pas de filters_double\n",
    "# MLP_end à 128, activation relu, kernel = (3,3), padding = same, avg pool, pool_frequency = 2,\n",
    "# pool_frequency_change à (0,0) (pas activée), learning_r à 0.001, momentum à 0.9, optimizer Adam\n",
    "#\n",
    "#4 tests\n",
    "\n",
    "# batch norm ou non sur 50 epochs\n",
    "\n",
    "# * rien\n",
    "# * batchnorm\n",
    "\n",
    "# batch norm ou non sur 100 epochs\n",
    "\n",
    "# * rien\n",
    "# * batchnorm\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4']\n",
    "list_epochs = [50, 50, 100, 100]\n",
    "list_batch_size = [100, 100, 100, 100]\n",
    "list_nb_layers = [8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0]\n",
    "list_batch_norm = [0, 1, 0, 1]\n",
    "list_dropout = [0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same']\n",
    "list_max_or_avg_pool = ['avg','avg','avg','avg']\n",
    "list_pool_frequency = [2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 14 -> variations des résultats selon les différentes combinaisons possibles de regu\n",
    "\n",
    "# CONSTANTES : epochs = 50, batch_size = 100, nb_layers = 8,\n",
    "# filters_per_layers à 64, pas de filters_double\n",
    "# MLP_end à 128, activation relu, kernel = (3,3), padding = same, avg pool, pool_frequency = 2,\n",
    "# pool_frequency_change à (0,0) (pas activée), learning_r à 0.001, momentum à 0.9, optimizer Adam\n",
    "#\n",
    "#12 tests\n",
    "\n",
    "# L1 / L2 + Dropout\n",
    "\n",
    "# * avec l1L2 = 0.0001 et Dropout à 0.3\n",
    "# * avec L2 = 0.0001 et Dropout à 0.3\n",
    "# * avec l1L2 = 0.0001 et Dropout à 0.4\n",
    "# * avec L2 = 0.0001 et Dropout à 0.4\n",
    "\n",
    "# L1 / L2 + Batchnorm\n",
    "\n",
    "# * avec l1L2 = 0.0001 et BatchNorm\n",
    "# * avec L2 = 0.0001 et BatchNorm\n",
    "\n",
    "# Dropout + Batchnorm\n",
    "\n",
    "# * avec Dropout à 0.3 et BatchNorm\n",
    "# * avec Dropout à 0.4 et BatchNorm\n",
    "\n",
    "# l1 / l2 + Dropout + Batchnorm\n",
    "\n",
    "# * avec l1L2 = 0.0001 et Dropout à 0.3 BatchNorm\n",
    "# * avec L2 = 0.0001 et Dropout à 0.3 et BatchNorm\n",
    "# * avec l1L2 = 0.0001 et Dropout à 0.4 BatchNorm\n",
    "# * avec L2 = 0.0001 et Dropout à 0.4 et BatchNorm\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']\n",
    "list_epochs = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
    "list_batch_size = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
    "list_nb_layers = [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
    "list_l1 = [0.0001, 0, 0.0001, 0, 0.0001, 0, 0, 0, 0.0001, 0, 0.0001, 0]\n",
    "list_l2 = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0, 0, 0.0001, 0.0001, 0.0001, 0.0001]\n",
    "list_batch_norm = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "list_dropout = [0.3, 0.3, 0.4, 0.4, 0, 0, 0.3, 0.4, 0.3, 0.3, 0.4, 0.4]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu','relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3)]\n",
    "list_padding = ['same','same','same','same','same','same','same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['avg','avg','avg','avg','avg','avg', 'avg', 'avg', 'avg', 'avg', 'avg', 'avg']\n",
    "list_pool_frequency = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0)]\n",
    "list_learning_r = [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
    "list_momentum = [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 15A -> la recherche du meilleur, vérifier que le meilleur modèle précédent est assez constant \n",
    "#(2 50epochs, 2 100 epochs) + check influence MLP à 256,512\n",
    "\n",
    "# CONSTANTES : batch_size = 100, nb_layers = 8,\n",
    "# filters_per_layers à 64, pas de filters_double\n",
    "# activation relu, kernel = (3,3), padding = same, avg pool, pool_frequency = 2,\n",
    "# pool_frequency_change à (0,0) (pas activée), learning_r à 0.001, momentum à 0.9, optimizer Adam\n",
    "# \n",
    "#6 tests\n",
    "\n",
    "#4 tests MLP 128\n",
    "# * 1 meilleur modele 50 epochs\n",
    "# * 1 meilleur modele 50 epochs\n",
    "\n",
    "# * 1 meilleur modele 100 epochs\n",
    "# * 1 meilleur modele 100 epochs\n",
    "\n",
    "#2 tests MLP à 256 et 512\n",
    "# * 1 meilleur modèle 50 epochs MLP à 256\n",
    "# * 1 meilleur modèle 50 epochs MLP à 512\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6']\n",
    "list_epochs = [50, 50, 100, 100, 50, 50]\n",
    "list_batch_size = [100, 100, 100, 100, 100, 100]\n",
    "list_nb_layers = [8,8,8,8,8,8]\n",
    "list_l1 = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001]\n",
    "list_l2 = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001]\n",
    "list_batch_norm = [1, 1, 1, 1, 1, 1]\n",
    "list_dropout = [0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 256, 512]\n",
    "list_activation = ['relu','relu','relu','relu','relu', 'relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['avg','avg','avg','avg','avg','avg']\n",
    "list_pool_frequency = [2, 2, 2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0),(0,0),(0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 15B -> la recherche du meilleur , meilleur modèle sur 300 epochs\n",
    "\n",
    "# CONSTANTES : batch_size = 100, nb_layers = 8,\n",
    "# filters_per_layers à 64, pas de filters_double, MLP à 128,\n",
    "# activation relu, kernel = (3,3), padding = same, avg pool, pool_frequency = 2,\n",
    "# pool_frequency_change à (0,0) (pas activée), learning_r à 0.001, momentum à 0.9, optimizer Adam\n",
    "# \n",
    "#1 test -> meilleur modèle à 300 epochs\n",
    "\n",
    "list_indiv_id = ['1']\n",
    "list_epochs = [300]\n",
    "list_batch_size = [100]\n",
    "list_nb_layers = [8]\n",
    "list_l1 = [0.0001]\n",
    "list_l2 = [0.0001]\n",
    "list_batch_norm = [1]\n",
    "list_dropout = [0.4]\n",
    "list_filters_per_layers = [64]\n",
    "list_filters_double = [0]\n",
    "list_MLP_end = [128]\n",
    "list_activation = ['relu']\n",
    "list_kernel = [(3,3)]\n",
    "list_padding = ['same']\n",
    "list_max_or_avg_pool = ['avg']\n",
    "list_pool_frequency = [2]\n",
    "list_pool_frequency_change = [(0,0)]\n",
    "list_learning_r = [0.001]\n",
    "list_momentum = [0.9]\n",
    "list_optimizer = ['Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 15C -> variations du meilleur modèle acutellement trouvé avec différentes data augmentation\n",
    "\n",
    "# CONSTANTES : epochs = 60, batch_size = 100, nb_layers = 8,\n",
    "# filters_per_layers à 64, pas de filters_double, MLP à 128,\n",
    "# activation relu, kernel = (3,3), padding = same, avg pool, pool_frequency = 2,\n",
    "# pool_frequency_change à (0,0) (pas activée), learning_r à 0.001, momentum à 0.9, optimizer Adam\n",
    "#\n",
    "#3 tests\n",
    "\n",
    "#Meilleur modèle avec des datas aug différentes\n",
    "#  * Petite data augmentation : variation sur la largeur et la hauteur de 0.1 et flip horizontal aléatoire\n",
    "#  * Data augmentation un peu plus étoffée : pareil avec un zoom pouvant varier jusqu'à 30%\n",
    "#  * Grosse Data Augmentation : pareil avec rotation pouvant aller jusq'à 45°\n",
    "\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3']\n",
    "list_epochs = [60, 60, 60]\n",
    "list_batch_size = [100, 100, 100]\n",
    "list_nb_layers = [8,8,8]\n",
    "list_l1 = [0.0001, 0.0001, 0.0001]\n",
    "list_l2 = [0.0001, 0.0001, 0.0001]\n",
    "list_batch_norm = [1, 1, 1]\n",
    "list_dropout = [0.4, 0.4, 0.4]\n",
    "list_filters_per_layers = [64, 64, 64]\n",
    "list_filters_double = [0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128]\n",
    "list_activation = ['relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same']\n",
    "list_max_or_avg_pool = ['avg','avg','avg']\n",
    "list_pool_frequency = [2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 15C FINAL -> meilleur modèle + faible data aug sur 200 epochs\n",
    "\n",
    "# CONSTANTES : epochs = 200, batch_size = 100, nb_layers = 8,\n",
    "# filters_per_layers à 64, pas de filters_double, MLP à 128,\n",
    "# activation relu, kernel = (3,3), padding = same, avg pool, pool_frequency = 2,\n",
    "# pool_frequency_change à (0,0) (pas activée), learning_r à 0.001, momentum à 0.9, optimizer Adam\n",
    "#\n",
    "#1 test\n",
    "\n",
    "#Meilleur modèle avec petite data augmentation : variation sur la largeur et la hauteur de 0.1 et flip horizontal aléatoire\n",
    "\n",
    "list_indiv_id = ['1']\n",
    "list_epochs = [200]\n",
    "list_batch_size = [100]\n",
    "list_nb_layers = [8]\n",
    "list_l1 = [0.0001]\n",
    "list_l2 = [0.0001]\n",
    "list_batch_norm = [1]\n",
    "list_dropout = [0.4]\n",
    "list_filters_per_layers = [64]\n",
    "list_filters_double = [0]\n",
    "list_MLP_end = [128]\n",
    "list_activation = ['relu']\n",
    "list_kernel = [(3,3)]\n",
    "list_padding = ['same']\n",
    "list_max_or_avg_pool = ['avg']\n",
    "list_pool_frequency = [2]\n",
    "list_pool_frequency_change = [(0,0)]\n",
    "list_learning_r = [0.001]\n",
    "list_momentum = [0.9]\n",
    "list_optimizer = ['Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests organisés dans le cadre du Projet avec + d'options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 4 -> variations des résultats selon le nombre de couches de pooling\n",
    "\n",
    "# CONSTANTES : epochs = 30 (30 car avec beaucoup de layers ça va être long), batch_size = 50, nb_layers = 8,\n",
    "# L1 ET L2 à 0, pas de batchnorm, pas de dropout, filters_per_layers à 64, pas de filters_double\n",
    "# MLP_end à 128, activation relu, kernel = (3,3), padding = same, max pool,\n",
    "# pool_frequency_change à (0,0) (pas activée), learning_r à 0.001, momentum à 0.9, optimizer Adam\n",
    "#\n",
    "\n",
    "# * 1 convnet du type (1 conv2D) -> 1 (pool)   (8 pools au total)\n",
    "\n",
    "# * 1 convnet du type (2 conv2D) -> 1 (pool)   (4 pools au total)\n",
    "\n",
    "# * 1 convnet du type (4 conv2D) -> 1 (pool)   (2 pools au total)\n",
    "\n",
    "# * 1 convnet du type (8 conv2D) -> 1 (pool)   (1 pool au total)\n",
    "\n",
    "# * 1 convnet du type (8 conv2D) -> 0 (pool)   (0 pool au total)\n",
    "\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5']\n",
    "list_epochs = [30, 30, 30, 30, 30]\n",
    "list_batch_size = [50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max']\n",
    "list_pool_frequency = [1, 2, 4, 8, 0]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0), (0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 5 -> variations des résultats selon le batch_size\n",
    "\n",
    "# CONSTANTES : epochs = 20 (20 car avec beaucoup de layers ça va être long), nb_layers = 8,\n",
    "# L1 ET L2 à 0, pas de batchnorm, pas de dropout, filters_per_layers à 64, pas de filters_double\n",
    "# MLP_end à 128, activation relu, kernel = (3,3), padding = same, max pool, pool_frequency = 2,\n",
    "# pool_frequency_change à (0,0) (pas activée), learning_r à 0.001, momentum à 0.9, optimizer Adam\n",
    "# \n",
    "\n",
    "# * 1 convnet du type batch_size = 50\n",
    "\n",
    "# * 1 convnet du type batch_size = 100\n",
    "\n",
    "# * 1 convnet du type batch_size = 150\n",
    "\n",
    "# * 1 convnet du type batch_size = 200\n",
    "\n",
    "# * 1 convnet du type batch_size = 500\n",
    "\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5']\n",
    "list_epochs = [20, 20, 20, 20, 20]\n",
    "list_batch_size = [50, 100, 150, 200, 500]\n",
    "list_nb_layers = [8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max']\n",
    "list_pool_frequency = [2, 2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0), (0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 6 -> variations des résultats selon le learning_rate\n",
    "\n",
    "# CONSTANTES : epochs = 30 (30 car avec beaucoup de layers ça va être long), batch_size = 100, nb_layers = 8,\n",
    "# L1 ET L2 à 0, pas de batchnorm, pas de dropout, filters_per_layers à 64, pas de filters_double\n",
    "# MLP_end à 128, activation relu, kernel = (3,3), padding = same, max pool, pool_frequency = 2,\n",
    "# pool_frequency_change à (0,0) (pas activée), momentum à 0.9, optimizer Adam\n",
    "# \n",
    "#8 tests\n",
    "\n",
    "# * 1 convnet de avec lr à 0.5\n",
    "\n",
    "# * 1 convnet de avec lr à 0.1\n",
    "\n",
    "# * 1 convnet de avec lr à 0.05\n",
    "\n",
    "# * 1 convnet de avec lr à 0.01\n",
    "\n",
    "# * 1 convnet de avec lr à 0.005\n",
    "\n",
    "# * 1 convnet de avec lr à 0.001\n",
    "\n",
    "# * 1 convnet de avec lr à 0.0005\n",
    "\n",
    "# * 1 convnet de avec lr à 0.0001\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6', '7', '8']\n",
    "list_epochs = [30, 30, 30, 30, 30, 30, 30, 30]\n",
    "list_batch_size = [100, 100, 100, 100, 100, 100, 100, 100]\n",
    "list_nb_layers = [8,8,8,8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu', 'relu', 'relu', 'relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max','max','max']\n",
    "list_pool_frequency = [2, 2, 2, 2, 2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0),(0,0),(0,0),(0,0),(0,0)]\n",
    "list_learning_r = [0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 7 -> variations des résultats selon le nb d'epochs et le max_or_avg_pool\n",
    "\n",
    "# CONSTANTES : batch_size = 100, nb_layers = 8,\n",
    "# L1 ET L2 à 0, pas de batchnorm, pas de dropout, filters_per_layers à 64, pas de filters_double\n",
    "# MLP_end à 128, activation relu, kernel = (3,3), padding = same, pool_frequency = 2,\n",
    "# pool_frequency_change à (0,0) (pas activée), learning_r à 0.001, momentum à 0.9, optimizer Adam\n",
    "# \n",
    "#6 tests\n",
    "\n",
    "# * 1 max pool 50\n",
    "\n",
    "# * 1 max pool 100\n",
    "\n",
    "# * 1 max pool 200\n",
    "\n",
    "# * 1 avg pool 50\n",
    "\n",
    "# * 1 avg pool 100\n",
    "\n",
    "# * 1 avg pool 200\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6']\n",
    "list_epochs = [50, 100, 200, 50, 100, 200]\n",
    "list_batch_size = [100, 100, 100, 100, 100, 100]\n",
    "list_nb_layers = [8,8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu', 'relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','avg','avg','avg']\n",
    "list_pool_frequency = [2, 2, 2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0),(0,0),(0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 8 -> variations des résultats selon le nb de convs + pool (layers block) \n",
    "#et le nb de filtres (evol conv et filtres sur le dernier test) avec double filtres sur les pools\n",
    "\n",
    "# CONSTANTES : epochs = 50, batch_size=100, nb_layers = 8,\n",
    "# L1 ET L2 à 0, pas de batchnorm, pas de dropout, filters_double = -1 (double sur chaque pool)\n",
    "# MLP_end à 128, activation relu, kernel = (3,3), padding = same, avg pool,\n",
    "# learning_r à 0.001, momentum à 0.9, optimizer Adam\n",
    "#\n",
    "#10 tests\n",
    "\n",
    "# Petits groupes de 2 avec avg pool et double filters sur chaque pool\n",
    "\n",
    "# * avec 6 layers en commencant à 16 filtres\n",
    "# * avec 6 layers en commencant à 32 filtres\n",
    "#\n",
    "# * avec 8 layers en commencant à 16 filtres\n",
    "# * avec 8 layers en commencant à 32 filtres\n",
    "\n",
    "# Petits groupes de 4 avec avg pool et double filters sur chaque pool\n",
    "\n",
    "# * avec 8 layers en commencant à 16 filtres\n",
    "# * avec 8 layers en commencant à 32 filtres\n",
    "#\n",
    "# * avec 16 layers en commencant à 16 filtres\n",
    "# * avec 16 layers en commencant à 32 filtres\n",
    "\n",
    "# 3 groupes de 2 conv + pool puis 2 groupes de 4 conv + pool (14 layers)\n",
    "#  avec double filters sur chaque pool\n",
    "\n",
    "# * avec 16 filtres au début\n",
    "# * avec 32 filtres au début\n",
    "\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "list_epochs = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
    "list_batch_size = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
    "list_nb_layers = [6,6,8,8,8,8,16,16,14,14]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [16, 32, 16, 32, 16, 32, 16, 32, 16, 32]\n",
    "list_filters_double = [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
    "list_MLP_end = [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu', 'relu', 'relu', 'relu', 'relu', 'relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3), (3,3), (3,3), (3,3), (3,3)]\n",
    "list_padding = ['same','same','same','same','same','same', 'same', 'same', 'same', 'same']\n",
    "list_max_or_avg_pool = ['avg','avg','avg','avg','avg','avg', 'avg', 'avg', 'avg', 'avg']\n",
    "list_pool_frequency = [2, 2, 2, 2, 4, 4, 4, 4, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0),(0,0),(0,0),(0,0),(0,0),(3,2),(3,2)]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001, 0.001, 0.001, 0.001, 0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9, 0.9, 0.9, 0.9, 0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam', 'Adam', 'Adam', 'Adam', 'Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 9 -> variations des résultats selon l'optimizer choisi (Adam ou SGD)\n",
    "\n",
    "# CONSTANTES : batch_size=100, nb_layers = 8,\n",
    "# L1 ET L2 à 0, pas de batchnorm, pas de dropout, filters_per_layers à 64, pas de filters_double\n",
    "# MLP_end à 128, activation relu, kernel = (3,3), padding = same, avg pool, pool_frequency = 2,\n",
    "# pool_frequency_change à (0,0) (pas activée), learning_r à 0.001, momentum à 0.9\n",
    "#\n",
    "#4 tests\n",
    "\n",
    "# Adam ou SGD sur 50 epochs\n",
    "\n",
    "# * Adam\n",
    "# * SGD\n",
    "\n",
    "# Adam ou SGD sur 100 epochs\n",
    "\n",
    "# * Adam\n",
    "# * SGD\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4']\n",
    "list_epochs = [50, 100, 50, 100]\n",
    "list_batch_size = [100, 100, 100, 100]\n",
    "list_nb_layers = [8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same']\n",
    "list_max_or_avg_pool = ['avg','avg','avg','avg']\n",
    "list_pool_frequency = [2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','SGD','Adam','SGD']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 10 -> variations des résultats selon l'activation (des conv2D) choisie (relu vs sigmoid vs tanh)\n",
    "\n",
    "# CONSTANTES :  batch_size = 100, nb_layers = 8,\n",
    "# L1 ET L2 à 0, pas de batchnorm, pas de dropout, filters_per_layers à 64, pas de filters_double\n",
    "# MLP_end à 128, kernel = (3,3), padding = same, avg pool, pool_frequency = 2,\n",
    "# pool_frequency_change à (0,0) (pas activée), learning_r à 0.001, momentum à 0.9, optimizer Adam\n",
    "#\n",
    "#6 tests\n",
    "\n",
    "# relu vs sigmoid vs tanh  sur 50 epochs\n",
    "\n",
    "# * relu\n",
    "# * sigmoid\n",
    "# * tanh\n",
    "\n",
    "# relu vs sigmoid vs tanh  sur 100 epochs\n",
    "\n",
    "# * relu\n",
    "# * sigmoid\n",
    "# * tanh\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6']\n",
    "list_epochs = [50, 50, 50, 100, 100, 100]\n",
    "list_batch_size = [100, 100, 100, 100, 100, 100]\n",
    "list_nb_layers = [8,8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','sigmoid','tanh','relu','sigmoid', 'tanh']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['avg','avg','avg','avg','avg','avg']\n",
    "list_pool_frequency = [2, 2, 2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0),(0,0),(0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests organisés dans le cadre du Projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 1 -> importance des layers (refaire le test 5 mais sans la regularization)\n",
    "\n",
    "# CONSTANTES : epochs = 30 (30 car avec beaucoup de layers ça va être long), batch_size = 50,\n",
    "# L1 ET L2 à 0, pas de batchnorm, pas de dropout, filters_per_layers à 64, pas de filters_double\n",
    "# Pas de MLP_end, activation relu, kernel = (3,3), padding = same, max pool, pool_frequency = 2,\n",
    "# pool_frequency_change à (0,0) (pas activée), learning_r à 0.001, momentum à 0.9, optimizer Adam\n",
    "\n",
    "#6 NB layers fixes -> 8, 16, 32, 64, 96, 128\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6']\n",
    "list_epochs = [30, 30, 30, 30, 30, 30]\n",
    "list_batch_size = [50, 50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,16,32,64,96,128]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [0, 0, 0, 0, 0, 0]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max']\n",
    "list_pool_frequency = [2, 2, 2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0),(0,0),(0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 2 -> importance des filters\n",
    "# CONSTANTES : epochs = 50, batch_size = 50, nb_layers à 8\n",
    "# L1 ET L2 à 0, pas de batchnorm, pas de dropout,\n",
    "# MLP_end à 128, activation relu, kernel = (3,3), padding = same, max pool, pool_frequency = 2,\n",
    "# pool_frequency_change à (0,0) (pas activée), learning_r à 0.001, momentum à 0.9, optimizer Adam\n",
    "# \n",
    "# * 5 convnets avec filters fixes\n",
    "#   - 1 sans filters double avec filters (16)\n",
    "#   - 1 sans filters double avec filters (32)\n",
    "#   - 1 sans filters double avec filters (64)\n",
    "#   - 1 sans filters double avec filters (96)\n",
    "#   - 1 sans filters double avec filters (128)\n",
    "\n",
    "# * 5 convnets avec filters evolutifs (filter double sur mes blocs de 2 couches de convolution)\n",
    "#   - 1 filters double (2) avec filters (16)  (va faire 16, 32, 64, 128)\n",
    "#   - 1 filters double (2) avec filters (32)  (va faire 32, 64, 128, 256)\n",
    "#   - 1 filters double (4) avec filters (16)  (va faire 16, 16, 32, 32)\n",
    "#   - 1 filters double (4) avec filters (32)  (va faire 32, 32, 64, 64)\n",
    "#   - 1 filters double (4) avec filters (64)  (va faire 64, 64, 128, 128)\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "list_epochs = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
    "list_batch_size = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,8,8,8,8,8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [16, 32, 64, 96, 128, 16, 32, 16, 32, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 2, 2, 4, 4, 4]\n",
    "list_MLP_end = [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max','max','max','max','max']\n",
    "list_pool_frequency = [2, 2, 2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0),(0,0),(0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 3 -> importance du MLP avant la softmax\n",
    "# CONSTANTES : epochs = 100, batch_size = 50, nb_layers à 8\n",
    "# L1 ET L2 à 0, pas de batchnorm, pas de dropout, filters_per_layers = 64, pas de filters_double\n",
    "# activation relu, kernel = (3,3), padding = same, max pool, pool_frequency = 2,\n",
    "# pool_frequency_change à (0,0) (pas activée), learning_r à 0.001, momentum à 0.9, optimizer Adam\n",
    "\n",
    "# * 1 convnet sans MLP\n",
    "\n",
    "# * 5 convnets avec MLP\n",
    "#   - 1 avec (MLP 32)\n",
    "#   - 1 avec (MLP 64)\n",
    "#   - 1 avec (MLP 128)\n",
    "#   - 1 avec (MLP 256)\n",
    "#   - 1 avec (MLP 512)\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6']\n",
    "list_epochs = [100, 100, 100, 100, 100, 100]\n",
    "list_batch_size = [50, 50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [0, 32, 64, 128, 256, 512]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max']\n",
    "list_pool_frequency = [2, 2, 2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0),(0,0),(0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premiers tests avec résultats cohérents (lr à 0.001, à tester avec autres hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quatrième test -> test 1 avec lr à 0.001 (et sans le bug, pour les bons résultats!)\n",
    "# CONSTANTES : nb_layers = 8, batch_size = 50, epochs 100, lr = 0.001, momentum = 0.9, optimizer Adam, \n",
    "#padding = same, maxpool, relu, kernel = (3,3)\n",
    "# \n",
    "# * 3 convnets sans regularization et MLP à 128\n",
    "#   - 2 filters double(2) avec filters (16, 32) \n",
    "#   - 1 sans filters double avec filters (32)\n",
    "\n",
    "# 2 convnets sans regu ou on test le MLP_end et filters 64\n",
    "#  * 1 sans double, filter 64 avec MLP_end(128)\n",
    "#  * 1 sans double, filter 64 avec MLP_end(0)\n",
    "\n",
    "# 5 convnets avec regu (+ MLP à 128) et filters 32 sans double\n",
    "#  * 1 convnet avec l1 à 0.01\n",
    "#  * 1 convnet avec l1 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec l2 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec L1 et L2 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec L1 et L2 à 0.01 + batchnorm + dropout à 0.2\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "list_epochs = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
    "list_batch_size = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,8,8,8,8,8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0, 0, 0.01, 0.01, 0.01]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2]\n",
    "list_filters_per_layers = [16, 32, 32, 64, 64, 32, 32, 32, 32, 32]\n",
    "list_filters_double = [2, 2, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 0, 128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max','max','max','max','max']\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "main_directory =(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\CONVNETS_\"\n",
    "                 +datetime.datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cinquième test-> Inlfuence des layers (sans bug affi et avec lr à 0.001):\n",
    "# CONSTANTES : filters = 64, batch_size = 50, epochs 30, lr = 0.001, momentum = 0.9, optimizer Adam, \n",
    "#padding = same, maxpool, relu, kernel = (3,3), avec regu L1L2 à 0.01 et dropout à 0.2 \n",
    "#(sans Batchnorm, sans MLP final, sans filters double)\n",
    "\n",
    "\n",
    "#6 NB layers fixes -> 8, 16, 32, 64, 96, 128\n",
    "\n",
    "# LEXIQUE PARAM : \n",
    "# * filters_double permet de savoir toutes les combien de couche on double les filtres, si 0 on double pas\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6']\n",
    "list_epochs = [1, 1, 1, 1, 1, 1]\n",
    "list_batch_size = [50, 50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,16,32,64,96,128]\n",
    "list_l1 = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "list_l2 = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [0, 0, 0, 0, 0, 0]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max']\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A partir d'ici, premiers tests avec lr trop haut (0.01) ?? et problème d'affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Premier Test (avec bug affi et lr trop haut):\n",
    "# CONSTANTES : nb_layers = 8, batch_size = 50, epochs 100, lr = 0.01, momentum = 0.9, optimizer Adam, \n",
    "#padding = same, maxpool, relu, kernel = (3,3)\n",
    "# \n",
    "# * 3 convnets sans regularization et MLP à 128\n",
    "#   - 2 filters double(2) avec filters (16, 32) \n",
    "#   - 1 sans filters double avec filters (32)\n",
    "\n",
    "# 2 convnets sans regu ou on test le MLP_end et filters 64\n",
    "#  * 1 sans double, filter 64 avec MLP_end(128)\n",
    "#  * 1 sans double, filter 64 avec MLP_end(0)\n",
    "\n",
    "# 5 convnets avec regu (+ MLP à 128) et filters 32 sans double\n",
    "#  * 1 convnet avec l1 à 0.01\n",
    "#  * 1 convnet avec l1 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec l2 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec L1 et L2 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec L1 et L2 à 0.01 + batchnorm + dropout à 0.2\n",
    "\n",
    "# LEXIQUE PARAM : \n",
    "# * filters_double permet de savoir toutes les combien de couche on double les filtres, si 0 on double pas\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "list_epochs = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
    "list_batch_size = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,8,8,8,8,8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0, 0, 0.01, 0.01, 0.01]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2]\n",
    "list_filters_per_layers = [16, 32, 32, 64, 64, 32, 32, 32, 32, 32]\n",
    "list_filters_double = [2, 2, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 0, 128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max','max','max','max','max']\n",
    "list_learning_r = [0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "main_directory =(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\CONVNETS_\"\n",
    "                 +datetime.datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(current)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deuxième test (avec bug affi et lr trop haut)-> Inlfuence des layers :\n",
    "# CONSTANTES : filters = 64, batch_size = 50, epochs 100, lr = 0.01, momentum = 0.9, optimizer Adam, \n",
    "#padding = same, maxpool, relu, kernel = (3,3), avec regu L1L2 à 0.01 et dropout à 0.2 \n",
    "#(sans Batchnorm, sans MLP final, sans filters double)\n",
    "\n",
    "\n",
    "#6 NB layers fixes ->8, 16, 32, 64, 96, 128\n",
    "\n",
    "# LEXIQUE PARAM : \n",
    "# * filters_double permet de savoir toutes les combien de couche on double les filtres, si 0 on double pas\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6']\n",
    "list_epochs = [1, 1, 1, 1, 1, 1]\n",
    "list_batch_size = [50, 50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,16,32,64,96,128]\n",
    "list_l1 = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "list_l2 = [0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [0, 0, 0, 0, 0, 0]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max']\n",
    "list_learning_r = [0.01,0.01,0.01,0.01,0.01,0.01]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Troisième test (progression à taton pour debug) -> Inlfuence des layers sur plus d'epochs et avec PMC 128 :\n",
    "# CONSTANTES : filters = 64, batch_size = 50, epochs 50, lr = 0.01, momentum = 0.9, optimizer Adam, \n",
    "#padding = same, maxpool, relu, kernel = (3,3), avec regu L1L2 à 0.01 et dropout à 0.2 \n",
    "#(sans Batchnorm, sans MLP final, sans filters double)\n",
    "\n",
    "\n",
    "#4 NB layers fixes -> 8, 16, 32, 64\n",
    "\n",
    "# LEXIQUE PARAM : \n",
    "# * filters_double permet de savoir toutes les combien de couche on double les filtres, si 0 on double pas\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4']\n",
    "list_epochs = [50, 50, 50, 50]\n",
    "list_batch_size = [50, 50, 50, 50]\n",
    "list_nb_layers = [8,16,32,64]\n",
    "list_l1 = [0.01, 0.01, 0.01, 0.01]\n",
    "list_l2 = [0.01, 0.01, 0.01, 0.01]\n",
    "list_batch_norm = [0, 0, 0, 0]\n",
    "list_dropout = [0.2, 0.2, 0.2, 0.2]\n",
    "list_filters_per_layers = [64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max']\n",
    "list_learning_r = [0.01,0.01,0.01,0.01]\n",
    "list_momentum = [0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Troisième test BIS (taton debug) -> Ne pas obtenir des résultats inutiles (0.1 en accuracy et 2.304 en loss) en faisant bouger très peu\n",
    "#de paramètres pour essayer de trouver ce qui empêche l'évolution\n",
    "\n",
    "#IDEES : \n",
    "# à cause de l'input shape fxé partout ?\n",
    "# Sans evolution des filters / batchnorm --> résultat dég ?\n",
    "# ...\n",
    "\n",
    "\n",
    "#4 NB layers fixes -> 8 pour l'instant\n",
    "\n",
    "#sous-test 1 => rien de modif pour id 1, batchnorm pour id 2 -> 0.1 vs 0.31\n",
    "#sous-test-2 => rien de modif pour id 1, evolu filters de 16 à 128 (*2 tous les 2 layers) -> pas de changement (pk ?)\n",
    "#sous-test-3 => rien de modif pour id 1, evolu filters de 32 à 256 (*2 tous les 2 layers) -> pas de changement (bizarre)\n",
    "#sous-test-4 => faire le même test que pour le 2eme indivi du \"premier test convnet\" mais avec 50 epochs\n",
    "# differences avec sous-test précédent = pas de l1 et L2, pas de dropout \n",
    "# -> AUCUN CHANGEMENT (PAREIL SI ON REFAIT SUR L ORIGINAL)\n",
    "#sous-test-5 => inspi VGG BLOCK 1 trouve sur net => Ok \n",
    "\n",
    "#IL FALLAIT METTRE EPOCHS - 1 DANS LES VAR QUI EXPLOITENT HISTORY,\n",
    "#REFAIRE DES TESTS\n",
    "\n",
    "#refaire sous-test 4 sans bug avec lr à 0.001 et batch size 64=> on voit tout \n",
    "# de suite une progression, descendre le lr a été très utile => 0.75 val_accuracy !\n",
    "\n",
    "# LEXIQUE PARAM : \n",
    "# * filters_double permet de savoir toutes les combien de couche on double les filtres, si 0 on double pas\n",
    "'''\n",
    "list_indiv_id = ['2']\n",
    "list_epochs = [10]\n",
    "list_batch_size = [64]\n",
    "list_nb_layers = [2]\n",
    "list_l1 = [0]\n",
    "list_l2 = [0]\n",
    "list_batch_norm = [0]\n",
    "list_dropout = [0]\n",
    "list_filters_per_layers = [32]\n",
    "list_filters_double = [0]\n",
    "list_MLP_end = [128]\n",
    "list_activation = ['relu']\n",
    "list_kernel = [(3,3)]\n",
    "list_padding = ['same']\n",
    "list_max_or_avg_pool = ['max']\n",
    "list_learning_r = [0.001]\n",
    "list_momentum = [0.9]\n",
    "list_optimizer = ['SGD']\n",
    "'''\n",
    "\n",
    "'''\n",
    "list_indiv_id = ['2']\n",
    "list_epochs = [50]\n",
    "list_batch_size = [64]\n",
    "list_nb_layers = [8]\n",
    "list_l1 = [0]\n",
    "list_l2 = [0]\n",
    "list_batch_norm = [0]\n",
    "list_dropout = [0]\n",
    "list_filters_per_layers = [32]\n",
    "list_filters_double = [2]\n",
    "list_MLP_end = [128]\n",
    "list_activation = ['relu']\n",
    "list_kernel = [(3,3)]\n",
    "list_padding = ['same']\n",
    "list_max_or_avg_pool = ['max']\n",
    "list_learning_r = [0.001]\n",
    "list_momentum = [0.9]\n",
    "list_optimizer = ['Adam']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
