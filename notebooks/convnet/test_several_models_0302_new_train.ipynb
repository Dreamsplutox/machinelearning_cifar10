{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7R0yO1XdBNh"
   },
   "source": [
    "### Démarrage de tensorboard et imports principaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rCCbrXXRuvi",
    "outputId": "7d91c098-39d8-4a89-adf8-3133ab8217de"
   },
   "outputs": [],
   "source": [
    "# Agrandir le notebook ?\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Commande à entrer dans le prompt anaconda si on veut éviter que notre jupyter crash\n",
    "#  sur un long entrainement\n",
    "#jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000\n",
    "\n",
    "# Démarrage de tensorboard pour notebook\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Dense, Flatten, \\\n",
    "Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import shutil  \n",
    "from math import ceil, floor\n",
    "\n",
    "from tensorflow.python.framework import ops #pour tenter de reset tensorboard, sans grand succès\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AF9xVI2bdzuT"
   },
   "source": [
    "### Hyper paramètres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#A PARTIR D ICI, NOUVELLE SYNTAXE (pool_frequency + pool_frequency_change et modif  filters_double)\\n\\n#Pool frequency permet de définir toutes les combiens de couches on va pool\\n\\n#Pool_frequency_change va modifier la frequence UNE FOIS dans le programme \\n#  se note sous la forme (index_pool_changement, modif) => (2, -1) -> après avoir fait 2 pools, \\n#  on fait pool_frequency = pool_frequency -1 => a la place de (conv2D * 2) + pool, on aura \\n#  jusqu\\'à la fin du programme (conv2D) + pool\\n\\n#filiters_double, quand il est init à -1, va faire doubler les filters des convs apres CHAQUE pooling\\n\\n#Real Test 4 -> variations des résultats selon le nombre de couches de pooling\\n\\n# CONSTANTES : nb_layers = 8, batch_size = 50, epochs = 50, lr = 0.001, momentum = 0.9, optimizer Adam, \\n#  padding = same, maxpool, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0, pas de regularization, MLP 128,\\n#  pool_frequency_change = (0,0)\\n# \\n\\n# * 1 convnet du type (1 conv2D) -> 1 (pool)   (8 pools au total)\\n\\n# * 1 convnet du type (2 conv2D) -> 1 (pool)   (4 pools au total)\\n\\n# * 1 convnet du type (4 conv2D) -> 1 (pool)   (2 pools au total)\\n\\n# * 1 convnet du type (8 conv2D) -> 1 (pool)   (1 pool au total)\\n\\n# * 1 convnet du type (8 conv2D) -> 0 (pool)   (0 pool au total)\\n\\n\\n\\nlist_indiv_id = [\\'1\\', \\'2\\', \\'3\\', \\'4\\', \\'5\\']\\nlist_epochs = [30, 30, 30, 30, 30]\\nlist_batch_size = [50, 50, 50, 50, 50]\\nlist_nb_layers = [8,8,8,8,8]\\nlist_l1 = [0, 0, 0, 0, 0]\\nlist_l2 = [0, 0, 0, 0, 0]\\nlist_batch_norm = [0, 0, 0, 0, 0]\\nlist_dropout = [0, 0, 0, 0, 0]\\nlist_filters_per_layers = [64, 64, 64, 64, 64]\\nlist_filters_double = [0, 0, 0, 0, 0]\\nlist_MLP_end = [128, 128, 128, 128, 128]\\nlist_activation = [\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\']\\nlist_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3)]\\nlist_padding = [\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\']\\nlist_max_or_avg_pool = [\\'max\\',\\'max\\',\\'max\\',\\'max\\',\\'max\\']\\nlist_pool_frequency = [1, 2, 4, 8, 0]\\nlist_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0), (0,0)]\\nlist_learning_r = [0.001,0.001,0.001,0.001,0.001]\\nlist_momentum = [0.9,0.9,0.9,0.9,0.9]\\nlist_optimizer = [\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\']\\n\\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\\nmain_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#A PARTIR D ICI, NOUVELLE SYNTAXE (pool_frequency + pool_frequency_change et modif  filters_double)\n",
    "\n",
    "#Pool frequency permet de définir toutes les combiens de couches on va pool\n",
    "\n",
    "#Pool_frequency_change va modifier la frequence UNE FOIS dans le programme \n",
    "#  se note sous la forme (index_pool_changement, modif) => (2, -1) -> après avoir fait 2 pools, \n",
    "#  on fait pool_frequency = pool_frequency -1 => a la place de (conv2D * 2) + pool, on aura \n",
    "#  jusqu'à la fin du programme (conv2D) + pool\n",
    "\n",
    "#filiters_double, quand il est init à -1, va faire doubler les filters des convs apres CHAQUE pooling\n",
    "\n",
    "#Real Test 4 -> variations des résultats selon le nombre de couches de pooling\n",
    "\n",
    "# CONSTANTES : nb_layers = 8, batch_size = 50, epochs = 50, lr = 0.001, momentum = 0.9, optimizer Adam, \n",
    "#  padding = same, maxpool, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0, pas de regularization, MLP 128,\n",
    "#  pool_frequency_change = (0,0)\n",
    "# \n",
    "\n",
    "# * 1 convnet du type (1 conv2D) -> 1 (pool)   (8 pools au total)\n",
    "\n",
    "# * 1 convnet du type (2 conv2D) -> 1 (pool)   (4 pools au total)\n",
    "\n",
    "# * 1 convnet du type (4 conv2D) -> 1 (pool)   (2 pools au total)\n",
    "\n",
    "# * 1 convnet du type (8 conv2D) -> 1 (pool)   (1 pool au total)\n",
    "\n",
    "# * 1 convnet du type (8 conv2D) -> 0 (pool)   (0 pool au total)\n",
    "\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5']\n",
    "list_epochs = [30, 30, 30, 30, 30]\n",
    "list_batch_size = [50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max']\n",
    "list_pool_frequency = [1, 2, 4, 8, 0]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0), (0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Real Test 5 -> variations des résultats selon le batch_size\\n\\n# CONSTANTES : nb_layers = 8, epochs = 20, lr = 0.001, momentum = 0.9, optimizer Adam, \\n#  padding = same, maxpool, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0, pas de regularization, MLP 128,\\n#  pool_frequency = 2, pool_frequency_change = (0,0)\\n# \\n\\n# * 1 convnet du type batch_size = 50\\n\\n# * 1 convnet du type batch_size = 100\\n\\n# * 1 convnet du type batch_size = 150\\n\\n# * 1 convnet du type batch_size = 200\\n\\n# * 1 convnet du type batch_size = 500\\n\\n\\n\\nlist_indiv_id = [\\'1\\', \\'2\\', \\'3\\', \\'4\\', \\'5\\']\\nlist_epochs = [20, 20, 20, 20, 20]\\nlist_batch_size = [50, 100, 150, 200, 500]\\nlist_nb_layers = [8,8,8,8,8]\\nlist_l1 = [0, 0, 0, 0, 0]\\nlist_l2 = [0, 0, 0, 0, 0]\\nlist_batch_norm = [0, 0, 0, 0, 0]\\nlist_dropout = [0, 0, 0, 0, 0]\\nlist_filters_per_layers = [64, 64, 64, 64, 64]\\nlist_filters_double = [0, 0, 0, 0, 0]\\nlist_MLP_end = [128, 128, 128, 128, 128]\\nlist_activation = [\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\']\\nlist_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3)]\\nlist_padding = [\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\']\\nlist_max_or_avg_pool = [\\'max\\',\\'max\\',\\'max\\',\\'max\\',\\'max\\']\\nlist_pool_frequency = [2, 2, 2, 2, 2]\\nlist_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0), (0,0)]\\nlist_learning_r = [0.001,0.001,0.001,0.001,0.001]\\nlist_momentum = [0.9,0.9,0.9,0.9,0.9]\\nlist_optimizer = [\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\']\\n\\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\\nmain_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Real Test 5 -> variations des résultats selon le batch_size\n",
    "\n",
    "# CONSTANTES : nb_layers = 8, epochs = 20, lr = 0.001, momentum = 0.9, optimizer Adam, \n",
    "#  padding = same, maxpool, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0, pas de regularization, MLP 128,\n",
    "#  pool_frequency = 2, pool_frequency_change = (0,0)\n",
    "# \n",
    "\n",
    "# * 1 convnet du type batch_size = 50\n",
    "\n",
    "# * 1 convnet du type batch_size = 100\n",
    "\n",
    "# * 1 convnet du type batch_size = 150\n",
    "\n",
    "# * 1 convnet du type batch_size = 200\n",
    "\n",
    "# * 1 convnet du type batch_size = 500\n",
    "\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5']\n",
    "list_epochs = [20, 20, 20, 20, 20]\n",
    "list_batch_size = [50, 100, 150, 200, 500]\n",
    "list_nb_layers = [8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max']\n",
    "list_pool_frequency = [2, 2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0), (0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 6 -> variations des résultats selon le learning_rate\n",
    "\n",
    "# CONSTANTES : nb_layers = 8, epochs = 30, batch_size=100, momentum = 0.9, optimizer Adam, \n",
    "#  padding = same, maxpool, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0, pas de regularization, MLP 128,\n",
    "#  pool_frequency = 2, pool_frequency_change = (0,0)\n",
    "# \n",
    "#8 tests\n",
    "\n",
    "# * 1 convnet de avec lr à 0.5\n",
    "\n",
    "# * 1 convnet de avec lr à 0.1\n",
    "\n",
    "# * 1 convnet de avec lr à 0.05\n",
    "\n",
    "# * 1 convnet de avec lr à 0.01\n",
    "\n",
    "# * 1 convnet de avec lr à 0.005\n",
    "\n",
    "# * 1 convnet de avec lr à 0.001\n",
    "\n",
    "# * 1 convnet de avec lr à 0.0005\n",
    "\n",
    "# * 1 convnet de avec lr à 0.0001\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6', '7', '8']\n",
    "list_epochs = [30, 30, 30, 30, 30, 30, 30, 30]\n",
    "list_batch_size = [100, 100, 100, 100, 100, 100, 100, 100]\n",
    "list_nb_layers = [8,8,8,8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu', 'relu', 'relu', 'relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max','max','max']\n",
    "list_pool_frequency = [2, 2, 2, 2, 2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0),(0,0),(0,0),(0,0),(0,0)]\n",
    "list_learning_r = [0.5,0.1,0.05,0.01,0.005,0.001,0.0005,0.0001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIIDw6pHdJbK"
   },
   "source": [
    "### Fonctions pour préparer le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tSU6Hl2Ruv0"
   },
   "outputs": [],
   "source": [
    "# Chargement des datasets de train et de validation + one hot encoding\n",
    "def load_dataset():\n",
    "    # Chargement des données cifar10\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode encoding sur les labels\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "# Normalisation pour accroître la vitesse du modèle (en redimensionnant les pixels)\n",
    "def prep_pixels(train, test):\n",
    "    # Convertion des int en float\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # Normalisation pour avoir des nombres entre 0 et 1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # Retourner les images normalisées\n",
    "    return train_norm, test_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTdXP2D-c2D5"
   },
   "source": [
    "### Classe Python pour définir les individus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jP0CrTpRuv8"
   },
   "outputs": [],
   "source": [
    "# Classe pour les convnets\n",
    "class IndividuConvnets:\n",
    "    def __init__(self, indiv_id='1', epochs=10, batch_size=1, nb_layers=2, l1=0, l2=0, batch_norm=0,\n",
    "                 dropout=0, filters_per_layers=64, filters_double=6, MLP_end=0, activation='relu',\n",
    "                 kernel=(3,3), padding='same', max_or_avg_pool=0, pool_frequency=2,\n",
    "                 pool_frequency_change = (0,0), learning_r=0.01, momentum=0.9, optimizer='SGD'):\n",
    "        \n",
    "        # Initialisation de nos variables\n",
    "        self.time_fit = datetime.datetime.now()\n",
    "        self.my_reguralizer = None\n",
    "        \n",
    "        \n",
    "        self.nb_layers = nb_layers\n",
    "            \n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.indiv_id = indiv_id\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = dropout\n",
    "        self.filters_per_layers = filters_per_layers\n",
    "                \n",
    "        self.filters_double = filters_double\n",
    "        \n",
    "        if MLP_end < 0:\n",
    "            self.MLP_end = 0\n",
    "        else:\n",
    "            self.MLP_end = MLP_end\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.kernel = kernel\n",
    "        self.padding = padding\n",
    "        self.max_or_avg_pool = max_or_avg_pool\n",
    "        self.pool_frequency = pool_frequency\n",
    "        self.pool_frequency_change = pool_frequency_change\n",
    "        self.learning_r = learning_r\n",
    "        self.momentum = momentum\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    # ToString()\n",
    "    def __str__(self):\n",
    "        ma_liste = []\n",
    "        ma_liste.append(\"indiv_id:{},\\n \".format(self.indiv_id))\n",
    "        ma_liste.append(\"epochs:{},\\n \".format(self.epochs))\n",
    "        ma_liste.append(\"batch_size:{},\\n \".format(self.batch_size))\n",
    "        ma_liste.append(\"nb_layers:{},\\n \".format(self.nb_layers))\n",
    "        ma_liste.append(\"l1:{},\\n \".format(self.l1))\n",
    "        ma_liste.append(\"l2:{},\\n \".format(self.l2))\n",
    "        ma_liste.append(\"batch_norm:{},\\n \".format(self.batch_norm))\n",
    "        ma_liste.append(\"dropout:{},\\n \".format(self.dropout))\n",
    "        ma_liste.append(\"filters_per_layers:{},\\n \".format(self.filters_per_layers))\n",
    "        ma_liste.append(\"filters_double:{},\\n \".format(self.filters_double))\n",
    "        ma_liste.append(\"MLP_end:{},\\n \".format(self.MLP_end))\n",
    "        ma_liste.append(\"activation:{},\\n \".format(self.activation))\n",
    "        ma_liste.append(\"kernel:\\n \")\n",
    "        ma_liste.append(\"{},\\n \".format(self.kernel))\n",
    "        ma_liste.append(\"padding:{},\\n \".format(self.padding))\n",
    "        ma_liste.append(\"max_or_avg_pool:{}\\n\".format(self.max_or_avg_pool))\n",
    "        ma_liste.append(\"pool_frequency:{}\\n\".format(self.pool_frequency))\n",
    "        ma_liste.append(\"pool_frequency_change:{}\\n\".format(self.pool_frequency_change))\n",
    "        ma_liste.append(\"learning_r:{}\\n\".format(self.learning_r))\n",
    "        ma_liste.append(\"momentum:{}\\n\".format(self.momentum))\n",
    "        ma_liste.append(\"optimizer:{}\\n\".format(self.optimizer))\n",
    "            \n",
    "        return ma_liste\n",
    "    \n",
    "    # (Modele 2 conv + norm ? + pool) * X -> MLP -> softmax sortie 10 -> MODELE BLOC 2\n",
    "    # D'autres modeles seront crees par la suite\n",
    "    def create_and_train_model(self, trainX, trainY, testX, testY, main_directory):\n",
    "        start = datetime.datetime.now()\n",
    "        \n",
    "        # Choix d'un emplacement pour les logs\n",
    "        log_dir=main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\tensorboard_data\\\\\"\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        print(\"log dir = \",log_dir)\n",
    "        \n",
    "        # l1 et l2\n",
    "        if self.l1 > 0 and self.l2 > 0:\n",
    "            self.my_regularizer = regularizers.l1_l2(l1=self.l1 / self.nb_layers,\n",
    "                                    l2=self.l2 / self.nb_layers)\n",
    "        if self.l1 > 0:\n",
    "            self.my_regularizer = regularizers.l1(self.l1 / self.nb_layers)\n",
    "        elif self.l2 > 0:\n",
    "            self.my_regularizer = regularizers.l2(self.l2 / self.nb_layers)\n",
    "        else:\n",
    "            self.my_reguralizer = None\n",
    "            \n",
    "        # Definir notre modèle basique\n",
    "        model = Sequential()\n",
    "\n",
    "        # Faire toutes les convs nécessaires (conv * 2 + max pool)\n",
    "        counter_filters_double = 0 # Var pour doubler les filtres\n",
    "        counter_pool_freq = 0 # var pour savoir où placer les couches de pooling\n",
    "        counter_pool = 0 # var pour nommer les max / avg pool\n",
    "        \n",
    "        #initialisation de variables locales pour ne pas modifier nos attributs evolutifs\n",
    "        # qui doivent être log à la fin\n",
    "        pool_frequency = self.pool_frequency\n",
    "        filters_per_layers = self.filters_per_layers\n",
    "        \n",
    "        for i in range(0, self.nb_layers):\n",
    "            \n",
    "            print(\"counter_pool_freq = \", counter_pool_freq)\n",
    "            # Traitement pour doubler les filtres tous les X couches de convo\n",
    "            if counter_filters_double >= self.filters_double and self.filters_double > 0:\n",
    "                filters_per_layers = filters_per_layers * 2\n",
    "                print(\"filters = \", filters_per_layers)\n",
    "                counter_filters_double = 0\n",
    "            \n",
    "            # Première conv, on fixe l'input shape\n",
    "            if i == 0:\n",
    "                model.add(Conv2D(filters_per_layers, self.kernel, activation=self.activation,\n",
    "                    kernel_regularizer=self.my_reguralizer, padding=self.padding,\n",
    "                    input_shape=(32, 32, 3), name='conv_'+str(filters_per_layers)+'_'+str(i+1)))\n",
    "            else:\n",
    "                # Couche de conv + rajouts selon nos hyperparams\n",
    "                model.add(Conv2D(filters_per_layers, self.kernel, activation=self.activation,\n",
    "                    kernel_regularizer=self.my_reguralizer, padding=self.padding,\n",
    "                                 name='conv_'+str(filters_per_layers)+'_'+str(i+1)))\n",
    "            \n",
    "            # Après avoir créé une conv on incrémente nos compteurs (sauf counter_pool)\n",
    "            counter_filters_double = counter_filters_double + 1\n",
    "            counter_pool_freq = counter_pool_freq + 1\n",
    "            \n",
    "            # Ajouts de la regularization / du pooling selon les hyperparamètres saisis\n",
    "            \n",
    "            if self.batch_norm == 1:\n",
    "                model.add(BatchNormalization( name='batchnorm_'+str(i+1)))\n",
    "            \n",
    "            if pool_frequency == counter_pool_freq:    \n",
    "                #go max ou avg pooling\n",
    "                if self.max_or_avg_pool == 'max':\n",
    "                    model.add(MaxPooling2D((2, 2), padding='same', \n",
    "                        name='max_pool_'+str(counter_pool+1)))\n",
    "                    counter_pool = counter_pool + 1\n",
    "                else:\n",
    "                    model.add(AveragePooling2D((2, 2), padding='same', \n",
    "                                name='avg_pool_'+str(counter_pool+1)))\n",
    "                    counter_pool = counter_pool + 1\n",
    "                \n",
    "                # Dropout sur les pools \n",
    "                if self.dropout > 0:\n",
    "                    model.add(Dropout(self.dropout, \n",
    "                                      name='Dropout_'+str(self.dropout)+'_'+str(counter_pool+1)))\n",
    "                    \n",
    "                # si filters_double à -1, on double les filtres apres le pooling\n",
    "                if self.filters_double == -1:\n",
    "                    filters_per_layers = filters_per_layers * 2\n",
    "                \n",
    "                # Après avoir mis un pool, on regarde si l'on doit changer ou non \n",
    "                #  la freq d'apparition de pools\n",
    "                if counter_pool == self.pool_frequency_change[0] \\\n",
    "                    and self.pool_frequency_change[0] != 0:\n",
    "                    \n",
    "                    pool_frequency = pool_frequency + self.pool_frequency_change[1]\n",
    "                    \n",
    "                counter_pool_freq = 0\n",
    "            \n",
    "        \n",
    "        # Fin des convs -> neural network classique\n",
    "        model.add(Flatten(name='Flatten'))\n",
    "        \n",
    "        print(\"i after for = \", i)\n",
    "        #tTrain dans un MLP avant la fin si on le souhaite\n",
    "        if self.MLP_end > 0:\n",
    "            model.add(Dense(128, activation='relu', kernel_regularizer=self.my_reguralizer,\n",
    "                            name='MLP_'+str(self.MLP_end)))\n",
    "            if self.batch_norm == 1:\n",
    "                model.add(BatchNormalization( name='batchnorm_finale'))\n",
    "            \n",
    "            #mettre dropout sur les Dense, moins opti sur des pools ? à tester si tmp ok\n",
    "            #(pas conv car importantes)\n",
    "            if self.dropout > 0:\n",
    "                model.add(Dropout(self.dropout, name='Dropout_'+str(self.dropout)+'_final'))\n",
    "        \n",
    "        #notre output\n",
    "        model.add(Dense(10, activation='softmax', name='output')) \n",
    "\n",
    "        # Compiler le modele\n",
    "        if self.optimizer == 'SGD':\n",
    "            print(\"SGD, learning_r = \", self.learning_r, \" momentum = \", self.momentum, \"\\n\")\n",
    "            opt = SGD(lr=self.learning_r, momentum=self.momentum)\n",
    "        else:\n",
    "            print(\"Adam learning_r = \", self.learning_r, \" momentum = \", self.momentum, \"\\n\")\n",
    "            opt = Adam(lr=self.learning_r, beta_1=self.momentum) # beta_1 => notation momentum Adam\n",
    "        model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Entrainer le modele\n",
    "        history = model.fit(trainX, trainY, epochs=self.epochs, batch_size=self.batch_size,\n",
    "                        validation_data=(testX, testY), verbose=1, callbacks=[tensorboard_callback])\n",
    "        \n",
    "        # Garder une trace du temps nécessaire pour fit (peut être pas la meilleure méthode)\n",
    "        end = datetime.datetime.now()\n",
    "        self.time_fit = end - start\n",
    "        print(\"\\nTime for fit = \", round(self.time_fit.total_seconds(),2)) # Round total_seconds()\n",
    "\n",
    "        return history, model\n",
    "    \n",
    "    \n",
    "    def save_model(self, history, model, main_directory, current_time):\n",
    "        \n",
    "        # Sauvegarde du modèle\n",
    "        plot_model(model, \"model.png\")\n",
    "        \n",
    "        # Deplacement modele au bon endroit\n",
    "        shutil.move(os.getcwd()+\"\\\\model.png\", main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\model.png\")\n",
    "        \n",
    "        # Afficher nos résultats dans un graphique matplotlib sauvegardé\n",
    "        pyplot.gcf().subplots_adjust(hspace = 0.5)\n",
    "\n",
    "        # Afficher la loss\n",
    "        pyplot.subplot(211)\n",
    "        pyplot.title('Cross Entropy Loss')\n",
    "        pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "        pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "        \n",
    "        # Afficher l'accuracy\n",
    "        pyplot.subplot(212)\n",
    "        pyplot.title('Classification Accuracy')\n",
    "        pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "        pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "        \n",
    "        # Sauvegarde\n",
    "        filename = main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\\"\n",
    "        pyplot.savefig(filename + 'plot.png')\n",
    "        pyplot.close()\n",
    "       \n",
    "        \n",
    "        print(\"LOSS : \", round(history.history['loss'][self.epochs-1].item(), 3))\n",
    "        print(\"VAL_LOSS : \", round(history.history['val_loss'][self.epochs-1].item(), 3))\n",
    "        print(\"ACCURACY : \", round(history.history['accuracy'][self.epochs-1].item(), 3))\n",
    "        print(\"VAL_ACCURACY : \", round(history.history['val_accuracy'][self.epochs-1].item(), 3))\n",
    "        \n",
    "        # attributs pour créer les csv indivudels et le csv global\n",
    "        self.loss = round(history.history['loss'][self.epochs-1].item(), 3)\n",
    "        self.val_loss = round(history.history['val_loss'][self.epochs-1].item(), 3)\n",
    "        self.accuracy = round(history.history['accuracy'][self.epochs-1].item(), 3)\n",
    "        self.val_accuracy = round(history.history['val_accuracy'][self.epochs-1].item(), 3)\n",
    "        self.time_taken = round(self.time_fit.total_seconds(),2)\n",
    "        \n",
    "        # Créer un dataframe pandas (avec hyperparams) et le sauvegarder en CSV\n",
    "        df = pd.DataFrame({'indiv_id': [self.indiv_id],\n",
    "                           'epochs': [self.epochs],\n",
    "                           'batch_size': [self.batch_size],\n",
    "                           'nb_layers': [self.nb_layers],\n",
    "                           'l1': [self.l1],\n",
    "                           'l2': [self.l2],\n",
    "                           'batch_norm': [self.batch_norm],\n",
    "                           'dropout': [self.dropout],\n",
    "                           'filters_per_layers': [self.filters_per_layers],\n",
    "                           'filters_double': [self.filters_double],\n",
    "                           'MLP_end': [self.MLP_end],\n",
    "                           'activation': [self.activation],\n",
    "                           'kernel': [self.kernel],\n",
    "                           'padding': [self.padding],\n",
    "                           'max_or_avg_pool': [self.max_or_avg_pool],\n",
    "                           'pool_frequency': [self.pool_frequency],\n",
    "                           'pool_frequency_change': [self.pool_frequency_change],\n",
    "                           'loss': [self.loss],\n",
    "                           'val_loss': [self.val_loss],\n",
    "                           'accuracy': [self.accuracy],\n",
    "                           'val_accuracy': [self.val_accuracy],\n",
    "                           'time_taken' : [self.time_taken],\n",
    "                           'learning_r' : [self.learning_r],\n",
    "                           'momentum' : [self.momentum],\n",
    "                           'optimizer' : [self.optimizer]\n",
    "                          })\n",
    "        \n",
    "        df.to_csv(path_or_buf=filename+\"recap.csv\",index=False)\n",
    "    \n",
    "    # Lance toutes les étapes\n",
    "    def exec_indiv(self, main_directory, current_time):\n",
    "        \n",
    "        # Charger les données\n",
    "        trainX, trainY, testX, testY = load_dataset()\n",
    "        \n",
    "        # Normaliser les données\n",
    "        trainX, testX = prep_pixels(trainX, testX)\n",
    "        \n",
    "        print(\"TrainX shape = \",np.shape(trainX))\n",
    "        print(\"TestX shape = \",np.shape(testX), \"\\n\")\n",
    "        # Créer et entrainer le modele\n",
    "        history, model = self.create_and_train_model(trainX, trainY, testX, testY, main_directory)\n",
    "        \n",
    "        # Sauvegarder le modèle\n",
    "        save = self.save_model(history, model, main_directory, current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buU7Dsqfs6Cv"
   },
   "source": [
    "### Classe Python qui va démarrer les tests des neural nets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Im542kkes6Cx"
   },
   "outputs": [],
   "source": [
    "# Classe générale qui va nous servir à effectuer des actions sur des individus\n",
    "class MyTraining:\n",
    "    # Prends un ID et une liste d'individus \n",
    "    def __init__(self, id_train, indiv_list):\n",
    "        \n",
    "        self.id_train = id_train\n",
    "        self.indiv_list = indiv_list\n",
    "    \n",
    "    def train(self, main_directory, current_time):\n",
    "        \n",
    "        print(\"Start training\\n\")\n",
    "        \n",
    "        for indiv in self.indiv_list:\n",
    "            print(\"indiv \", indiv.indiv_id, \"\\n\")\n",
    "            indiv.exec_indiv(main_directory, current_time)\n",
    "            print(\"-----------------------------------------------------------------\\n\")\n",
    "        \n",
    "        # Fusion des csv \n",
    "        merge_csv = pd.DataFrame(columns=['indiv_id', 'epochs', 'nb_layers', 'l1', 'l2', 'batch_norm', 'dropout',\n",
    "                                          'filters_per_layers', 'filters_double', 'MLP_end', 'activation', 'kernel',\n",
    "                                          'padding','max_or_avg_pool', 'pool_frequency', 'pool_frequency_change', 'loss',\n",
    "                                          'val_loss', 'accuracy', 'val_accuracy', 'time_taken','learning_r',\n",
    "                                          'momentum', 'optimizer'])\n",
    "        \n",
    "        for indiv in self.indiv_list:\n",
    "            merge_csv = merge_csv.append(\n",
    "                             {'indiv_id': indiv.indiv_id, 'epochs': indiv.epochs, 'batch_size': indiv.batch_size,\n",
    "                              'nb_layers' : indiv.nb_layers,'l1' : indiv.l1, 'l2' : indiv.l2, 'batch_norm': indiv.batch_norm,\n",
    "                              'dropout' : indiv.dropout,'filters_per_layers' : indiv.filters_per_layers,\n",
    "                              'filters_double' : indiv.filters_double,'MLP_end' : indiv.MLP_end,\n",
    "                              'activation' : indiv.activation,'kernel' : indiv.kernel,'padding' : indiv.padding,\n",
    "                              'max_or_avg_pool' : indiv.max_or_avg_pool,'pool_frequency' : indiv.pool_frequency,\n",
    "                              'pool_frequency_change' : indiv.pool_frequency_change,'loss' : indiv.loss,\n",
    "                              'val_loss' : indiv.val_loss,'accuracy' : indiv.accuracy, 'val_accuracy' : indiv.val_accuracy,\n",
    "                              'time_taken' : indiv.time_taken,'learning_r' : indiv.learning_r,'momentum': indiv.momentum,\n",
    "                              'optimizer' : indiv.optimizer\n",
    "                             },ignore_index=True)\n",
    "        \n",
    "        # sauvegarde\n",
    "        merge_csv.to_csv(main_directory+\"\\\\combined_recap.csv\", index=False)\n",
    "            \n",
    "    \n",
    "    def all_indiv(self):\n",
    "        \n",
    "        # Affiche les caractéristiques de l'ensemble des individus\n",
    "        for indiv in self.indiv_list:\n",
    "            print('\\n'.join(indiv.__str__()))\n",
    "            for tir in range(80): print('-', end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIGBnz_4bj74"
   },
   "source": [
    "### Traitement général (train de l'ensemble des modèles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjIV_jF6RuwF",
    "outputId": "6759a6e1-3664-4264-c87d-18266651af09",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indiv_id:1,\n",
      " \n",
      "epochs:30,\n",
      " \n",
      "batch_size:100,\n",
      " \n",
      "nb_layers:8,\n",
      " \n",
      "l1:0,\n",
      " \n",
      "l2:0,\n",
      " \n",
      "batch_norm:0,\n",
      " \n",
      "dropout:0,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:128,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:max\n",
      "\n",
      "pool_frequency:2\n",
      "\n",
      "pool_frequency_change:(0, 0)\n",
      "\n",
      "learning_r:0.5\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "indiv_id:2,\n",
      " \n",
      "epochs:30,\n",
      " \n",
      "batch_size:100,\n",
      " \n",
      "nb_layers:8,\n",
      " \n",
      "l1:0,\n",
      " \n",
      "l2:0,\n",
      " \n",
      "batch_norm:0,\n",
      " \n",
      "dropout:0,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:128,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:max\n",
      "\n",
      "pool_frequency:2\n",
      "\n",
      "pool_frequency_change:(0, 0)\n",
      "\n",
      "learning_r:0.1\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "indiv_id:3,\n",
      " \n",
      "epochs:30,\n",
      " \n",
      "batch_size:100,\n",
      " \n",
      "nb_layers:8,\n",
      " \n",
      "l1:0,\n",
      " \n",
      "l2:0,\n",
      " \n",
      "batch_norm:0,\n",
      " \n",
      "dropout:0,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:128,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:max\n",
      "\n",
      "pool_frequency:2\n",
      "\n",
      "pool_frequency_change:(0, 0)\n",
      "\n",
      "learning_r:0.05\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "indiv_id:4,\n",
      " \n",
      "epochs:30,\n",
      " \n",
      "batch_size:100,\n",
      " \n",
      "nb_layers:8,\n",
      " \n",
      "l1:0,\n",
      " \n",
      "l2:0,\n",
      " \n",
      "batch_norm:0,\n",
      " \n",
      "dropout:0,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:128,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:max\n",
      "\n",
      "pool_frequency:2\n",
      "\n",
      "pool_frequency_change:(0, 0)\n",
      "\n",
      "learning_r:0.01\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "indiv_id:5,\n",
      " \n",
      "epochs:30,\n",
      " \n",
      "batch_size:100,\n",
      " \n",
      "nb_layers:8,\n",
      " \n",
      "l1:0,\n",
      " \n",
      "l2:0,\n",
      " \n",
      "batch_norm:0,\n",
      " \n",
      "dropout:0,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:128,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:max\n",
      "\n",
      "pool_frequency:2\n",
      "\n",
      "pool_frequency_change:(0, 0)\n",
      "\n",
      "learning_r:0.005\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "indiv_id:6,\n",
      " \n",
      "epochs:30,\n",
      " \n",
      "batch_size:100,\n",
      " \n",
      "nb_layers:8,\n",
      " \n",
      "l1:0,\n",
      " \n",
      "l2:0,\n",
      " \n",
      "batch_norm:0,\n",
      " \n",
      "dropout:0,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:128,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:max\n",
      "\n",
      "pool_frequency:2\n",
      "\n",
      "pool_frequency_change:(0, 0)\n",
      "\n",
      "learning_r:0.001\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "indiv_id:7,\n",
      " \n",
      "epochs:30,\n",
      " \n",
      "batch_size:100,\n",
      " \n",
      "nb_layers:8,\n",
      " \n",
      "l1:0,\n",
      " \n",
      "l2:0,\n",
      " \n",
      "batch_norm:0,\n",
      " \n",
      "dropout:0,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:128,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:max\n",
      "\n",
      "pool_frequency:2\n",
      "\n",
      "pool_frequency_change:(0, 0)\n",
      "\n",
      "learning_r:0.0005\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "indiv_id:8,\n",
      " \n",
      "epochs:30,\n",
      " \n",
      "batch_size:100,\n",
      " \n",
      "nb_layers:8,\n",
      " \n",
      "l1:0,\n",
      " \n",
      "l2:0,\n",
      " \n",
      "batch_norm:0,\n",
      " \n",
      "dropout:0,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:128,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:max\n",
      "\n",
      "pool_frequency:2\n",
      "\n",
      "pool_frequency_change:(0, 0)\n",
      "\n",
      "learning_r:0.0001\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start training\n",
      "\n",
      "indiv  1 \n",
      "\n",
      "TrainX shape =  (50000, 32, 32, 3)\n",
      "TestX shape =  (10000, 32, 32, 3) \n",
      "\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\logs\\convnets\\logs_2020-02-03-21-25\\log_1\\tensorboard_data\\\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "i after for =  7\n",
      "Adam learning_r =  0.5  momentum =  0.9 \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "50000/50000 [==============================] - 21s 418us/sample - loss: 27901335986034.5469 - accuracy: 0.0990 - val_loss: 2.3502 - val_accuracy: 0.1000\n",
      "Epoch 2/30\n",
      "50000/50000 [==============================] - 17s 335us/sample - loss: 2.3295 - accuracy: 0.0971 - val_loss: 2.3199 - val_accuracy: 0.1000\n",
      "Epoch 3/30\n",
      "50000/50000 [==============================] - 18s 362us/sample - loss: 2.3300 - accuracy: 0.1007 - val_loss: 2.3313 - val_accuracy: 0.1000\n",
      "Epoch 4/30\n",
      "50000/50000 [==============================] - 19s 385us/sample - loss: 2.3324 - accuracy: 0.1009 - val_loss: 2.3328 - val_accuracy: 0.1000\n",
      "Epoch 5/30\n",
      "50000/50000 [==============================] - 22s 436us/sample - loss: 2.3313 - accuracy: 0.1021 - val_loss: 2.3360 - val_accuracy: 0.1000\n",
      "Epoch 6/30\n",
      "50000/50000 [==============================] - 21s 420us/sample - loss: 2.3339 - accuracy: 0.0996 - val_loss: 2.3461 - val_accuracy: 0.1000\n",
      "Epoch 7/30\n",
      "50000/50000 [==============================] - 20s 409us/sample - loss: 2.3310 - accuracy: 0.1024 - val_loss: 2.3369 - val_accuracy: 0.1000\n",
      "Epoch 8/30\n",
      "50000/50000 [==============================] - 21s 414us/sample - loss: 2.3304 - accuracy: 0.0993 - val_loss: 2.3185 - val_accuracy: 0.1000\n",
      "Epoch 9/30\n",
      "50000/50000 [==============================] - 21s 418us/sample - loss: 2.3344 - accuracy: 0.0985 - val_loss: 2.3152 - val_accuracy: 0.1000\n",
      "Epoch 10/30\n",
      "50000/50000 [==============================] - 22s 431us/sample - loss: 2.3300 - accuracy: 0.1037 - val_loss: 2.3262 - val_accuracy: 0.1000\n",
      "Epoch 11/30\n",
      "50000/50000 [==============================] - 23s 464us/sample - loss: 2.3285 - accuracy: 0.0993 - val_loss: 2.3246 - val_accuracy: 0.1000- loss: 2.3283 \n",
      "Epoch 12/30\n",
      "50000/50000 [==============================] - 23s 451us/sample - loss: 2.3317 - accuracy: 0.0995 - val_loss: 2.3405 - val_accuracy: 0.1000\n",
      "Epoch 13/30\n",
      "50000/50000 [==============================] - 21s 426us/sample - loss: 2.3327 - accuracy: 0.0989 - val_loss: 2.3480 - val_accuracy: 0.1000\n",
      "Epoch 14/30\n",
      "50000/50000 [==============================] - 21s 430us/sample - loss: 2.3340 - accuracy: 0.0985 - val_loss: 2.3672 - val_accuracy: 0.1000\n",
      "Epoch 15/30\n",
      "50000/50000 [==============================] - 21s 429us/sample - loss: 2.3337 - accuracy: 0.1002 - val_loss: 2.3496 - val_accuracy: 0.1000\n",
      "Epoch 16/30\n",
      "50000/50000 [==============================] - 22s 438us/sample - loss: 2.3328 - accuracy: 0.0996 - val_loss: 2.3207 - val_accuracy: 0.1000\n",
      "Epoch 17/30\n",
      "50000/50000 [==============================] - 22s 435us/sample - loss: 2.3326 - accuracy: 0.0989 - val_loss: 2.3449 - val_accuracy: 0.1000\n",
      "Epoch 18/30\n",
      "50000/50000 [==============================] - 22s 444us/sample - loss: 2.3349 - accuracy: 0.1004 - val_loss: 2.3271 - val_accuracy: 0.1000\n",
      "Epoch 19/30\n",
      "50000/50000 [==============================] - 22s 438us/sample - loss: 2.3325 - accuracy: 0.0985 - val_loss: 2.3563 - val_accuracy: 0.1000\n",
      "Epoch 20/30\n",
      "50000/50000 [==============================] - 22s 441us/sample - loss: 2.3330 - accuracy: 0.1027 - val_loss: 2.3379 - val_accuracy: 0.1000\n",
      "Epoch 21/30\n",
      "50000/50000 [==============================] - 22s 442us/sample - loss: 2.3319 - accuracy: 0.1006 - val_loss: 2.3253 - val_accuracy: 0.1000\n",
      "Epoch 22/30\n",
      "50000/50000 [==============================] - 22s 444us/sample - loss: 2.3336 - accuracy: 0.1000 - val_loss: 2.3405 - val_accuracy: 0.1000\n",
      "Epoch 23/30\n",
      "50000/50000 [==============================] - 23s 461us/sample - loss: 2.3294 - accuracy: 0.1010 - val_loss: 2.3231 - val_accuracy: 0.1000\n",
      "Epoch 27/30\n",
      "50000/50000 [==============================] - 23s 453us/sample - loss: 2.3307 - accuracy: 0.0997 - val_loss: 2.3369 - val_accuracy: 0.1000\n",
      "Epoch 28/30\n",
      "50000/50000 [==============================] - 22s 444us/sample - loss: 2.3328 - accuracy: 0.1001 - val_loss: 2.3504 - val_accuracy: 0.1000\n",
      "Epoch 29/30\n",
      "50000/50000 [==============================] - 23s 460us/sample - loss: 2.3358 - accuracy: 0.0991 - val_loss: 2.3313 - val_accuracy: 0.1000\n",
      "Epoch 30/30\n",
      "50000/50000 [==============================] - 23s 457us/sample - loss: 2.3336 - accuracy: 0.1011 - val_loss: 2.3412 - val_accuracy: 0.1000\n",
      "\n",
      "Time for fit =  648.71\n",
      "LOSS :  2.334\n",
      "VAL_LOSS :  2.341\n",
      "ACCURACY :  0.101\n",
      "VAL_ACCURACY :  0.1\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "indiv  2 \n",
      "\n",
      "TrainX shape =  (50000, 32, 32, 3)\n",
      "TestX shape =  (10000, 32, 32, 3) \n",
      "\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\logs\\convnets\\logs_2020-02-03-21-25\\log_2\\tensorboard_data\\\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i after for =  7\n",
      "Adam learning_r =  0.1  momentum =  0.9 \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "  100/50000 [..............................] - ETA: 17:29 - loss: 2.3026 - accuracy: 0.1000WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.110204). Check your callbacks.\n",
      "50000/50000 [==============================] - 27s 544us/sample - loss: 646169.8201 - accuracy: 0.0987 - val_loss: 2.3055 - val_accuracy: 0.1000\n",
      "Epoch 2/30\n",
      "50000/50000 [==============================] - 30s 598us/sample - loss: 2.3065 - accuracy: 0.0975 - val_loss: 2.3065 - val_accuracy: 0.1000\n",
      "Epoch 3/30\n",
      "50000/50000 [==============================] - 27s 535us/sample - loss: 2.3101 - accuracy: 0.0998 - val_loss: 2.3052 - val_accuracy: 0.1000\n",
      "Epoch 7/30\n",
      "50000/50000 [==============================] - 30s 597us/sample - loss: 2.3097 - accuracy: 0.0990 - val_loss: 2.3041 - val_accuracy: 0.1000\n",
      "Epoch 8/30\n",
      "50000/50000 [==============================] - 27s 539us/sample - loss: 2.3088 - accuracy: 0.1006 - val_loss: 2.3067 - val_accuracy: 0.1000\n",
      "Epoch 9/30\n",
      "50000/50000 [==============================] - 25s 493us/sample - loss: 2.3096 - accuracy: 0.0996 - val_loss: 2.3086 - val_accuracy: 0.1000\n",
      "Epoch 10/30\n",
      "50000/50000 [==============================] - 26s 526us/sample - loss: 2.3102 - accuracy: 0.0983 - val_loss: 2.3116 - val_accuracy: 0.1000\n",
      "Epoch 11/30\n",
      "50000/50000 [==============================] - 33s 656us/sample - loss: 2.3106 - accuracy: 0.0998 - val_loss: 2.3076 - val_accuracy: 0.1000\n",
      "Epoch 12/30\n",
      "50000/50000 [==============================] - 27s 534us/sample - loss: 2.3095 - accuracy: 0.0978 - val_loss: 2.3064 - val_accuracy: 0.1000\n",
      "Epoch 13/30\n",
      "50000/50000 [==============================] - 28s 565us/sample - loss: 2.3094 - accuracy: 0.0997 - val_loss: 2.3055 - val_accuracy: 0.1000\n",
      "Epoch 14/30\n",
      "50000/50000 [==============================] - 27s 539us/sample - loss: 2.3096 - accuracy: 0.1006 - val_loss: 2.3096 - val_accuracy: 0.1000ccurac - ETA: 2s - loss: 2.3098 - ac - ETA: 2s - loss: 2.3097  - ETA: 1s - loss: 2.3\n",
      "Epoch 19/30\n",
      "50000/50000 [==============================] - 25s 506us/sample - loss: 2.3095 - accuracy: 0.1011 - val_loss: 2.3070 - val_accuracy: 0.1000\n",
      "Epoch 20/30\n",
      "50000/50000 [==============================] - 25s 495us/sample - loss: 2.3096 - accuracy: 0.1004 - val_loss: 2.3096 - val_accuracy: 0.1000\n",
      "Epoch 21/30\n",
      "50000/50000 [==============================] - 26s 511us/sample - loss: 2.3093 - accuracy: 0.0986 - val_loss: 2.3083 - val_accuracy: 0.1000\n",
      "Epoch 22/30\n",
      "50000/50000 [==============================] - 25s 508us/sample - loss: 2.3097 - accuracy: 0.1014 - val_loss: 2.3084 - val_accuracy: 0.1000\n",
      "Epoch 23/30\n",
      "50000/50000 [==============================] - 34s 686us/sample - loss: 2.3094 - accuracy: 0.0998 - val_loss: 2.3105 - val_accuracy: 0.1000\n",
      "Epoch 24/30\n",
      "50000/50000 [==============================] - 48s 970us/sample - loss: 2.3093 - accuracy: 0.1007 - val_loss: 2.3089 - val_accuracy: 0.1000\n",
      "Epoch 25/30\n",
      "50000/50000 [==============================] - 46s 921us/sample - loss: 2.3104 - accuracy: 0.0989 - val_loss: 2.3091 - val_accuracy: 0.1000\n",
      "Epoch 26/30\n",
      "50000/50000 [==============================] - 47s 949us/sample - loss: 2.3061 - accuracy: 0.0992 - val_loss: 2.3059 - val_accuracy: 0.1000\n",
      "Epoch 9/30\n",
      "50000/50000 [==============================] - 28s 567us/sample - loss: 2.3064 - accuracy: 0.0987 - val_loss: 2.3046 - val_accuracy: 0.1000\n",
      "Epoch 10/30\n",
      "50000/50000 [==============================] - 24s 476us/sample - loss: 2.3065 - accuracy: 0.1006 - val_loss: 2.3089 - val_accuracy: 0.1000\n",
      "Epoch 11/30\n",
      "50000/50000 [==============================] - 24s 478us/sample - loss: 2.3068 - accuracy: 0.0988 - val_loss: 2.3052 - val_accuracy: 0.1000\n",
      "Epoch 12/30\n",
      "50000/50000 [==============================] - 26s 512us/sample - loss: 2.3060 - accuracy: 0.0996 - val_loss: 2.3050 - val_accuracy: 0.1000\n",
      "Epoch 13/30\n",
      "50000/50000 [==============================] - 24s 472us/sample - loss: 2.3065 - accuracy: 0.0972 - val_loss: 2.3060 - val_accuracy: 0.1000\n",
      "Epoch 14/30\n",
      "50000/50000 [==============================] - 28s 557us/sample - loss: 2.3065 - accuracy: 0.1001 - val_loss: 2.3063 - val_accuracy: 0.1000\n",
      "Epoch 15/30\n",
      "50000/50000 [==============================] - 27s 536us/sample - loss: 2.3060 - accuracy: 0.0968 - val_loss: 2.3063 - val_accuracy: 0.1000\n",
      "Epoch 16/30\n",
      "50000/50000 [==============================] - 24s 482us/sample - loss: 2.3067 - accuracy: 0.0989 - val_loss: 2.3076 - val_accuracy: 0.1000 18s - loss: 2.3064 - accuracy:\n",
      "Epoch 24/30\n",
      "50000/50000 [==============================] - 23s 469us/sample - loss: 2.3059 - accuracy: 0.1037 - val_loss: 2.3084 - val_accuracy: 0.1000\n",
      "Epoch 25/30\n",
      "50000/50000 [==============================] - 24s 486us/sample - loss: 2.3061 - accuracy: 0.0988 - val_loss: 2.3046 - val_accuracy: 0.1000\n",
      "Epoch 26/30\n",
      "50000/50000 [==============================] - 25s 500us/sample - loss: 2.3062 - accuracy: 0.1028 - val_loss: 2.3043 - val_accuracy: 0.1000\n",
      "Epoch 27/30\n",
      "50000/50000 [==============================] - 25s 505us/sample - loss: 2.3067 - accuracy: 0.0989 - val_loss: 2.3064 - val_accuracy: 0.1000\n",
      "Epoch 28/30\n",
      "50000/50000 [==============================] - 25s 493us/sample - loss: 2.3064 - accuracy: 0.1029 - val_loss: 2.3069 - val_accuracy: 0.1000\n",
      "Epoch 29/30\n",
      "50000/50000 [==============================] - 24s 477us/sample - loss: 2.3061 - accuracy: 0.1001 - val_loss: 2.3051 - val_accuracy: 0.1000\n",
      "Epoch 30/30\n",
      "50000/50000 [==============================] - 24s 475us/sample - loss: 2.3062 - accuracy: 0.1008 - val_loss: 2.3072 - val_accuracy: 0.1000\n",
      "\n",
      "Time for fit =  839.95\n",
      "LOSS :  2.306\n",
      "VAL_LOSS :  2.307\n",
      "ACCURACY :  0.101\n",
      "VAL_ACCURACY :  0.1\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "indiv  4 \n",
      "\n",
      "TrainX shape =  (50000, 32, 32, 3)\n",
      "TestX shape =  (10000, 32, 32, 3) \n",
      "\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\logs\\convnets\\logs_2020-02-03-21-25\\log_4\\tensorboard_data\\\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "i after for =  7\n",
      "Adam learning_r =  0.01  momentum =  0.9 \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "  100/50000 [..............................] - ETA: 18:59 - loss: 2.3036 - accuracy: 0.0900WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.174228). Check your callbacks.\n",
      "50000/50000 [==============================] - 26s 528us/sample - loss: 2.3338 - accuracy: 0.0991 - val_loss: 2.3031 - val_accuracy: 0.1000\n",
      "Epoch 2/30\n",
      "50000/50000 [==============================] - 24s 471us/sample - loss: 2.3035 - accuracy: 0.0998 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 3/30\n",
      "50000/50000 [==============================] - 23s 463us/sample - loss: 2.3033 - accuracy: 0.0983 - val_loss: 2.3035 - val_accuracy: 0.1000\n",
      "Epoch 4/30\n",
      "50000/50000 [==============================] - 24s 472us/sample - loss: 2.3034 - accuracy: 0.1001 - val_loss: 2.3031 - val_accuracy: 0.1000\n",
      "Epoch 11/30\n",
      "50000/50000 [==============================] - 24s 478us/sample - loss: 2.3034 - accuracy: 0.1006 - val_loss: 2.3036 - val_accuracy: 0.1000\n",
      "Epoch 12/30\n",
      "50000/50000 [==============================] - 23s 463us/sample - loss: 2.3035 - accuracy: 0.0995 - val_loss: 2.3032 - val_accuracy: 0.1000\n",
      "Epoch 13/30\n",
      "50000/50000 [==============================] - 24s 472us/sample - loss: 2.3035 - accuracy: 0.0991 - val_loss: 2.3029 - val_accuracy: 0.1000\n",
      "Epoch 14/30\n",
      "50000/50000 [==============================] - 26s 521us/sample - loss: 2.3034 - accuracy: 0.0990 - val_loss: 2.3029 - val_accuracy: 0.1000\n",
      "Epoch 15/30\n",
      "50000/50000 [==============================] - 24s 474us/sample - loss: 2.3033 - accuracy: 0.1010 - val_loss: 2.3030 - val_accuracy: 0.1000\n",
      "Epoch 16/30\n",
      "50000/50000 [==============================] - 25s 508us/sample - loss: 2.3035 - accuracy: 0.0997 - val_loss: 2.3030 - val_accuracy: 0.1000\n",
      "Epoch 17/30\n",
      "50000/50000 [==============================] - 24s 472us/sample - loss: 2.3033 - accuracy: 0.0992 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 23s 462us/sample - loss: 2.3035 - accuracy: 0.0988 - val_loss: 2.3031 - val_accuracy: 0.1000\n",
      "Epoch 23/30\n",
      "50000/50000 [==============================] - 23s 462us/sample - loss: 2.3033 - accuracy: 0.0990 - val_loss: 2.3032 - val_accuracy: 0.1000\n",
      "Epoch 24/30\n",
      "50000/50000 [==============================] - 24s 475us/sample - loss: 2.3033 - accuracy: 0.1004 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 28/30\n",
      "50000/50000 [==============================] - 23s 465us/sample - loss: 2.3034 - accuracy: 0.1001 - val_loss: 2.3032 - val_accuracy: 0.1000\n",
      "Epoch 29/30\n",
      "50000/50000 [==============================] - 23s 468us/sample - loss: 2.3034 - accuracy: 0.0993 - val_loss: 2.3030 - val_accuracy: 0.1000\n",
      "Epoch 30/30\n",
      "50000/50000 [==============================] - 25s 493us/sample - loss: 2.3034 - accuracy: 0.0984 - val_loss: 2.3039 - val_accuracy: 0.1000\n",
      "\n",
      "Time for fit =  729.94\n",
      "LOSS :  2.303\n",
      "VAL_LOSS :  2.304\n",
      "ACCURACY :  0.098\n",
      "VAL_ACCURACY :  0.1\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "indiv  5 \n",
      "\n",
      "TrainX shape =  (50000, 32, 32, 3)\n",
      "TestX shape =  (10000, 32, 32, 3) \n",
      "\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\logs\\convnets\\logs_2020-02-03-21-25\\log_5\\tensorboard_data\\\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "i after for =  7\n",
      "Adam learning_r =  0.005  momentum =  0.9 \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "  100/50000 [..............................] - ETA: 21:49 - loss: 2.3035 - accuracy: 0.0400WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.131092). Check your callbacks.\n",
      "50000/50000 [==============================] - 27s 532us/sample - loss: 1.9090 - accuracy: 0.2886 - val_loss: 1.6863 - val_accuracy: 0.3705\n",
      "Epoch 2/30\n",
      "50000/50000 [==============================] - 25s 496us/sample - loss: 1.5482 - accuracy: 0.4290 - val_loss: 1.4543 - val_accuracy: 0.4639\n",
      "Epoch 3/30\n",
      "50000/50000 [==============================] - 24s 486us/sample - loss: 1.4170 - accuracy: 0.4837 - val_loss: 1.3495 - val_accuracy: 0.5100\n",
      "Epoch 4/30\n",
      "50000/50000 [==============================] - 27s 534us/sample - loss: 1.3414 - accuracy: 0.5167 - val_loss: 1.3412 - val_accuracy: 0.5164\n",
      "Epoch 5/30\n",
      "50000/50000 [==============================] - 25s 496us/sample - loss: 1.1926 - accuracy: 0.5736 - val_loss: 1.2438 - val_accuracy: 0.5612\n",
      "Epoch 9/30\n",
      "50000/50000 [==============================] - 25s 501us/sample - loss: 1.1663 - accuracy: 0.5838 - val_loss: 1.2696 - val_accuracy: 0.5503\n",
      "Epoch 10/30\n",
      "50000/50000 [==============================] - 28s 557us/sample - loss: 1.1563 - accuracy: 0.5885 - val_loss: 1.1973 - val_accuracy: 0.5763\n",
      "Epoch 11/30\n",
      "50000/50000 [==============================] - 25s 501us/sample - loss: 1.1062 - accuracy: 0.6079 - val_loss: 1.1959 - val_accuracy: 0.5866\n",
      "Epoch 15/30\n",
      "50000/50000 [==============================] - 25s 491us/sample - loss: 1.0862 - accuracy: 0.6153 - val_loss: 1.1645 - val_accuracy: 0.5977\n",
      "Epoch 16/30\n",
      "50000/50000 [==============================] - 25s 503us/sample - loss: 1.0790 - accuracy: 0.6161 - val_loss: 1.1657 - val_accuracy: 0.5883\n",
      "Epoch 17/30\n",
      "50000/50000 [==============================] - 25s 502us/sample - loss: 1.0642 - accuracy: 0.6235 - val_loss: 1.1402 - val_accuracy: 0.6026\n",
      "Epoch 18/30\n",
      "50000/50000 [==============================] - 25s 491us/sample - loss: 1.0395 - accuracy: 0.6334 - val_loss: 1.1358 - val_accuracy: 0.6022\n",
      "Epoch 21/30\n",
      "50000/50000 [==============================] - 24s 481us/sample - loss: 1.0360 - accuracy: 0.6335 - val_loss: 1.1483 - val_accuracy: 0.6003\n",
      "Epoch 22/30\n",
      "50000/50000 [==============================] - 24s 480us/sample - loss: 1.0310 - accuracy: 0.6359 - val_loss: 1.1537 - val_accuracy: 0.5993\n",
      "Epoch 23/30\n",
      "50000/50000 [==============================] - 24s 482us/sample - loss: 1.0234 - accuracy: 0.6402 - val_loss: 1.0949 - val_accuracy: 0.6228\n",
      "Epoch 24/30\n",
      "50000/50000 [==============================] - 30s 592us/sample - loss: 1.0028 - accuracy: 0.6474 - val_loss: 1.1080 - val_accuracy: 0.6165\n",
      "Epoch 27/30\n",
      "50000/50000 [==============================] - 28s 563us/sample - loss: 1.0011 - accuracy: 0.6465 - val_loss: 1.0855 - val_accuracy: 0.6202\n",
      "Epoch 28/30\n",
      "50000/50000 [==============================] - 31s 618us/sample - loss: 0.9799 - accuracy: 0.6540 - val_loss: 1.1133 - val_accuracy: 0.6151\n",
      "Epoch 29/30\n",
      "50000/50000 [==============================] - 30s 593us/sample - loss: 1.1574 - accuracy: 0.5804 - val_loss: 0.9985 - val_accuracy: 0.6410\n",
      "Epoch 3/30\n",
      "50000/50000 [==============================] - 33s 667us/sample - loss: 0.9224 - accuracy: 0.6714 - val_loss: 0.8602 - val_accuracy: 0.6969\n",
      "Epoch 4/30\n",
      "50000/50000 [==============================] - 31s 611us/sample - loss: 0.5337 - accuracy: 0.8135 - val_loss: 0.6866 - val_accuracy: 0.7668\n",
      "Epoch 8/30\n",
      "50000/50000 [==============================] - 29s 582us/sample - loss: 0.4676 - accuracy: 0.8368 - val_loss: 0.7065 - val_accuracy: 0.7654\n",
      "Epoch 9/30\n",
      "50000/50000 [==============================] - 31s 623us/sample - loss: 0.4213 - accuracy: 0.8529 - val_loss: 0.6771 - val_accuracy: 0.7760\n",
      "Epoch 10/30\n",
      "50000/50000 [==============================] - 34s 685us/sample - loss: 0.3747 - accuracy: 0.8686 - val_loss: 0.6947 - val_accuracy: 0.7819\n",
      "Epoch 11/30\n",
      "50000/50000 [==============================] - 29s 585us/sample - loss: 0.2149 - accuracy: 0.9239 - val_loss: 0.8824 - val_accuracy: 0.7749\n",
      "Epoch 16/30\n",
      "50000/50000 [==============================] - 31s 611us/sample - loss: 0.1970 - accuracy: 0.9298 - val_loss: 0.9542 - val_accuracy: 0.7743\n",
      "Epoch 17/30\n",
      "50000/50000 [==============================] - 31s 613us/sample - loss: 0.1851 - accuracy: 0.9344 - val_loss: 0.9428 - val_accuracy: 0.7688\n",
      "Epoch 18/30\n",
      "50000/50000 [==============================] - 33s 664us/sample - loss: 0.1818 - accuracy: 0.9354 - val_loss: 1.0363 - val_accuracy: 0.7643\n",
      "Epoch 19/30\n",
      "50000/50000 [==============================] - 30s 595us/sample - loss: 0.1211 - accuracy: 0.9581 - val_loss: 1.2099 - val_accuracy: 0.7644\n",
      "Epoch 28/30\n",
      "50000/50000 [==============================] - 31s 623us/sample - loss: 0.1181 - accuracy: 0.9597 - val_loss: 1.1599 - val_accuracy: 0.7711\n",
      "Epoch 29/30\n",
      "50000/50000 [==============================] - 32s 637us/sample - loss: 0.1147 - accuracy: 0.9602 - val_loss: 1.2782 - val_accuracy: 0.7676\n",
      "Epoch 30/30\n",
      "50000/50000 [==============================] - 37s 745us/sample - loss: 0.1020 - accuracy: 0.9646 - val_loss: 1.2385 - val_accuracy: 0.7717\n",
      "\n",
      "Time for fit =  938.64\n",
      "LOSS :  0.102\n",
      "VAL_LOSS :  1.239\n",
      "ACCURACY :  0.965\n",
      "VAL_ACCURACY :  0.772\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "indiv  7 \n",
      "\n",
      "TrainX shape =  (50000, 32, 32, 3)\n",
      "TestX shape =  (10000, 32, 32, 3) \n",
      "\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\logs\\convnets\\logs_2020-02-03-21-25\\log_7\\tensorboard_data\\\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "i after for =  7\n",
      "Adam learning_r =  0.0005  momentum =  0.9 \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "  100/50000 [..............................] - ETA: 24:02 - loss: 2.3018 - accuracy: 0.1000WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.147609). Check your callbacks.\n",
      "50000/50000 [==============================] - 35s 699us/sample - loss: 1.6805 - accuracy: 0.3768 - val_loss: 1.4138 - val_accuracy: 0.4888\n",
      "Epoch 2/30\n",
      "50000/50000 [==============================] - 31s 628us/sample - loss: 0.4271 - accuracy: 0.8493 - val_loss: 0.7234 - val_accuracy: 0.7631\n",
      "Epoch 11/30\n",
      "50000/50000 [==============================] - 32s 644us/sample - loss: 0.3749 - accuracy: 0.8670 - val_loss: 0.8023 - val_accuracy: 0.7610\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 33s 650us/sample - loss: 0.3352 - accuracy: 0.8808 - val_loss: 0.7389 - val_accuracy: 0.7770\n",
      "Epoch 13/30\n",
      "50000/50000 [==============================] - 31s 615us/sample - loss: 0.2896 - accuracy: 0.8971 - val_loss: 0.8332 - val_accuracy: 0.7670\n",
      "Epoch 14/30\n",
      "50000/50000 [==============================] - 41s 824us/sample - loss: 0.0862 - accuracy: 0.9687 - val_loss: 1.3203 - val_accuracy: 0.7670\n",
      "Epoch 26/30\n",
      "50000/50000 [==============================] - 32s 642us/sample - loss: 0.0929 - accuracy: 0.9671 - val_loss: 1.2251 - val_accuracy: 0.7655y\n",
      "Epoch 27/30\n",
      "50000/50000 [==============================] - 33s 664us/sample - loss: 0.0901 - accuracy: 0.9680 - val_loss: 1.2705 - val_accuracy: 0.7628\n",
      "Epoch 28/30\n",
      "50000/50000 [==============================] - 32s 644us/sample - loss: 0.0870 - accuracy: 0.9693 - val_loss: 1.2124 - val_accuracy: 0.7699\n",
      "Epoch 29/30\n",
      "50000/50000 [==============================] - 34s 683us/sample - loss: 0.0810 - accuracy: 0.9712 - val_loss: 1.3835 - val_accuracy: 0.7576\n",
      "Epoch 30/30\n",
      "50000/50000 [==============================] - 41s 823us/sample - loss: 0.0788 - accuracy: 0.9719 - val_loss: 1.4927 - val_accuracy: 0.7558: 0.0788 - accuracy: \n",
      "\n",
      "Time for fit =  1028.13\n",
      "LOSS :  0.079\n",
      "VAL_LOSS :  1.493\n",
      "ACCURACY :  0.972\n",
      "VAL_ACCURACY :  0.756\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "indiv  8 \n",
      "\n",
      "TrainX shape =  (50000, 32, 32, 3)\n",
      "TestX shape =  (10000, 32, 32, 3) \n",
      "\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\logs\\convnets\\logs_2020-02-03-21-25\\log_8\\tensorboard_data\\\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "i after for =  7\n",
      "Adam learning_r =  0.0001  momentum =  0.9 \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "  100/50000 [..............................] - ETA: 32:44 - loss: 2.3034 - accuracy: 0.0800WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.194479). Check your callbacks.\n",
      "50000/50000 [==============================] - 35s 704us/sample - loss: 1.8182 - accuracy: 0.3264 - val_loss: 1.5730 - val_accuracy: 0.4194\n",
      "Epoch 2/30\n",
      "50000/50000 [==============================] - 32s 642us/sample - loss: 1.5134 - accuracy: 0.4463 - val_loss: 1.4660 - val_accuracy: 0.4597\n",
      "Epoch 3/30\n",
      "50000/50000 [==============================] - 37s 732us/sample - loss: 1.3819 - accuracy: 0.4991 - val_loss: 1.3207 - val_accuracy: 0.5270\n",
      "Epoch 4/30\n",
      "50000/50000 [==============================] - 37s 735us/sample - loss: 0.8900 - accuracy: 0.6865 - val_loss: 0.9546 - val_accuracy: 0.6579\n",
      "Epoch 14/30\n",
      "50000/50000 [==============================] - 35s 705us/sample - loss: 0.8637 - accuracy: 0.6994 - val_loss: 0.9790 - val_accuracy: 0.6497\n",
      "Epoch 15/30\n",
      "50000/50000 [==============================] - 35s 706us/sample - loss: 0.8437 - accuracy: 0.7058 - val_loss: 0.9623 - val_accuracy: 0.6571\n",
      "Epoch 16/30\n",
      "50000/50000 [==============================] - 34s 690us/sample - loss: 0.8161 - accuracy: 0.7155 - val_loss: 0.9396 - val_accuracy: 0.6722\n",
      "Epoch 17/30\n",
      "50000/50000 [==============================] - 31s 622us/sample - loss: 0.7886 - accuracy: 0.7264 - val_loss: 0.8972 - val_accuracy: 0.6821\n",
      "Epoch 18/30\n",
      "40300/50000 [=======================>......] - ETA: 6s - loss: 0.7730 - accuracy: 0.7298 ETA: 7s - loss: 0.7723 - accura - ETA: 7s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 31s 629us/sample - loss: 0.7096 - accuracy: 0.7521 - val_loss: 0.8902 - val_accuracy: 0.6902\n",
      "Epoch 22/30\n",
      "50000/50000 [==============================] - 31s 625us/sample - loss: 0.6866 - accuracy: 0.7628 - val_loss: 0.8689 - val_accuracy: 0.6966\n",
      "Epoch 23/30\n",
      "50000/50000 [==============================] - 31s 610us/sample - loss: 0.6727 - accuracy: 0.7643 - val_loss: 0.8542 - val_accuracy: 0.7011\n",
      "Epoch 24/30\n",
      "50000/50000 [==============================] - 33s 666us/sample - loss: 0.6504 - accuracy: 0.7744 - val_loss: 0.8334 - val_accuracy: 0.7117\n",
      "Epoch 25/30\n",
      "25700/50000 [==============>...............] - ETA: 14s - loss: 0.6331 - accuracy: 0.7814"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 31s 619us/sample - loss: 0.5641 - accuracy: 0.8039 - val_loss: 0.8218 - val_accuracy: 0.7272\n",
      "Epoch 30/30\n",
      "49600/50000 [============================>.] - ETA: 0s - loss: 0.5425 - accuracy: 0.8109"
     ]
    }
   ],
   "source": [
    "# Création des individus (des neurals nets, ici convnet)\n",
    "list_indiv = []\n",
    "for num in range(len(list_indiv_id)):\n",
    "    list_indiv.append(IndividuConvnets(\n",
    "        list_indiv_id[num],\n",
    "          list_epochs[num],\n",
    "          list_batch_size[num],\n",
    "          list_nb_layers[num],\n",
    "          list_l1[num],\n",
    "          list_l2[num],\n",
    "          list_batch_norm[num],\n",
    "          list_dropout[num],\n",
    "          list_filters_per_layers[num],\n",
    "          list_filters_double[num],\n",
    "          list_MLP_end[num],\n",
    "          list_activation[num],\n",
    "          list_kernel[num],\n",
    "          list_padding[num],\n",
    "          list_max_or_avg_pool[num],\n",
    "          list_pool_frequency[num],\n",
    "          list_pool_frequency_change[num],\n",
    "          list_learning_r[num],\n",
    "          list_momentum[num],\n",
    "          list_optimizer[num]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Chargement de la classe training, affichag\n",
    "training_1 = MyTraining(1, list_indiv)\n",
    "training_1.all_indiv()\n",
    "training_1.train(main_directory, current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxpbXOw1RuwM"
   },
   "source": [
    "### Partie tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jm6vpk3MRuwQ"
   },
   "outputs": [],
   "source": [
    "# Procedure pour utiliser tensorboard\n",
    "#  1 load la première cell\n",
    "#  2 utiliser la derniere cell avec --logdir (précisez bien votre répertoire, plus sur que ça\n",
    "#    fonctionne avec une string \"mon_path\"\n",
    "#  3 Vous NE POURREZ PLUS update tensorboard sur ce port et il y aura des bugs, pour éviter ça\n",
    "#    quand vous voulez faire une update, fermez jupyter notebook (shutdown total) et réouvrez le \n",
    "#    OU, faites kernel->interrupt et changez de port + de folder de log\n",
    "\n",
    "#si vous voulez tenter de tuer des process\n",
    "#os.system(\"taskkill /im tensorboard.exe /f\") #kill tous les processus qui utilisent tensorboard\n",
    "#os.system('!kill 18776') #kill le processus X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Lj738x3Ruwa",
    "outputId": "b7c6a334-5e14-4d18-fc6e-7427bf231326"
   },
   "outputs": [],
   "source": [
    "# Liste des ports utilisés par tensorboard, attention ça se remplit vite et il faut kill jupyter pour clean\n",
    "from tensorboard import notebook\n",
    "notebook.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seEJieCtRuwq",
    "outputId": "051fb9d1-b28c-4221-c61c-a8dfbb158362",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code pour démarrer tensorboard dans le dossier souhaité [PRECISEZ BIEN LE DOSSIER ICI]\n",
    "%tensorboard --logdir \"CONVNETS_20200119-0243\\logs_20200119-093909\\tensorboard_data\" --port=6066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upoC4cmYRuwz",
    "outputId": "c0390d77-555a-49f0-934c-b2740b5e2ba0"
   },
   "outputs": [],
   "source": [
    "# Si vous avez la folie des grandeurs\n",
    "notebook.display(port=6066, height=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLuEIeDvRuw7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Fichier CSV combined_recap + Graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TO0o33QYs6FP",
    "outputId": "0ebfe25a-2965-4220-ccf7-914ae220328f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Commandes pandas utiles\n",
    "data_csv = pd.read_csv(main_directory + \"\\\\combined_recap.csv\")\n",
    "#data_csv = pd.read_csv(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\CONVNETS_20200119-2043\\\\combined_recap.csv\")\n",
    "data.head()\n",
    "#meilleure accuracy, moins pire loss par ex\n",
    "#data_csv.sort_values([\"elapsed\"], axis=0, \n",
    "                 #ascending=[False], inplace=True) \n",
    "\n",
    "# Afficher uniquement certaines colonnes\n",
    "#dataX = data_csv.filter(items=['elapsed', 'label'])\n",
    "\n",
    "#récupérer uniquement où la loss est < à X et ou kernel = (3,3) par exemple\n",
    "#dataX = data_csv.loc[(data_csv['elapsed'] > 700) & (data_csv['threadName'].str.contains('Thread Group 1-2'))]\n",
    "#dataX\n",
    "\n",
    "#pd.set_option('display.max_rows', data3.shape[0]+1) #nombre de row max à afficher\n",
    "#data_csv = pd.read_csv(main_directory+\"\\\\logs_20200116-204456\\\\recap.csv\")\n",
    "#data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGXNwkMCRuxE"
   },
   "outputs": [],
   "source": [
    "image = pyplot.imread(main_directory + \"\\\\logs_20200119-093909\\\\plot.png\")\n",
    "#image = pyplot.imread(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\logs_20200119-093909\\\\plot.png\")\n",
    "pyplot.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "colab": {
   "collapsed_sections": [],
   "name": "test_several_models_1701_modif.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
