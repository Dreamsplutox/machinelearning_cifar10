{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7R0yO1XdBNh"
   },
   "source": [
    "### Démarrage de tensorboard et imports principaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rCCbrXXRuvi",
    "outputId": "7d91c098-39d8-4a89-adf8-3133ab8217de"
   },
   "outputs": [],
   "source": [
    "# Agrandir le notebook ?\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Démarrage de tensorboard pour notebook\n",
    "%load_ext tensorboard\n",
    "\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import shutil  \n",
    "from math import ceil, floor\n",
    "\n",
    "from tensorflow.python.framework import ops #pour tenter de reset tensorboard, sans grand succès\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AF9xVI2bdzuT"
   },
   "source": [
    "### Hyper paramètres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Parametres de verification : \\n\\nlist_indiv_id = [\\'1\\', \\'2\\']\\nlist_epochs = [1, 1]\\nlist_batch_size = [100, 64]\\nlist_nb_layers = [6, 2]\\nlist_l1 = [0.01, 0]\\nlist_l2 = [0.01, 0.001]\\nlist_batch_norm = [0, 1]\\nlist_dropout = [0, 0.2]\\nlist_filters_per_layers = [64, 32]\\nlist_filters_double = [2, 0]\\nlist_MLP_end = [120, 0]\\nlist_activation = [\\'relu\\', \\'relu\\']\\nlist_kernel = [(3,3), (3,3)]\\nlist_padding = [\\'same\\', \\'same\\']\\nlist_max_or_avg_pool = [\\'max\\', \\'avg\\']\\nlist_learning_r = [0.01, 0.01]\\nlist_momentum = [0.9, 0.85]\\nlist_optimizer = [\\'SGD\\', \\'Adam\\']\\n\\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\\nmain_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Parametres de verification : \n",
    "\n",
    "list_indiv_id = ['1', '2']\n",
    "list_epochs = [1, 1]\n",
    "list_batch_size = [100, 64]\n",
    "list_nb_layers = [6, 2]\n",
    "list_l1 = [0.01, 0]\n",
    "list_l2 = [0.01, 0.001]\n",
    "list_batch_norm = [0, 1]\n",
    "list_dropout = [0, 0.2]\n",
    "list_filters_per_layers = [64, 32]\n",
    "list_filters_double = [2, 0]\n",
    "list_MLP_end = [120, 0]\n",
    "list_activation = ['relu', 'relu']\n",
    "list_kernel = [(3,3), (3,3)]\n",
    "list_padding = ['same', 'same']\n",
    "list_max_or_avg_pool = ['max', 'avg']\n",
    "list_learning_r = [0.01, 0.01]\n",
    "list_momentum = [0.9, 0.85]\n",
    "list_optimizer = ['SGD', 'Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Premier Test :\\n# CONSTANTES : nb_layers = 8, batch_size = 50, epochs 100, lr = 0.01, momentum = 0.9, optimizer Adam, \\n#padding = same, maxpool, relu, kernel = (3,3)\\n# \\n# * 3 convnets sans regularization et MLP à 128\\n#   - 2 filters double(2) avec filters (16, 32) \\n#   - 1 sans filters double avec filters (32)\\n\\n# 2 convnets sans regu ou on test le MLP_end et filters 64\\n#  * 1 sans double, filter 64 avec MLP_end(128)\\n#  * 1 sans double, filter 64 avec MLP_end(0)\\n\\n# 5 convnets avec regu (+ MLP à 128) et filters 32 sans double\\n#  * 1 convnet avec l1 à 0.01\\n#  * 1 convnet avec l1 à 0.01 et batchnorm\\n#  * 1 convnet avec l2 à 0.01 et batchnorm\\n#  * 1 convnet avec L1 et L2 à 0.01 et batchnorm\\n#  * 1 convnet avec L1 et L2 à 0.01 + batchnorm + dropout à 0.2\\n\\n# LEXIQUE PARAM : \\n# * filters_double permet de savoir toutes les combien de couche on double les filtres, si 0 on double pas\\n\\nlist_indiv_id = [\\'1\\', \\'2\\', \\'3\\', \\'4\\', \\'5\\', \\'6\\', \\'7\\', \\'8\\', \\'9\\', \\'10\\']\\nlist_epochs = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\\nlist_batch_size = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\\nlist_nb_layers = [8,8,8,8,8,8,8,8,8,8]\\nlist_l1 = [0, 0, 0, 0, 0, 0.01, 0.01, 0.01, 0.01, 0.01]\\nlist_l2 = [0, 0, 0, 0, 0, 0, 0, 0.01, 0.01, 0.01]\\nlist_batch_norm = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\\nlist_dropout = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2]\\nlist_filters_per_layers = [16, 32, 32, 64, 64, 32, 32, 32, 32, 32]\\nlist_filters_double = [2, 2, 0, 0, 0, 0, 0, 0, 0, 0]\\nlist_MLP_end = [128, 128, 128, 128, 0, 128, 128, 128, 128, 128]\\nlist_activation = [\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\']\\nlist_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\\nlist_padding = [\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\']\\nlist_max_or_avg_pool = [\\'max\\',\\'max\\',\\'max\\',\\'max\\',\\'max\\',\\'max\\',\\'max\\',\\'max\\',\\'max\\',\\'max\\']\\nlist_learning_r = [0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01]\\nlist_momentum = [0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9]\\nlist_optimizer = [\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\']\\n\\nmain_directory =(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\CONVNETS_\"\\n                 +datetime.datetime.now().strftime(\"%Y%m%d-%H%M\"))\\n\\ncurrent_directory = os.getcwd()\\nprint(current)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Premier Test :\n",
    "# CONSTANTES : nb_layers = 8, batch_size = 50, epochs 100, lr = 0.01, momentum = 0.9, optimizer Adam, \n",
    "#padding = same, maxpool, relu, kernel = (3,3)\n",
    "# \n",
    "# * 3 convnets sans regularization et MLP à 128\n",
    "#   - 2 filters double(2) avec filters (16, 32) \n",
    "#   - 1 sans filters double avec filters (32)\n",
    "\n",
    "# 2 convnets sans regu ou on test le MLP_end et filters 64\n",
    "#  * 1 sans double, filter 64 avec MLP_end(128)\n",
    "#  * 1 sans double, filter 64 avec MLP_end(0)\n",
    "\n",
    "# 5 convnets avec regu (+ MLP à 128) et filters 32 sans double\n",
    "#  * 1 convnet avec l1 à 0.01\n",
    "#  * 1 convnet avec l1 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec l2 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec L1 et L2 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec L1 et L2 à 0.01 + batchnorm + dropout à 0.2\n",
    "\n",
    "# LEXIQUE PARAM : \n",
    "# * filters_double permet de savoir toutes les combien de couche on double les filtres, si 0 on double pas\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "list_epochs = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
    "list_batch_size = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,8,8,8,8,8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0, 0, 0.01, 0.01, 0.01]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2]\n",
    "list_filters_per_layers = [16, 32, 32, 64, 64, 32, 32, 32, 32, 32]\n",
    "list_filters_double = [2, 2, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 0, 128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max','max','max','max','max']\n",
    "list_learning_r = [0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "main_directory =(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\CONVNETS_\"\n",
    "                 +datetime.datetime.now().strftime(\"%Y%m%d-%H%M\"))\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(current)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Troisième test -> Inlfuence des layers sur plus d'epochs et avec PMC 128 :\n",
    "# CONSTANTES : filters = 64, batch_size = 50, epochs 50, lr = 0.01, momentum = 0.9, optimizer Adam, \n",
    "#padding = same, maxpool, relu, kernel = (3,3), avec regu L1L2 à 0.01 et dropout à 0.2 \n",
    "#(sans Batchnorm, sans MLP final, sans filters double)\n",
    "\n",
    "\n",
    "#6 NB layers fixes -> 8, 16, 32, 64\n",
    "\n",
    "# LEXIQUE PARAM : \n",
    "# * filters_double permet de savoir toutes les combien de couche on double les filtres, si 0 on double pas\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4']\n",
    "list_epochs = [50, 50, 50, 50]\n",
    "list_batch_size = [50, 50, 50, 50]\n",
    "list_nb_layers = [8,16,32,64]\n",
    "list_l1 = [0.01, 0.01, 0.01, 0.01]\n",
    "list_l2 = [0.01, 0.01, 0.01, 0.01]\n",
    "list_batch_norm = [0, 0, 0, 0]\n",
    "list_dropout = [0.2, 0.2, 0.2, 0.2]\n",
    "list_filters_per_layers = [64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max']\n",
    "list_learning_r = [0.01,0.01,0.01,0.01]\n",
    "list_momentum = [0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIIDw6pHdJbK"
   },
   "source": [
    "### Fonctions pour préparer le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tSU6Hl2Ruv0"
   },
   "outputs": [],
   "source": [
    "# Chargement des datasets de train et de validation + one hot encoding\n",
    "def load_dataset():\n",
    "    # Chargement des données cifar10\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode encoding sur les labels\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "# Normalisation pour accroître la vitesse du modèle (en redimensionnant les pixels)\n",
    "def prep_pixels(train, test):\n",
    "    # Convertion des int en float\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # Normalisation pour avoir des nombres entre 0 et 1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # Retourner les images normalisées\n",
    "    return train_norm, test_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTdXP2D-c2D5"
   },
   "source": [
    "### Classe Python pour définir les individus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jP0CrTpRuv8"
   },
   "outputs": [],
   "source": [
    "# Classe pour les convnets\n",
    "class IndividuConvnets:\n",
    "    def __init__(self, indiv_id='1', epochs=10, batch_size=1, nb_layers=2, l1=0, l2=0, batch_norm=0, dropout=0, filters_per_layers=64, filters_double=6, MLP_end=0, activation='relu', kernel=(3,3), padding='same', max_or_avg_pool=0, learning_r=0.01, momentum=0.9, optimizer='SGD'):\n",
    "        # Initialisation de nos variables\n",
    "        self.time_fit = datetime.datetime.now()\n",
    "        self.my_reguralizer = 'he_uniform'\n",
    "        \n",
    "        if nb_layers < 2:\n",
    "            self.nb_layers = 2\n",
    "        else:\n",
    "            self.nb_layers = nb_layers\n",
    "            \n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.indiv_id = indiv_id\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = dropout\n",
    "        self.filters_per_layers = filters_per_layers\n",
    "        \n",
    "        # filters_double doit être supérieur ou égal à 2 OU égal à 0\n",
    "        if filters_double < 2:\n",
    "            self.filters_double = 0\n",
    "        else : \n",
    "            self.filters_double = filters_double\n",
    "        \n",
    "        if MLP_end < 0:\n",
    "            self.MLP_end = 0\n",
    "        else:\n",
    "            self.MLP_end = MLP_end\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.kernel = kernel\n",
    "        self.padding = padding\n",
    "        self.max_or_avg_pool = max_or_avg_pool\n",
    "        self.learning_r = learning_r\n",
    "        self.momentum = momentum\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    # ToString()\n",
    "    def __str__(self):\n",
    "        ma_liste = []\n",
    "        ma_liste.append(\"indiv_id:{},\\n \".format(self.indiv_id))\n",
    "        ma_liste.append(\"epochs:{},\\n \".format(self.epochs))\n",
    "        ma_liste.append(\"batch_size:{},\\n \".format(self.batch_size))\n",
    "        ma_liste.append(\"nb_layers:{},\\n \".format(self.nb_layers))\n",
    "        ma_liste.append(\"l1:{},\\n \".format(self.l1))\n",
    "        ma_liste.append(\"l2:{},\\n \".format(self.l2))\n",
    "        ma_liste.append(\"batch_norm:{},\\n \".format(self.batch_norm))\n",
    "        ma_liste.append(\"dropout:{},\\n \".format(self.dropout))\n",
    "        ma_liste.append(\"filters_per_layers:{},\\n \".format(self.filters_per_layers))\n",
    "        ma_liste.append(\"filters_double:{},\\n \".format(self.filters_double))\n",
    "        ma_liste.append(\"MLP_end:{},\\n \".format(self.MLP_end))\n",
    "        ma_liste.append(\"activation:{},\\n \".format(self.activation))\n",
    "        ma_liste.append(\"kernel:\\n \")\n",
    "        ma_liste.append(\"{},\\n \".format(self.kernel))\n",
    "        ma_liste.append(\"padding:{},\\n \".format(self.padding))\n",
    "        ma_liste.append(\"max_or_avg_pool:{}\\n\".format(self.max_or_avg_pool))\n",
    "        ma_liste.append(\"learning_r:{}\\n\".format(self.learning_r))\n",
    "        ma_liste.append(\"momentum:{}\\n\".format(self.momentum))\n",
    "        ma_liste.append(\"optimizer:{}\\n\".format(self.optimizer))\n",
    "            \n",
    "        return ma_liste\n",
    "    \n",
    "    # (Modele 2 conv + norm ? + pool) * X -> MLP -> softmax sortie 10 -> MODELE BLOC 2\n",
    "    # D'autres modeles seront crees par la suite\n",
    "    def create_and_train_model(self, trainX, trainY, testX, testY, main_directory):\n",
    "        start = datetime.datetime.now()\n",
    "        \n",
    "        # Choix d'un emplacement pour les logs\n",
    "        log_dir=main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\tensorboard_data\\\\\"\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        print(\"log dir = \",log_dir)\n",
    "        \n",
    "        # l1 et l2\n",
    "        if self.l1 > 0 and self.l2 > 0:\n",
    "            self.my_regularizer = regularizers.l1_l2(l1=self.l1 / self.nb_layers, l2=self.l2 / self.nb_layers)\n",
    "        if self.l1 > 0:\n",
    "            self.my_regularizer = regularizers.l1(self.l1 / self.nb_layers)\n",
    "        elif self.l2 > 0:\n",
    "            self.my_regularizer = regularizers.l2(self.l2 / self.nb_layers)\n",
    "        else:\n",
    "            self.my_reguralizer = None\n",
    "            \n",
    "        # Definir notre modèle basique\n",
    "        model = Sequential()\n",
    "\n",
    "        # Faire toutes les convs nécessaires (conv * 2 + max pool)\n",
    "        double_count = 0 # Var pour doubler les filtres\n",
    "        \n",
    "        for i in range(0, self.nb_layers):\n",
    "            # mieux gérer les ids\n",
    "            if i % 2 != 0:\n",
    "                continue\n",
    "            # Traitement pour doubler les filtres\n",
    "            if double_count >= self.filters_double and self.filters_double != 0:\n",
    "                self.filters_per_layers = self.filters_per_layers * 2\n",
    "                print(\"filters = \", self.filters_per_layers)\n",
    "                double_count = 0\n",
    "            \n",
    "            # Choix du bloc (2 conv pool ou 1 conv pool si nb impair)\n",
    "            if self.nb_layers - i != 1:\n",
    "                print(\"i = \",i, \" 2 pools\")\n",
    "                # 2 conv + pool\n",
    "                model.add(Conv2D(self.filters_per_layers, self.kernel, activation=self.activation, kernel_initializer=self.my_reguralizer, padding=self.padding,input_shape=(32, 32, 3), name='conv_'+str(self.filters_per_layers)+'_'+str(i)))\n",
    "                model.add(Conv2D(self.filters_per_layers, self.kernel, activation=self.activation, kernel_initializer=self.my_reguralizer, padding=self.padding,input_shape=(32, 32, 3), name='conv_'+str(self.filters_per_layers)+'_'+str(i+1)))\n",
    "                \n",
    "                if self.batch_norm == 1:\n",
    "                    model.add(BatchNormalization( name='batchnorm_'+str(i/2)))\n",
    "                \n",
    "                # Max ou Avg pooling\n",
    "                if self.max_or_avg_pool == 'max':\n",
    "                    model.add(MaxPooling2D((2, 2), padding='same', name='max_pool_'+str(i/2)))\n",
    "                else:\n",
    "                    model.add(AveragePooling2D((2, 2), padding='same', name='avg_pool_'+str(i/2)))\n",
    "                \n",
    "            else:\n",
    "                print(\"i = \",i, \" 1 pool\")\n",
    "                # 1 conv + pool si nombre impair de couches (nb_layers)\n",
    "                model.add(Conv2D(self.filters_per_layers, self.kernel, activation=self.activation, kernel_initializer=self.my_reguralizer, padding=self.padding,input_shape=(32, 32, 3), name='conv_'+str(self.filters_per_layers)+'_'+str(i)))\n",
    "                if self.batch_norm == 1:\n",
    "                    model.add(BatchNormalization(name='batchnorm_'+str(ceil(i/2))))\n",
    "                \n",
    "                # Max or Avg pooling\n",
    "                if self.max_or_avg_pool == 'max':\n",
    "                    model.add(MaxPooling2D((2, 2), padding='same', name='max_pool_'+str(ceil(i/2))))\n",
    "                else:\n",
    "                    model.add(AveragePooling2D((2, 2), padding='same', name='avg_pool_'+str(ceil(i/2))))\n",
    "                    \n",
    "            double_count = double_count + 2\n",
    "        \n",
    "        \n",
    "        # Fin des convs -> neural network classique\n",
    "        model.add(Flatten(name='Flatten'))\n",
    "        \n",
    "        #tTrain dans un MLP avant la fin si on le souhaite\n",
    "        if self.MLP_end > 0:\n",
    "            model.add(Dense(128, activation='relu', kernel_initializer=self.my_reguralizer, name='MLP_'+str(self.MLP_end)))\n",
    "\n",
    "            #mettre dropout sur les Dense, pas opti sur les convnets (mais on peut le faire pour le démontrer ??)\n",
    "            if self.dropout > 0:\n",
    "                model.add(Dropout(self.dropout))\n",
    "        \n",
    "        #notre output\n",
    "        model.add(Dense(10, activation='softmax', name='output')) \n",
    "\n",
    "        # Compiler le modele\n",
    "        if self.optimizer == 'SGD':\n",
    "            print(\"SGD, learning_r = \", self.learning_r, \" momentum = \", self.momentum, \"\\n\")\n",
    "            opt = SGD(lr=self.learning_r, momentum=self.momentum)\n",
    "        else:\n",
    "            print(\"Adam learning_r = \", self.learning_r, \" momentum = \", self.momentum, \"\\n\")\n",
    "            opt = Adam(lr=self.learning_r, beta_1=self.momentum) # beta_1 => notation pour momentum Adam\n",
    "        model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Entrainer le modele\n",
    "        history = model.fit(trainX, trainY, epochs=self.epochs, batch_size=self.batch_size, validation_data=(testX, testY), verbose=1, callbacks=[tensorboard_callback])\n",
    "        \n",
    "        # Garder une trace du temps nécessaire pour fit (peut être pas la meilleure méthode)\n",
    "        end = datetime.datetime.now()\n",
    "        self.time_fit = end - start\n",
    "        print(\"\\nTime for fit = \", round(self.time_fit.total_seconds(),2)) # Round avec total_seconds()\n",
    "\n",
    "        return history, model\n",
    "    \n",
    "    \n",
    "    def save_model(self, history, model, main_directory, current_time):\n",
    "        \n",
    "        # Sauvegarde du modèle\n",
    "        plot_model(model, \"model.png\")\n",
    "        \n",
    "        # Deplacement modele au bon endroit\n",
    "        shutil.move(os.getcwd()+\"\\\\model.png\", main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\model.png\")\n",
    "        \n",
    "        # Afficher nos résultats dans un graphique matplotlib sauvegardé\n",
    "        pyplot.gcf().subplots_adjust(hspace = 0.5)\n",
    "\n",
    "        # Afficher la loss\n",
    "        pyplot.subplot(211)\n",
    "        pyplot.title('Cross Entropy Loss')\n",
    "        pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "        pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "        \n",
    "        # Afficher l'accuracy\n",
    "        pyplot.subplot(212)\n",
    "        pyplot.title('Classification Accuracy')\n",
    "        pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "        pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "        \n",
    "        # Sauvegarde\n",
    "        filename = main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\\"\n",
    "        pyplot.savefig(filename + 'plot.png')\n",
    "        pyplot.close()\n",
    "       \n",
    "        \n",
    "        print(\"LOSS : \", round(history.history['loss'][0].item(), 3))\n",
    "        print(\"VAL_LOSS : \", round(history.history['val_loss'][0].item(), 3))\n",
    "        print(\"ACCURACY : \", round(history.history['accuracy'][0].item(), 3))\n",
    "        print(\"VAL_ACCURACY : \", round(history.history['val_accuracy'][0].item(), 3))\n",
    "        \n",
    "        # attributs pour créer les csv indivudels et le csv global\n",
    "        self.loss = round(history.history['loss'][0].item(), 3)\n",
    "        self.val_loss = round(history.history['val_loss'][0].item(), 3)\n",
    "        self.accuracy = round(history.history['accuracy'][0].item(), 3)\n",
    "        self.val_accuracy = round(history.history['val_accuracy'][0].item(), 3)\n",
    "        self.time_taken = round(self.time_fit.total_seconds(),2)\n",
    "        \n",
    "        # Créer un dataframe pandas (avec hyperparams) et le sauvegarder en CSV\n",
    "        df = pd.DataFrame({'indiv_id': [self.indiv_id],\n",
    "                           'epochs': [self.epochs],\n",
    "                           'batch_size': [self.batch_size],\n",
    "                           'nb_layers': [self.nb_layers],\n",
    "                           'l1': [self.l1],\n",
    "                           'l2': [self.l2],\n",
    "                           'batch_norm': [self.batch_norm],\n",
    "                           'dropout': [self.dropout],\n",
    "                           'filters_per_layers': [self.filters_per_layers],\n",
    "                           'filters_double': [self.filters_double],\n",
    "                           'MLP_end': [self.MLP_end],\n",
    "                           'activation': [self.activation],\n",
    "                           'kernel': [self.kernel],\n",
    "                           'padding': [self.padding],\n",
    "                           'max_or_avg_pool': [self.max_or_avg_pool],\n",
    "                           'loss': [self.loss],\n",
    "                           'val_loss': [self.val_loss],\n",
    "                           'accuracy': [self.accuracy],\n",
    "                           'val_accuracy': [self.val_accuracy],\n",
    "                           'time_taken' : [self.time_taken],\n",
    "                           'learning_r' : [self.learning_r],\n",
    "                           'momentum' : [self.momentum],\n",
    "                           'optimizer' : [self.optimizer]\n",
    "                          })\n",
    "        \n",
    "        df.to_csv(path_or_buf=filename+\"recap.csv\",index=False)\n",
    "    \n",
    "    # Lance toutes les étapes\n",
    "    def exec_indiv(self, main_directory, current_time):\n",
    "        \n",
    "        # Charger les données\n",
    "        trainX, trainY, testX, testY = load_dataset()\n",
    "        \n",
    "        # Normaliser les données\n",
    "        trainX, testX = prep_pixels(trainX, testX)\n",
    "        \n",
    "        print(\"TrainX shape = \",np.shape(trainX))\n",
    "        print(\"TestX shape = \",np.shape(testX), \"\\n\")\n",
    "        # Créer et entrainer le modele\n",
    "        history, model = self.create_and_train_model(trainX, trainY, testX, testY, main_directory)\n",
    "        \n",
    "        # Sauvegarder le modèle\n",
    "        save = self.save_model(history, model, main_directory, current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buU7Dsqfs6Cv"
   },
   "source": [
    "### Classe Python qui va démarrer les tests des neural nets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Im542kkes6Cx"
   },
   "outputs": [],
   "source": [
    "# Classe générale qui va nous servir à effectuer des actions sur des individus\n",
    "class MyTraining:\n",
    "    # Prends un ID et une liste d'individus \n",
    "    def __init__(self, id_train, indiv_list):\n",
    "        \n",
    "        self.id_train = id_train\n",
    "        self.indiv_list = indiv_list\n",
    "    \n",
    "    def train(self, main_directory, current_time):\n",
    "        \n",
    "        print(\"Start training\\n\")\n",
    "        \n",
    "        for indiv in self.indiv_list:\n",
    "            print(\"indiv \", indiv.indiv_id, \"\\n\")\n",
    "            indiv.exec_indiv(main_directory, current_time)\n",
    "            print(\"-----------------------------------------------------------------\\n\")\n",
    "        \n",
    "        # Fusion des csv \n",
    "        merge_csv = pd.DataFrame(columns=['indiv_id', 'epochs', 'nb_layers', 'l1', 'l2', 'batch_norm', 'dropout',\n",
    "                                          'filters_per_layers', 'filters_double', 'MLP_end', 'activation', 'kernel',\n",
    "                                          'padding','max_or_avg_pool','loss', 'val_loss', 'accuracy', 'val_accuracy',\n",
    "                                          'time_taken','learning_r', 'momentum', 'optimizer'])\n",
    "        \n",
    "        for indiv in self.indiv_list:\n",
    "            merge_csv = merge_csv.append({'indiv_id': indiv.indiv_id, 'epochs': indiv.epochs, 'batch_size': indiv.batch_size,\n",
    "                              'nb_layers' : indiv.nb_layers,'l1' : indiv.l1, 'l2' : indiv.l2, 'batch_norm': indiv.batch_norm,\n",
    "                              'dropout' : indiv.dropout,'filters_per_layers' : indiv.filters_per_layers,\n",
    "                              'filters_double' : indiv.filters_double,'MLP_end' : indiv.MLP_end,'activation' : indiv.activation,\n",
    "                              'kernel' : indiv.kernel,'padding' : indiv.padding, 'max_or_avg_pool' : indiv.max_or_avg_pool,\n",
    "                              'loss' : indiv.loss,'val_loss' : indiv.val_loss, 'accuracy' : indiv.accuracy, \n",
    "                              'val_accuracy' : indiv.val_accuracy,'time_taken' : indiv.time_taken, 'learning_r' : indiv.learning_r,\n",
    "                              'momentum': indiv.momentum, 'optimizer' : indiv.optimizer},ignore_index=True)\n",
    "        \n",
    "        # sauvegarde\n",
    "        merge_csv.to_csv(main_directory+\"\\\\combined_recap.csv\", index=False)\n",
    "            \n",
    "    \n",
    "    def all_indiv(self):\n",
    "        \n",
    "        # Affiche les caractéristiques de l'ensemble des individus\n",
    "        for indiv in self.indiv_list:\n",
    "            print('\\n'.join(indiv.__str__()))\n",
    "            for tir in range(80): print('-', end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIGBnz_4bj74"
   },
   "source": [
    "### Traitement général (train de l'ensemble des modèles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjIV_jF6RuwF",
    "outputId": "6759a6e1-3664-4264-c87d-18266651af09",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indiv_id:1,\n",
      " \n",
      "epochs:50,\n",
      " \n",
      "batch_size:50,\n",
      " \n",
      "nb_layers:8,\n",
      " \n",
      "l1:0.01,\n",
      " \n",
      "l2:0.01,\n",
      " \n",
      "batch_norm:0,\n",
      " \n",
      "dropout:0.2,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:128,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:max\n",
      "\n",
      "learning_r:0.01\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "indiv_id:2,\n",
      " \n",
      "epochs:50,\n",
      " \n",
      "batch_size:50,\n",
      " \n",
      "nb_layers:16,\n",
      " \n",
      "l1:0.01,\n",
      " \n",
      "l2:0.01,\n",
      " \n",
      "batch_norm:0,\n",
      " \n",
      "dropout:0.2,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:128,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:max\n",
      "\n",
      "learning_r:0.01\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "indiv_id:3,\n",
      " \n",
      "epochs:50,\n",
      " \n",
      "batch_size:50,\n",
      " \n",
      "nb_layers:32,\n",
      " \n",
      "l1:0.01,\n",
      " \n",
      "l2:0.01,\n",
      " \n",
      "batch_norm:0,\n",
      " \n",
      "dropout:0.2,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:128,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:max\n",
      "\n",
      "learning_r:0.01\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "indiv_id:4,\n",
      " \n",
      "epochs:50,\n",
      " \n",
      "batch_size:50,\n",
      " \n",
      "nb_layers:64,\n",
      " \n",
      "l1:0.01,\n",
      " \n",
      "l2:0.01,\n",
      " \n",
      "batch_norm:0,\n",
      " \n",
      "dropout:0.2,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:128,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:max\n",
      "\n",
      "learning_r:0.01\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start training\n",
      "\n",
      "indiv  1 \n",
      "\n",
      "TrainX shape =  (50000, 32, 32, 3)\n",
      "TestX shape =  (10000, 32, 32, 3) \n",
      "\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\logs\\convnets\\logs_2020-01-28-19-04\\log_1\\tensorboard_data\\\n",
      "i =  0  2 pools\n",
      "i =  2  2 pools\n",
      "i =  4  2 pools\n",
      "i =  6  2 pools\n",
      "Adam learning_r =  0.01  momentum =  0.9 \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 25s 497us/sample - loss: 2.3887 - accuracy: 0.1007 - val_loss: 2.3038 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 22s 447us/sample - loss: 2.3036 - accuracy: 0.1006 - val_loss: 2.3037 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 24s 484us/sample - loss: 2.3037 - accuracy: 0.1000 - val_loss: 2.3035 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 26s 514us/sample - loss: 2.3037 - accuracy: 0.0978 - val_loss: 2.3044 - val_accuracy: 0.1000\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 26s 513us/sample - loss: 2.3037 - accuracy: 0.1001 - val_loss: 2.3040 - val_accuracy: 0.1000\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 28s 562us/sample - loss: 2.3038 - accuracy: 0.0962 - val_loss: 2.3035 - val_accuracy: 0.1000\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 27s 544us/sample - loss: 2.3036 - accuracy: 0.1006 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 25s 493us/sample - loss: 2.3039 - accuracy: 0.0971 - val_loss: 2.3033 - val_accuracy: 0.1000\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 25s 499us/sample - loss: 2.3037 - accuracy: 0.1005 - val_loss: 2.3031 - val_accuracy: 0.1000\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 27s 543us/sample - loss: 2.3037 - accuracy: 0.1008 - val_loss: 2.3032 - val_accuracy: 0.1000\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 25s 504us/sample - loss: 2.3036 - accuracy: 0.1002 - val_loss: 2.3032 - val_accuracy: 0.1000\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 25s 508us/sample - loss: 2.3037 - accuracy: 0.0996 - val_loss: 2.3035 - val_accuracy: 0.1000\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 25s 508us/sample - loss: 2.3038 - accuracy: 0.1002 - val_loss: 2.3033 - val_accuracy: 0.1000\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 24s 483us/sample - loss: 2.3037 - accuracy: 0.0996 - val_loss: 2.3030 - val_accuracy: 0.1000\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 24s 472us/sample - loss: 2.3037 - accuracy: 0.0996 - val_loss: 2.3030 - val_accuracy: 0.1000\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 25s 499us/sample - loss: 2.3036 - accuracy: 0.0999 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 26s 525us/sample - loss: 2.3037 - accuracy: 0.0993 - val_loss: 2.3029 - val_accuracy: 0.1000\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 29s 575us/sample - loss: 2.3037 - accuracy: 0.0995 - val_loss: 2.3044 - val_accuracy: 0.1000\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 28s 553us/sample - loss: 2.3038 - accuracy: 0.1003 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 32s 649us/sample - loss: 2.3036 - accuracy: 0.1004 - val_loss: 2.3038 - val_accuracy: 0.1000\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 36s 726us/sample - loss: 2.3040 - accuracy: 0.0952 - val_loss: 2.3036 - val_accuracy: 0.1000\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 34s 689us/sample - loss: 2.3037 - accuracy: 0.0999 - val_loss: 2.3036 - val_accuracy: 0.1000\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 29s 581us/sample - loss: 2.3037 - accuracy: 0.0977 - val_loss: 2.3035 - val_accuracy: 0.1000\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 26s 518us/sample - loss: 2.3037 - accuracy: 0.0992 - val_loss: 2.3035 - val_accuracy: 0.1000\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 28s 567us/sample - loss: 2.3036 - accuracy: 0.1011 - val_loss: 2.3040 - val_accuracy: 0.1000\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 24s 489us/sample - loss: 2.3038 - accuracy: 0.0987 - val_loss: 2.3032 - val_accuracy: 0.1000\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 26s 511us/sample - loss: 2.3036 - accuracy: 0.1003 - val_loss: 2.3042 - val_accuracy: 0.1000\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 25s 509us/sample - loss: 2.3039 - accuracy: 0.1007 - val_loss: 2.3036 - val_accuracy: 0.1000\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 25s 499us/sample - loss: 2.3038 - accuracy: 0.0999 - val_loss: 2.3038 - val_accuracy: 0.1000\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 24s 489us/sample - loss: 2.3038 - accuracy: 0.0989 - val_loss: 2.3035 - val_accuracy: 0.1000\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 27s 550us/sample - loss: 2.3037 - accuracy: 0.0994 - val_loss: 2.3042 - val_accuracy: 0.1000\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 31s 616us/sample - loss: 2.3037 - accuracy: 0.0995 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 34s 678us/sample - loss: 2.3038 - accuracy: 0.0994 - val_loss: 2.3032 - val_accuracy: 0.1000\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 32s 650us/sample - loss: 2.3039 - accuracy: 0.0992 - val_loss: 2.3035 - val_accuracy: 0.1000\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 33s 669us/sample - loss: 2.3037 - accuracy: 0.0989 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 28s 570us/sample - loss: 2.3038 - accuracy: 0.0988 - val_loss: 2.3033 - val_accuracy: 0.1000\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 26s 512us/sample - loss: 2.3036 - accuracy: 0.1017 - val_loss: 2.3036 - val_accuracy: 0.1000\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 27s 535us/sample - loss: 2.3037 - accuracy: 0.0992 - val_loss: 2.3030 - val_accuracy: 0.1000\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 28s 569us/sample - loss: 2.3037 - accuracy: 0.0974 - val_loss: 2.3030 - val_accuracy: 0.1000\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 25s 509us/sample - loss: 2.3040 - accuracy: 0.0980 - val_loss: 2.3030 - val_accuracy: 0.1000\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 25s 501us/sample - loss: 2.3040 - accuracy: 0.0948 - val_loss: 2.3033 - val_accuracy: 0.1000\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 25s 491us/sample - loss: 2.3038 - accuracy: 0.0986 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 24s 477us/sample - loss: 2.3038 - accuracy: 0.0993 - val_loss: 2.3031 - val_accuracy: 0.1000\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 24s 487us/sample - loss: 2.3037 - accuracy: 0.0983 - val_loss: 2.3041 - val_accuracy: 0.1000\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 25s 503us/sample - loss: 2.3038 - accuracy: 0.0975 - val_loss: 2.3030 - val_accuracy: 0.1000\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 25s 499us/sample - loss: 2.3037 - accuracy: 0.0996 - val_loss: 2.3035 - val_accuracy: 0.1000\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 25s 499us/sample - loss: 2.3036 - accuracy: 0.1011 - val_loss: 2.3037 - val_accuracy: 0.1000\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 24s 479us/sample - loss: 2.3038 - accuracy: 0.0997 - val_loss: 2.3036 - val_accuracy: 0.1000\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 24s 481us/sample - loss: 2.3036 - accuracy: 0.1010 - val_loss: 2.3037 - val_accuracy: 0.1000\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 25s 508us/sample - loss: 2.3037 - accuracy: 0.0992 - val_loss: 2.3036 - val_accuracy: 0.1000\n",
      "\n",
      "Time for fit =  1338.93\n",
      "LOSS :  2.389\n",
      "VAL_LOSS :  2.304\n",
      "ACCURACY :  0.101\n",
      "VAL_ACCURACY :  0.1\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "indiv  2 \n",
      "\n",
      "TrainX shape =  (50000, 32, 32, 3)\n",
      "TestX shape =  (10000, 32, 32, 3) \n",
      "\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\logs\\convnets\\logs_2020-01-28-19-04\\log_2\\tensorboard_data\\\n",
      "i =  0  2 pools\n",
      "i =  2  2 pools\n",
      "i =  4  2 pools\n",
      "i =  6  2 pools\n",
      "i =  8  2 pools\n",
      "i =  10  2 pools\n",
      "i =  12  2 pools\n",
      "i =  14  2 pools\n",
      "Adam learning_r =  0.01  momentum =  0.9 \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "   50/50000 [..............................] - ETA: 1:18:29 - loss: 2.3025 - accuracy: 0.1400WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.227408). Check your callbacks.\n",
      "50000/50000 [==============================] - 33s 668us/sample - loss: 2.3069 - accuracy: 0.0975 - val_loss: 2.3032 - val_accuracy: 0.1000\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 29s 587us/sample - loss: 2.3036 - accuracy: 0.0995 - val_loss: 2.3032 - val_accuracy: 0.1000\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 34s 683us/sample - loss: 2.3038 - accuracy: 0.0988 - val_loss: 2.3037 - val_accuracy: 0.1000\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 31s 625us/sample - loss: 2.3038 - accuracy: 0.0965 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 30s 606us/sample - loss: 2.3038 - accuracy: 0.1032 - val_loss: 2.3037 - val_accuracy: 0.1000\n",
      "Epoch 6/50\n",
      "28900/50000 [================>.............] - ETA: 12s - loss: 2.3038 - accuracy: 0.0964"
     ]
    }
   ],
   "source": [
    "# Création des individus (des neurals nets, ici convnet)\n",
    "list_indiv = []\n",
    "for num in range(len(list_indiv_id)):\n",
    "    list_indiv.append(IndividuConvnets(\n",
    "        list_indiv_id[num],\n",
    "          list_epochs[num],\n",
    "          list_batch_size[num],\n",
    "          list_nb_layers[num],\n",
    "          list_l1[num],\n",
    "          list_l2[num],\n",
    "          list_batch_norm[num],\n",
    "          list_dropout[num],\n",
    "          list_filters_per_layers[num],\n",
    "          list_filters_double[num],\n",
    "          list_MLP_end[num],\n",
    "          list_activation[num],\n",
    "          list_kernel[num],\n",
    "          list_padding[num],\n",
    "          list_max_or_avg_pool[num],\n",
    "          list_learning_r[num],\n",
    "          list_momentum[num],\n",
    "          list_optimizer[num]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Chargement de la classe training, affichag\n",
    "\n",
    "training_1.all_indiv()\n",
    "training_1.train(main_directory, current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxpbXOw1RuwM"
   },
   "source": [
    "### Partie tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jm6vpk3MRuwQ"
   },
   "outputs": [],
   "source": [
    "# Procedure pour utiliser tensorboard\n",
    "#  1 load la première cell\n",
    "#  2 utiliser la derniere cell avec --logdir (précisez bien votre répertoire, plus sur que ça\n",
    "#    fonctionne avec une string \"mon_path\"\n",
    "#  3 Vous NE POURREZ PLUS update tensorboard sur ce port et il y aura des bugs, pour éviter ça\n",
    "#    quand vous voulez faire une update, fermez jupyter notebook (shutdown total) et réouvrez le \n",
    "#    OU, faites kernel->interrupt et changez de port + de folder de log\n",
    "\n",
    "#si vous voulez tenter de tuer des process\n",
    "#os.system(\"taskkill /im tensorboard.exe /f\") #kill tous les processus qui utilisent tensorboard\n",
    "#os.system('!kill 18776') #kill le processus X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Lj738x3Ruwa",
    "outputId": "b7c6a334-5e14-4d18-fc6e-7427bf231326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No known TensorBoard instances running.\n"
     ]
    }
   ],
   "source": [
    "# Liste des ports utilisés par tensorboard, attention ça se remplit vite et il faut kill jupyter pour clean\n",
    "from tensorboard import notebook\n",
    "notebook.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seEJieCtRuwq",
    "outputId": "051fb9d1-b28c-4221-c61c-a8dfbb158362",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6066 (pid 15048), started 0:00:03 ago. (Use '!kill 15048' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-fa4d40411f0479a7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-fa4d40411f0479a7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6066;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code pour démarrer tensorboard dans le dossier souhaité [PRECISEZ BIEN LE DOSSIER ICI]\n",
    "%tensorboard --logdir \"CONVNETS_20200119-0243\\logs_20200119-093909\\tensorboard_data\" --port=6066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upoC4cmYRuwz",
    "outputId": "c0390d77-555a-49f0-934c-b2740b5e2ba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting TensorBoard with logdir CONVNETS_20200119-0243\\logs_20200119-093909\\tensorboard_data (started 0:33:53 ago; port 6066, pid 15048).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-115646a538e00e39\" width=\"100%\" height=\"1000\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-115646a538e00e39\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6066;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Si vous avez la folie des grandeurs\n",
    "notebook.display(port=6066, height=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLuEIeDvRuw7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Fichier CSV combined_recap + Graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TO0o33QYs6FP",
    "outputId": "0ebfe25a-2965-4220-ccf7-914ae220328f",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'C:\\\\Users\\\\arnau\\\\Desktop\\\\quatri\\xc3\\xa8me_ann\\xc3\\xa9e\\\\Deep_Learning\\\\Projet_cifar-10\\\\CONVNETS_20200119-2043\\\\combined_recap.csv' does not exist: b'C:\\\\Users\\\\arnau\\\\Desktop\\\\quatri\\xc3\\xa8me_ann\\xc3\\xa9e\\\\Deep_Learning\\\\Projet_cifar-10\\\\CONVNETS_20200119-2043\\\\combined_recap.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f86225d9af66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Commandes pandas utiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata_csv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\CONVNETS_20200119-2043\\\\combined_recap.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#meilleure accuracy, moins pire loss par ex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2gpu_py36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2gpu_py36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2gpu_py36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2gpu_py36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2gpu_py36\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'C:\\\\Users\\\\arnau\\\\Desktop\\\\quatri\\xc3\\xa8me_ann\\xc3\\xa9e\\\\Deep_Learning\\\\Projet_cifar-10\\\\CONVNETS_20200119-2043\\\\combined_recap.csv' does not exist: b'C:\\\\Users\\\\arnau\\\\Desktop\\\\quatri\\xc3\\xa8me_ann\\xc3\\xa9e\\\\Deep_Learning\\\\Projet_cifar-10\\\\CONVNETS_20200119-2043\\\\combined_recap.csv'"
     ]
    }
   ],
   "source": [
    "# Commandes pandas utiles\n",
    "data_csv = pd.read_csv(main_directory + \"\\\\combined_recap.csv\")\n",
    "#data_csv = pd.read_csv(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\CONVNETS_20200119-2043\\\\combined_recap.csv\")\n",
    "data.head()\n",
    "#meilleure accuracy, moins pire loss par ex\n",
    "#data_csv.sort_values([\"elapsed\"], axis=0, \n",
    "                 #ascending=[False], inplace=True) \n",
    "\n",
    "# Afficher uniquement certaines colonnes\n",
    "#dataX = data_csv.filter(items=['elapsed', 'label'])\n",
    "\n",
    "#récupérer uniquement où la loss est < à X et ou kernel = (3,3) par exemple\n",
    "#dataX = data_csv.loc[(data_csv['elapsed'] > 700) & (data_csv['threadName'].str.contains('Thread Group 1-2'))]\n",
    "#dataX\n",
    "\n",
    "#pd.set_option('display.max_rows', data3.shape[0]+1) #nombre de row max à afficher\n",
    "#data_csv = pd.read_csv(main_directory+\"\\\\logs_20200116-204456\\\\recap.csv\")\n",
    "#data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGXNwkMCRuxE"
   },
   "outputs": [],
   "source": [
    "image = pyplot.imread(main_directory + \"\\\\logs_20200119-093909\\\\plot.png\")\n",
    "#image = pyplot.imread(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\logs_20200119-093909\\\\plot.png\")\n",
    "pyplot.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "colab": {
   "collapsed_sections": [],
   "name": "test_several_models_1701_modif.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
