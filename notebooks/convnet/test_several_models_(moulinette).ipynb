{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7R0yO1XdBNh"
   },
   "source": [
    "### Démarrage de tensorboard et imports principaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rCCbrXXRuvi",
    "outputId": "7d91c098-39d8-4a89-adf8-3133ab8217de"
   },
   "outputs": [],
   "source": [
    "# Agrandir le notebook ?\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Commande à entrer dans le prompt anaconda si on veut éviter que notre jupyter crash\n",
    "#  sur un long entrainement\n",
    "#jupyter notebook --NotebookApp.iopub_data_rate_limit=100000000\n",
    "\n",
    "# Démarrage de tensorboard pour notebook\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Dense, Flatten, \\\n",
    "Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import shutil  \n",
    "from math import ceil, floor\n",
    "\n",
    "from tensorflow.python.framework import ops #pour tenter de reset tensorboard, sans grand succès\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AF9xVI2bdzuT"
   },
   "source": [
    "### Hyper paramètres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#A PARTIR D ICI, NOUVELLE SYNTAXE (pool_frequency + pool_frequency_change et modif  filters_double)\\n\\n#Pool frequency permet de définir toutes les combiens de couches on va pool\\n\\n#Pool_frequency_change va modifier la frequence UNE FOIS dans le programme \\n#  se note sous la forme (index_pool_changement, modif) => (2, -1) -> après avoir fait 2 pools, \\n#  on fait pool_frequency = pool_frequency -1 => a la place de (conv2D * 2) + pool, on aura \\n#  jusqu\\'à la fin du programme (conv2D) + pool\\n\\n#filiters_double, quand il est init à -1, va faire doubler les filters des convs apres CHAQUE pooling\\n\\n#Real Test 4 -> variations des résultats selon le nombre de couches de pooling\\n\\n# CONSTANTES : nb_layers = 8, batch_size = 50, epochs = 50, lr = 0.001, momentum = 0.9, optimizer Adam, \\n#  padding = same, maxpool, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0, pas de regularization, MLP 128,\\n#  pool_frequency_change = (0,0)\\n# \\n\\n# * 1 convnet du type (1 conv2D) -> 1 (pool)   (8 pools au total)\\n\\n# * 1 convnet du type (2 conv2D) -> 1 (pool)   (4 pools au total)\\n\\n# * 1 convnet du type (4 conv2D) -> 1 (pool)   (2 pools au total)\\n\\n# * 1 convnet du type (8 conv2D) -> 1 (pool)   (1 pool au total)\\n\\n# * 1 convnet du type (8 conv2D) -> 0 (pool)   (0 pool au total)\\n\\n\\n\\nlist_indiv_id = [\\'1\\', \\'2\\', \\'3\\', \\'4\\', \\'5\\']\\nlist_epochs = [30, 30, 30, 30, 30]\\nlist_batch_size = [50, 50, 50, 50, 50]\\nlist_nb_layers = [8,8,8,8,8]\\nlist_l1 = [0, 0, 0, 0, 0]\\nlist_l2 = [0, 0, 0, 0, 0]\\nlist_batch_norm = [0, 0, 0, 0, 0]\\nlist_dropout = [0, 0, 0, 0, 0]\\nlist_filters_per_layers = [64, 64, 64, 64, 64]\\nlist_filters_double = [0, 0, 0, 0, 0]\\nlist_MLP_end = [128, 128, 128, 128, 128]\\nlist_activation = [\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\']\\nlist_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3)]\\nlist_padding = [\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\']\\nlist_max_or_avg_pool = [\\'max\\',\\'max\\',\\'max\\',\\'max\\',\\'max\\']\\nlist_pool_frequency = [1, 2, 4, 8, 0]\\nlist_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0), (0,0)]\\nlist_learning_r = [0.001,0.001,0.001,0.001,0.001]\\nlist_momentum = [0.9,0.9,0.9,0.9,0.9]\\nlist_optimizer = [\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\']\\n\\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\\nmain_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#A PARTIR D ICI, NOUVELLE SYNTAXE (pool_frequency + pool_frequency_change et modif  filters_double)\n",
    "\n",
    "#Pool frequency permet de définir toutes les combiens de couches on va pool\n",
    "\n",
    "#Pool_frequency_change va modifier la frequence UNE FOIS dans le programme \n",
    "#  se note sous la forme (index_pool_changement, modif) => (2, -1) -> après avoir fait 2 pools, \n",
    "#  on fait pool_frequency = pool_frequency -1 => a la place de (conv2D * 2) + pool, on aura \n",
    "#  jusqu'à la fin du programme (conv2D) + pool\n",
    "\n",
    "#filiters_double, quand il est init à -1, va faire doubler les filters des convs apres CHAQUE pooling\n",
    "\n",
    "#Real Test 4 -> variations des résultats selon le nombre de couches de pooling\n",
    "\n",
    "# CONSTANTES : nb_layers = 8, batch_size = 50, epochs = 50, lr = 0.001, momentum = 0.9, optimizer Adam, \n",
    "#  padding = same, maxpool, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0, pas de regularization, MLP 128,\n",
    "#  pool_frequency_change = (0,0)\n",
    "# \n",
    "\n",
    "# * 1 convnet du type (1 conv2D) -> 1 (pool)   (8 pools au total)\n",
    "\n",
    "# * 1 convnet du type (2 conv2D) -> 1 (pool)   (4 pools au total)\n",
    "\n",
    "# * 1 convnet du type (4 conv2D) -> 1 (pool)   (2 pools au total)\n",
    "\n",
    "# * 1 convnet du type (8 conv2D) -> 1 (pool)   (1 pool au total)\n",
    "\n",
    "# * 1 convnet du type (8 conv2D) -> 0 (pool)   (0 pool au total)\n",
    "\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5']\n",
    "list_epochs = [30, 30, 30, 30, 30]\n",
    "list_batch_size = [50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max']\n",
    "list_pool_frequency = [1, 2, 4, 8, 0]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0), (0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Real Test 12 -> variations des résultats selon le dropout\\n\\n# CONSTANTES : nb_layers = 8, batch_size=100, lr à 0.001, momentum = 0.9, epochs = 50, optimizer Adam, avg pooling\\n#  padding = same, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0, pas de regularization sauf dropout, MLP 128,\\n#  pool_frequency = 2, pool_frequency_change = (0,0)\\n# \\n#6 tests\\n\\n# * 1 dropout à 0.1\\n# * 1 dropout à 0.2\\n# * 1 dropout à 0.3\\n# * 1 dropout à 0.4\\n# * 1 dropout à 0.5\\n# * 1 dropout à 0.6\\n\\n\\nlist_indiv_id = [\\'1\\', \\'2\\', \\'3\\', \\'4\\', \\'5\\', \\'6\\']\\nlist_epochs = [1, 1, 1, 1, 1, 50]\\nlist_batch_size = [100, 100, 100, 100, 100, 100]\\nlist_nb_layers = [8,8,8,8,8,8]\\nlist_l1 = [0, 0, 0, 0, 0, 0]\\nlist_l2 = [0, 0, 0, 0, 0, 0]\\nlist_batch_norm = [0, 0, 0, 0, 0, 0]\\nlist_dropout = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\\nlist_filters_per_layers = [64, 64, 64, 64, 64, 64]\\nlist_filters_double = [0, 0, 0, 0, 0, 0, 0, 0]\\nlist_MLP_end = [128, 128, 128, 128, 128, 128]\\nlist_activation = [\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\', \\'relu\\']\\nlist_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\\nlist_padding = [\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\']\\nlist_max_or_avg_pool = [\\'avg\\',\\'avg\\',\\'avg\\',\\'avg\\',\\'avg\\',\\'avg\\']\\nlist_pool_frequency = [2, 2, 2, 2, 2, 2]\\nlist_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0),(0,0),(0,0)]\\nlist_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001]\\nlist_momentum = [0.9,0.9,0.9,0.9,0.9,0.9]\\nlist_optimizer = [\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\']\\n\\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\\nmain_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Real Test 12 -> variations des résultats selon le dropout\n",
    "\n",
    "# CONSTANTES : nb_layers = 8, batch_size=100, lr à 0.001, momentum = 0.9, epochs = 50, optimizer Adam, avg pooling\n",
    "#  padding = same, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0, pas de regularization sauf dropout, MLP 128,\n",
    "#  pool_frequency = 2, pool_frequency_change = (0,0)\n",
    "# \n",
    "#6 tests\n",
    "\n",
    "# * 1 dropout à 0.1\n",
    "# * 1 dropout à 0.2\n",
    "# * 1 dropout à 0.3\n",
    "# * 1 dropout à 0.4\n",
    "# * 1 dropout à 0.5\n",
    "# * 1 dropout à 0.6\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6']\n",
    "list_epochs = [1, 1, 1, 1, 1, 50]\n",
    "list_batch_size = [100, 100, 100, 100, 100, 100]\n",
    "list_nb_layers = [8,8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu', 'relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['avg','avg','avg','avg','avg','avg']\n",
    "list_pool_frequency = [2, 2, 2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0),(0,0),(0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Real Test 13 -> variations des résultats selon batchnorm\\n\\n# CONSTANTES : nb_layers=  8, filters_per_layers = 64, batch_size=100, lr à 0.001, momentum = 0.9, optimizer Adam, \\n#  padding = same, avg_pool, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0,\\n#  pas de regularization sauf batchnorm, MLP 128,\\n#  pool_frequency = 2, pool_frequency_change = (0,0), filters_double = 0 (jamais)\\n#\\n#4 tests\\n\\n# batch norm ou non sur 50 epochs\\n\\n# * rien\\n# * batchnorm\\n\\n# batch norm ou non sur 100 epochs\\n\\n# * rien\\n# * batchnorm\\n\\nlist_indiv_id = [\\'1\\', \\'2\\', \\'3\\', \\'4\\']\\nlist_epochs = [1, 1, 1, 1]\\nlist_batch_size = [100, 100, 100, 100]\\nlist_nb_layers = [8,8,8,8]\\nlist_l1 = [0, 0, 0, 0]\\nlist_l2 = [0, 0, 0, 0]\\nlist_batch_norm = [0, 1, 0, 1]\\nlist_dropout = [0, 0, 0, 0]\\nlist_filters_per_layers = [64, 64, 64, 64]\\nlist_filters_double = [0, 0, 0, 0]\\nlist_MLP_end = [128, 128, 128, 128]\\nlist_activation = [\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\']\\nlist_kernel = [(3,3),(3,3),(3,3),(3,3)]\\nlist_padding = [\\'same\\',\\'same\\',\\'same\\',\\'same\\']\\nlist_max_or_avg_pool = [\\'avg\\',\\'avg\\',\\'avg\\',\\'avg\\']\\nlist_pool_frequency = [2, 2, 2, 2]\\nlist_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0)]\\nlist_learning_r = [0.001,0.001,0.001,0.001]\\nlist_momentum = [0.9,0.9,0.9,0.9]\\nlist_optimizer = [\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\']\\n\\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\\nmain_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Real Test 13 -> variations des résultats selon batchnorm\n",
    "\n",
    "# CONSTANTES : nb_layers=  8, filters_per_layers = 64, batch_size=100, lr à 0.001, momentum = 0.9, optimizer Adam, \n",
    "#  padding = same, avg_pool, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0,\n",
    "#  pas de regularization sauf batchnorm, MLP 128,\n",
    "#  pool_frequency = 2, pool_frequency_change = (0,0), filters_double = 0 (jamais)\n",
    "#\n",
    "#4 tests\n",
    "\n",
    "# batch norm ou non sur 50 epochs\n",
    "\n",
    "# * rien\n",
    "# * batchnorm\n",
    "\n",
    "# batch norm ou non sur 100 epochs\n",
    "\n",
    "# * rien\n",
    "# * batchnorm\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4']\n",
    "list_epochs = [1, 1, 1, 1]\n",
    "list_batch_size = [100, 100, 100, 100]\n",
    "list_nb_layers = [8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0]\n",
    "list_batch_norm = [0, 1, 0, 1]\n",
    "list_dropout = [0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same']\n",
    "list_max_or_avg_pool = ['avg','avg','avg','avg']\n",
    "list_pool_frequency = [2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Real Test 14 -> variations des résultats selon les différentes combinaisons possibles de regu\\n\\n# CONSTANTES : nb_layers = 8, epochs = 50, batch_size=100, lr à 0.001, momentum = 0.9, optimizer Adam, \\n#  padding = same, avg_pool, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0,\\n#  MLP 128,\\n#  pool_frequency = 2, pool_frequency_change = (0,0), filters_double = 0 (filtres ne doublent pas)\\n#\\n#12 tests\\n\\n# L1 / L2 + Dropout\\n\\n# * avec l1L2 = 0.0001 et Dropout à 0.3\\n# * avec L2 = 0.0001 et Dropout à 0.3\\n# * avec l1L2 = 0.0001 et Dropout à 0.4\\n# * avec L2 = 0.0001 et Dropout à 0.4\\n\\n# L1 / L2 + Batchnorm\\n\\n# * avec l1L2 = 0.0001 et BatchNorm\\n# * avec L2 = 0.0001 et BatchNorm\\n\\n# Dropout + Batchnorm\\n\\n# * avec Dropout à 0.3 et BatchNorm\\n# * avec Dropout à 0.4 et BatchNorm\\n\\n# l1 / l2 + Dropout + Batchnorm\\n\\n# * avec l1L2 = 0.0001 et Dropout à 0.3 BatchNorm\\n# * avec L2 = 0.0001 et Dropout à 0.3 et BatchNorm\\n# * avec l1L2 = 0.0001 et Dropout à 0.4 BatchNorm\\n# * avec L2 = 0.0001 et Dropout à 0.4 et BatchNorm\\n\\nlist_indiv_id = [\\'1\\', \\'2\\', \\'3\\', \\'4\\', \\'5\\', \\'6\\', \\'7\\', \\'8\\', \\'9\\', \\'10\\', \\'11\\', \\'12\\']\\nlist_epochs = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\\nlist_batch_size = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\\nlist_nb_layers = [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\\nlist_l1 = [0.0001, 0, 0.0001, 0, 0.0001, 0, 0, 0, 0.0001, 0, 0.0001, 0]\\nlist_l2 = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0, 0, 0.0001, 0.0001, 0.0001, 0.0001]\\nlist_batch_norm = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\\nlist_dropout = [0.3, 0.3, 0.4, 0.4, 0, 0, 0.3, 0.4, 0.3, 0.3, 0.4, 0.4]\\nlist_filters_per_layers = [64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]\\nlist_filters_double = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\\nlist_MLP_end = [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128]\\nlist_activation = [\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\']\\nlist_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3)]\\nlist_padding = [\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\']\\nlist_max_or_avg_pool = [\\'avg\\',\\'avg\\',\\'avg\\',\\'avg\\',\\'avg\\',\\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\']\\nlist_pool_frequency = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\\nlist_pool_frequency_change = [(0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0)]\\nlist_learning_r = [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\\nlist_momentum = [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\\nlist_optimizer = [\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\']\\n\\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\\nmain_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Real Test 14 -> variations des résultats selon les différentes combinaisons possibles de regu\n",
    "\n",
    "# CONSTANTES : nb_layers = 8, epochs = 50, batch_size=100, lr à 0.001, momentum = 0.9, optimizer Adam, \n",
    "#  padding = same, avg_pool, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0,\n",
    "#  MLP 128,\n",
    "#  pool_frequency = 2, pool_frequency_change = (0,0), filters_double = 0 (filtres ne doublent pas)\n",
    "#\n",
    "#12 tests\n",
    "\n",
    "# L1 / L2 + Dropout\n",
    "\n",
    "# * avec l1L2 = 0.0001 et Dropout à 0.3\n",
    "# * avec L2 = 0.0001 et Dropout à 0.3\n",
    "# * avec l1L2 = 0.0001 et Dropout à 0.4\n",
    "# * avec L2 = 0.0001 et Dropout à 0.4\n",
    "\n",
    "# L1 / L2 + Batchnorm\n",
    "\n",
    "# * avec l1L2 = 0.0001 et BatchNorm\n",
    "# * avec L2 = 0.0001 et BatchNorm\n",
    "\n",
    "# Dropout + Batchnorm\n",
    "\n",
    "# * avec Dropout à 0.3 et BatchNorm\n",
    "# * avec Dropout à 0.4 et BatchNorm\n",
    "\n",
    "# l1 / l2 + Dropout + Batchnorm\n",
    "\n",
    "# * avec l1L2 = 0.0001 et Dropout à 0.3 BatchNorm\n",
    "# * avec L2 = 0.0001 et Dropout à 0.3 et BatchNorm\n",
    "# * avec l1L2 = 0.0001 et Dropout à 0.4 BatchNorm\n",
    "# * avec L2 = 0.0001 et Dropout à 0.4 et BatchNorm\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']\n",
    "list_epochs = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
    "list_batch_size = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
    "list_nb_layers = [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
    "list_l1 = [0.0001, 0, 0.0001, 0, 0.0001, 0, 0, 0, 0.0001, 0, 0.0001, 0]\n",
    "list_l2 = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0, 0, 0.0001, 0.0001, 0.0001, 0.0001]\n",
    "list_batch_norm = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "list_dropout = [0.3, 0.3, 0.4, 0.4, 0, 0, 0.3, 0.4, 0.3, 0.3, 0.4, 0.4]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu','relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3)]\n",
    "list_padding = ['same','same','same','same','same','same','same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['avg','avg','avg','avg','avg','avg', 'avg', 'avg', 'avg', 'avg', 'avg', 'avg']\n",
    "list_pool_frequency = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0), (0,0)]\n",
    "list_learning_r = [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
    "list_momentum = [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Real Test 15A -> la recherche du meilleur, vérifier que le meilleur modèle précédent est assez constant (2 50epochs, 2 100 epochs) + check influence MLP à 256,512\\n\\n# CONSTANTES : nb_layers = 8, batch_size=100, lr à 0.001, momentum = 0.9, optimizer Adam, avg pooling\\n#  padding = same, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0, full regu (batchnorm, dropout à 0.4, l1l2 à 0.0001),\\n#  pool_frequency = 2, pool_frequency_change = (0,0)\\n# \\n#6 tests\\n\\n#4 tests MLP 128\\n# * 1 meilleur modele 50 epochs\\n# * 1 meilleur modele 50 epochs\\n\\n# * 1 meilleur modele 100 epochs\\n# * 1 meilleur modele 100 epochs\\n\\n#2 tests MLP à 256 et 512\\n# * 1 meilleur modèle 50 epochs MLP à 256\\n# * 1 meilleur modèle 50 epochs MLP à 512\\n\\n\\nlist_indiv_id = [\\'1\\', \\'2\\', \\'3\\', \\'4\\', \\'5\\', \\'6\\']\\nlist_epochs = [50, 50, 100, 100, 50, 50]\\nlist_batch_size = [100, 100, 100, 100, 100, 100]\\nlist_nb_layers = [8,8,8,8,8,8]\\nlist_l1 = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001]\\nlist_l2 = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001]\\nlist_batch_norm = [1, 1, 1, 1, 1, 1]\\nlist_dropout = [0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\\nlist_filters_per_layers = [64, 64, 64, 64, 64, 64]\\nlist_filters_double = [0, 0, 0, 0, 0, 0, 0, 0]\\nlist_MLP_end = [128, 128, 128, 128, 256, 512]\\nlist_activation = [\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\', \\'relu\\']\\nlist_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\\nlist_padding = [\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\']\\nlist_max_or_avg_pool = [\\'avg\\',\\'avg\\',\\'avg\\',\\'avg\\',\\'avg\\',\\'avg\\']\\nlist_pool_frequency = [2, 2, 2, 2, 2, 2]\\nlist_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0),(0,0),(0,0)]\\nlist_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001]\\nlist_momentum = [0.9,0.9,0.9,0.9,0.9,0.9]\\nlist_optimizer = [\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\']\\n\\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\\nmain_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Real Test 15A -> la recherche du meilleur, vérifier que le meilleur modèle précédent est assez constant (2 50epochs, 2 100 epochs) + check influence MLP à 256,512\n",
    "\n",
    "# CONSTANTES : nb_layers = 8, batch_size=100, lr à 0.001, momentum = 0.9, optimizer Adam, avg pooling\n",
    "#  padding = same, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0, full regu (batchnorm, dropout à 0.4, l1l2 à 0.0001),\n",
    "#  pool_frequency = 2, pool_frequency_change = (0,0)\n",
    "# \n",
    "#6 tests\n",
    "\n",
    "#4 tests MLP 128\n",
    "# * 1 meilleur modele 50 epochs\n",
    "# * 1 meilleur modele 50 epochs\n",
    "\n",
    "# * 1 meilleur modele 100 epochs\n",
    "# * 1 meilleur modele 100 epochs\n",
    "\n",
    "#2 tests MLP à 256 et 512\n",
    "# * 1 meilleur modèle 50 epochs MLP à 256\n",
    "# * 1 meilleur modèle 50 epochs MLP à 512\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6']\n",
    "list_epochs = [50, 50, 100, 100, 50, 50]\n",
    "list_batch_size = [100, 100, 100, 100, 100, 100]\n",
    "list_nb_layers = [8,8,8,8,8,8]\n",
    "list_l1 = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001]\n",
    "list_l2 = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001]\n",
    "list_batch_norm = [1, 1, 1, 1, 1, 1]\n",
    "list_dropout = [0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [128, 128, 128, 128, 256, 512]\n",
    "list_activation = ['relu','relu','relu','relu','relu', 'relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['avg','avg','avg','avg','avg','avg']\n",
    "list_pool_frequency = [2, 2, 2, 2, 2, 2]\n",
    "list_pool_frequency_change = [(0,0),(0,0),(0,0),(0,0),(0,0),(0,0)]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Real Test 15B -> la recherche du meilleur , meilleur modèle sur 300 epochs\\n\\n# CONSTANTES : nb_layers = 8, 300 epochs MLP à 128, batch_size=100, lr à 0.001, momentum = 0.9, optimizer Adam, avg pooling\\n#  padding = same, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0, full regu (batchnorm, dropout à 0.4, l1l2 à 0.0001),\\n#  pool_frequency = 2, pool_frequency_change = (0,0)\\n# \\n#1 test -> meilleur modèle à 300 epochs\\n\\nlist_indiv_id = [\\'1\\']\\nlist_epochs = [300]\\nlist_batch_size = [100]\\nlist_nb_layers = [8]\\nlist_l1 = [0.0001]\\nlist_l2 = [0.0001]\\nlist_batch_norm = [1]\\nlist_dropout = [0.4]\\nlist_filters_per_layers = [64]\\nlist_filters_double = [0]\\nlist_MLP_end = [128]\\nlist_activation = [\\'relu\\']\\nlist_kernel = [(3,3)]\\nlist_padding = [\\'same\\']\\nlist_max_or_avg_pool = [\\'avg\\']\\nlist_pool_frequency = [2]\\nlist_pool_frequency_change = [(0,0)]\\nlist_learning_r = [0.001]\\nlist_momentum = [0.9]\\nlist_optimizer = [\\'Adam\\']\\n\\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\\nmain_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Real Test 15B -> la recherche du meilleur , meilleur modèle sur 300 epochs\n",
    "\n",
    "# CONSTANTES : nb_layers = 8, 300 epochs MLP à 128, batch_size=100, lr à 0.001, momentum = 0.9, optimizer Adam, avg pooling\n",
    "#  padding = same, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0, full regu (batchnorm, dropout à 0.4, l1l2 à 0.0001),\n",
    "#  pool_frequency = 2, pool_frequency_change = (0,0)\n",
    "# \n",
    "#1 test -> meilleur modèle à 300 epochs\n",
    "\n",
    "list_indiv_id = ['1']\n",
    "list_epochs = [300]\n",
    "list_batch_size = [100]\n",
    "list_nb_layers = [8]\n",
    "list_l1 = [0.0001]\n",
    "list_l2 = [0.0001]\n",
    "list_batch_norm = [1]\n",
    "list_dropout = [0.4]\n",
    "list_filters_per_layers = [64]\n",
    "list_filters_double = [0]\n",
    "list_MLP_end = [128]\n",
    "list_activation = ['relu']\n",
    "list_kernel = [(3,3)]\n",
    "list_padding = ['same']\n",
    "list_max_or_avg_pool = ['avg']\n",
    "list_pool_frequency = [2]\n",
    "list_pool_frequency_change = [(0,0)]\n",
    "list_learning_r = [0.001]\n",
    "list_momentum = [0.9]\n",
    "list_optimizer = ['Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verif batch_size sur 50 epochs entre batch_size 200 et 500\n",
    "\n",
    "# CONSTANTES : nb_layers = 8, epochs = 50, MLP à 128, batch_size=100, lr à 0.001, momentum = 0.9, optimizer Adam, avg pooling\n",
    "#  padding = same, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0,\n",
    "#full regu (batchnorm, dropout à 0.4, l1l2 à 0.0001),\n",
    "#  pool_frequency = 2, pool_frequency_change = (0,0)\n",
    "# \n",
    "#1 test -> meilleur modèle à 300 epochs\n",
    "\n",
    "list_indiv_id = ['1', '2']\n",
    "list_epochs = [50, 50]\n",
    "list_batch_size = [200, 500]\n",
    "list_nb_layers = [8,8]\n",
    "list_l1 = [0.0001, 0.0001]\n",
    "list_l2 = [0.0001, 0.0001]\n",
    "list_batch_norm = [1, 1]\n",
    "list_dropout = [0.4, 0.4]\n",
    "list_filters_per_layers = [64, 64]\n",
    "list_filters_double = [0, 0]\n",
    "list_MLP_end = [128, 128]\n",
    "list_activation = ['relu', 'relu']\n",
    "list_kernel = [(3,3), (3,3)]\n",
    "list_padding = ['same', 'same']\n",
    "list_max_or_avg_pool = ['avg', 'avg']\n",
    "list_pool_frequency = [2, 2]\n",
    "list_pool_frequency_change = [(0,0), (0,0)]\n",
    "list_learning_r = [0.001, 0.001]\n",
    "list_momentum = [0.9, 0.9]\n",
    "list_optimizer = ['Adam', 'Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIIDw6pHdJbK"
   },
   "source": [
    "### Fonctions pour préparer le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tSU6Hl2Ruv0"
   },
   "outputs": [],
   "source": [
    "# Chargement des datasets de train et de validation + one hot encoding\n",
    "def load_dataset():\n",
    "    # Chargement des données cifar10\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode encoding sur les labels\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "# Normalisation pour accroître la vitesse du modèle (en redimensionnant les pixels)\n",
    "def prep_pixels(train, test):\n",
    "    # Convertion des int en float\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # Normalisation pour avoir des nombres entre 0 et 1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # Retourner les images normalisées\n",
    "    return train_norm, test_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTdXP2D-c2D5"
   },
   "source": [
    "### Classe Python pour définir les individus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jP0CrTpRuv8"
   },
   "outputs": [],
   "source": [
    "# Classe pour les convnets\n",
    "class IndividuConvnets:\n",
    "    def __init__(self, indiv_id='1', epochs=10, batch_size=1, nb_layers=2, l1=0, l2=0, batch_norm=0,\n",
    "                 dropout=0, filters_per_layers=64, filters_double=6, MLP_end=0, activation='relu',\n",
    "                 kernel=(3,3), padding='same', max_or_avg_pool=0, pool_frequency=2,\n",
    "                 pool_frequency_change = (0,0), learning_r=0.01, momentum=0.9, optimizer='SGD'):\n",
    "        \n",
    "        # Initialisation de nos variables\n",
    "        self.time_fit = datetime.datetime.now()\n",
    "        self.my_reguralizer = None\n",
    "        \n",
    "        \n",
    "        self.nb_layers = nb_layers\n",
    "            \n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.indiv_id = indiv_id\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = dropout\n",
    "        self.filters_per_layers = filters_per_layers\n",
    "                \n",
    "        self.filters_double = filters_double\n",
    "        \n",
    "        if MLP_end < 0:\n",
    "            self.MLP_end = 0\n",
    "        else:\n",
    "            self.MLP_end = MLP_end\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.kernel = kernel\n",
    "        self.padding = padding\n",
    "        self.max_or_avg_pool = max_or_avg_pool\n",
    "        self.pool_frequency = pool_frequency\n",
    "        self.pool_frequency_change = pool_frequency_change\n",
    "        self.learning_r = learning_r\n",
    "        self.momentum = momentum\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    # ToString()\n",
    "    def __str__(self):\n",
    "        ma_liste = []\n",
    "        ma_liste.append(\"indiv_id:{},\\n \".format(self.indiv_id))\n",
    "        ma_liste.append(\"epochs:{},\\n \".format(self.epochs))\n",
    "        ma_liste.append(\"batch_size:{},\\n \".format(self.batch_size))\n",
    "        ma_liste.append(\"nb_layers:{},\\n \".format(self.nb_layers))\n",
    "        ma_liste.append(\"l1:{},\\n \".format(self.l1))\n",
    "        ma_liste.append(\"l2:{},\\n \".format(self.l2))\n",
    "        ma_liste.append(\"batch_norm:{},\\n \".format(self.batch_norm))\n",
    "        ma_liste.append(\"dropout:{},\\n \".format(self.dropout))\n",
    "        ma_liste.append(\"filters_per_layers:{},\\n \".format(self.filters_per_layers))\n",
    "        ma_liste.append(\"filters_double:{},\\n \".format(self.filters_double))\n",
    "        ma_liste.append(\"MLP_end:{},\\n \".format(self.MLP_end))\n",
    "        ma_liste.append(\"activation:{},\\n \".format(self.activation))\n",
    "        ma_liste.append(\"kernel:\\n \")\n",
    "        ma_liste.append(\"{},\\n \".format(self.kernel))\n",
    "        ma_liste.append(\"padding:{},\\n \".format(self.padding))\n",
    "        ma_liste.append(\"max_or_avg_pool:{}\\n\".format(self.max_or_avg_pool))\n",
    "        ma_liste.append(\"pool_frequency:{}\\n\".format(self.pool_frequency))\n",
    "        ma_liste.append(\"pool_frequency_change:{}\\n\".format(self.pool_frequency_change))\n",
    "        ma_liste.append(\"learning_r:{}\\n\".format(self.learning_r))\n",
    "        ma_liste.append(\"momentum:{}\\n\".format(self.momentum))\n",
    "        ma_liste.append(\"optimizer:{}\\n\".format(self.optimizer))\n",
    "            \n",
    "        return ma_liste\n",
    "    \n",
    "    # (Modele 2 conv + norm ? + pool) * X -> MLP -> softmax sortie 10 -> MODELE BLOC 2\n",
    "    # D'autres modeles seront crees par la suite\n",
    "    def create_and_train_model(self, trainX, trainY, testX, testY, main_directory):\n",
    "        start = datetime.datetime.now()\n",
    "        \n",
    "        # Choix d'un emplacement pour les logs\n",
    "        log_dir=main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\tensorboard_data\\\\\"\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        print(\"log dir = \",log_dir)\n",
    "        \n",
    "        # l1 et l2\n",
    "        if self.l1 > 0 and self.l2 > 0:\n",
    "            self.my_regularizer = regularizers.l1_l2(l1=self.l1 / self.nb_layers,\n",
    "                                    l2=self.l2 / self.nb_layers)\n",
    "        elif self.l1 > 0:\n",
    "            self.my_regularizer = regularizers.l1(self.l1 / self.nb_layers)\n",
    "        elif self.l2 > 0:\n",
    "            self.my_regularizer = regularizers.l2(self.l2 / self.nb_layers)\n",
    "        else:\n",
    "            self.my_reguralizer = None\n",
    "            \n",
    "        # Definir notre modèle basique\n",
    "        model = Sequential()\n",
    "\n",
    "        # Faire toutes les convs nécessaires (conv * 2 + max pool)\n",
    "        counter_filters_double = 0 # Var pour doubler les filtres\n",
    "        counter_pool_freq = 0 # var pour savoir où placer les couches de pooling\n",
    "        counter_pool = 0 # var pour nommer les max / avg pool\n",
    "        \n",
    "        #initialisation de variables locales pour ne pas modifier nos attributs evolutifs\n",
    "        # qui doivent être log à la fin\n",
    "        pool_frequency = self.pool_frequency\n",
    "        filters_per_layers = self.filters_per_layers\n",
    "        \n",
    "        for i in range(0, self.nb_layers):\n",
    "            \n",
    "            print(\"counter_pool_freq = \", counter_pool_freq)\n",
    "            # Traitement pour doubler les filtres tous les X couches de convo\n",
    "            if counter_filters_double >= self.filters_double and self.filters_double > 0:\n",
    "                filters_per_layers = filters_per_layers * 2\n",
    "                print(\"filters = \", filters_per_layers)\n",
    "                counter_filters_double = 0\n",
    "            \n",
    "            # Première conv, on fixe l'input shape\n",
    "            if i == 0:\n",
    "                model.add(Conv2D(filters_per_layers, self.kernel, activation=self.activation,\n",
    "                    kernel_regularizer=self.my_reguralizer, padding=self.padding,\n",
    "                    input_shape=(32, 32, 3), name='conv_'+str(filters_per_layers)+'_'+str(i+1)))\n",
    "            else:\n",
    "                # Couche de conv + rajouts selon nos hyperparams\n",
    "                model.add(Conv2D(filters_per_layers, self.kernel, activation=self.activation,\n",
    "                    kernel_regularizer=self.my_reguralizer, padding=self.padding,\n",
    "                                 name='conv_'+str(filters_per_layers)+'_'+str(i+1)))\n",
    "            \n",
    "            # Après avoir créé une conv on incrémente nos compteurs (sauf counter_pool)\n",
    "            counter_filters_double = counter_filters_double + 1\n",
    "            counter_pool_freq = counter_pool_freq + 1\n",
    "            \n",
    "            # Ajouts de la regularization / du pooling selon les hyperparamètres saisis\n",
    "            \n",
    "            if self.batch_norm == 1:\n",
    "                model.add(BatchNormalization( name='batchnorm_'+str(i+1)))\n",
    "            \n",
    "            if pool_frequency == counter_pool_freq:    \n",
    "                #go max ou avg pooling\n",
    "                if self.max_or_avg_pool == 'max':\n",
    "                    model.add(MaxPooling2D((2, 2), padding='same', \n",
    "                        name='max_pool_'+str(counter_pool+1)))\n",
    "                    counter_pool = counter_pool + 1\n",
    "                else:\n",
    "                    model.add(AveragePooling2D((2, 2), padding='same', \n",
    "                                name='avg_pool_'+str(counter_pool+1)))\n",
    "                    counter_pool = counter_pool + 1\n",
    "                \n",
    "                # Dropout sur les pools \n",
    "                if self.dropout > 0:\n",
    "                    model.add(Dropout(self.dropout, \n",
    "                                      name='Dropout_'+str(self.dropout)+'_'+str(counter_pool+1)))\n",
    "                    \n",
    "                # si filters_double à -1, on double les filtres apres le pooling\n",
    "                if self.filters_double == -1:\n",
    "                    filters_per_layers = filters_per_layers * 2\n",
    "                \n",
    "                # Après avoir mis un pool, on regarde si l'on doit changer ou non \n",
    "                #  la freq d'apparition de pools\n",
    "                if counter_pool == self.pool_frequency_change[0] \\\n",
    "                    and self.pool_frequency_change[0] != 0:\n",
    "                    \n",
    "                    pool_frequency = pool_frequency + self.pool_frequency_change[1]\n",
    "                    \n",
    "                counter_pool_freq = 0\n",
    "            \n",
    "        \n",
    "        # Fin des convs -> neural network classique\n",
    "        model.add(Flatten(name='Flatten'))\n",
    "        \n",
    "        print(\"i after for = \", i)\n",
    "        #tTrain dans un MLP avant la fin si on le souhaite\n",
    "        if self.MLP_end > 0:\n",
    "            model.add(Dense(self.MLP_end, activation='relu', kernel_regularizer=self.my_reguralizer,\n",
    "                            name='MLP_'+str(self.MLP_end)))\n",
    "            if self.batch_norm == 1:\n",
    "                model.add(BatchNormalization( name='batchnorm_finale'))\n",
    "            \n",
    "            #mettre dropout sur les Dense, moins opti sur des pools ? à tester si tmp ok\n",
    "            #(pas conv car importantes)\n",
    "            if self.dropout > 0:\n",
    "                model.add(Dropout(self.dropout, name='Dropout_'+str(self.dropout)+'_final'))\n",
    "        \n",
    "        #notre output\n",
    "        model.add(Dense(10, activation='softmax', name='output')) \n",
    "\n",
    "        # Compiler le modele\n",
    "        if self.optimizer == 'SGD':\n",
    "            print(\"SGD, learning_r = \", self.learning_r, \" momentum = \", self.momentum, \"\\n\")\n",
    "            opt = SGD(lr=self.learning_r, momentum=self.momentum)\n",
    "        else:\n",
    "            print(\"Adam learning_r = \", self.learning_r, \" momentum = \", self.momentum, \"\\n\")\n",
    "            opt = Adam(lr=self.learning_r, beta_1=self.momentum) # beta_1 => notation momentum Adam\n",
    "        model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Entrainer le modele\n",
    "        history = model.fit(trainX, trainY, epochs=self.epochs, batch_size=self.batch_size,\n",
    "                        validation_data=(testX, testY), verbose=1, callbacks=[tensorboard_callback])\n",
    "        \n",
    "        # Garder une trace du temps nécessaire pour fit (peut être pas la meilleure méthode)\n",
    "        end = datetime.datetime.now()\n",
    "        self.time_fit = end - start\n",
    "        print(\"\\nTime for fit = \", round(self.time_fit.total_seconds(),2)) # Round total_seconds()\n",
    "\n",
    "        return history, model\n",
    "    \n",
    "    \n",
    "    def save_model(self, history, model, main_directory, current_time):\n",
    "        \n",
    "        # Sauvegarde du modèle\n",
    "        plot_model(model, \"model.png\")\n",
    "        \n",
    "        # Deplacement modele au bon endroit\n",
    "        shutil.move(os.getcwd()+\"\\\\model.png\", main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\model.png\")\n",
    "        \n",
    "        # Afficher nos résultats dans un graphique matplotlib sauvegardé\n",
    "        pyplot.gcf().subplots_adjust(hspace = 0.5)\n",
    "\n",
    "        # Afficher la loss\n",
    "        pyplot.subplot(211)\n",
    "        pyplot.title('Cross Entropy Loss')\n",
    "        pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "        pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "        \n",
    "        # Afficher l'accuracy\n",
    "        pyplot.subplot(212)\n",
    "        pyplot.title('Classification Accuracy')\n",
    "        pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "        pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "        \n",
    "        # Sauvegarde\n",
    "        filename = main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\\"\n",
    "        pyplot.savefig(filename + 'plot.png')\n",
    "        pyplot.close()\n",
    "       \n",
    "        \n",
    "        print(\"LOSS : \", round(history.history['loss'][self.epochs-1].item(), 3))\n",
    "        print(\"VAL_LOSS : \", round(history.history['val_loss'][self.epochs-1].item(), 3))\n",
    "        print(\"ACCURACY : \", round(history.history['accuracy'][self.epochs-1].item(), 3))\n",
    "        print(\"VAL_ACCURACY : \", round(history.history['val_accuracy'][self.epochs-1].item(), 3))\n",
    "        \n",
    "        # attributs pour créer les csv indivudels et le csv global\n",
    "        self.loss = round(history.history['loss'][self.epochs-1].item(), 3)\n",
    "        self.val_loss = round(history.history['val_loss'][self.epochs-1].item(), 3)\n",
    "        self.accuracy = round(history.history['accuracy'][self.epochs-1].item(), 3)\n",
    "        self.val_accuracy = round(history.history['val_accuracy'][self.epochs-1].item(), 3)\n",
    "        self.time_taken = round(self.time_fit.total_seconds(),2)\n",
    "        \n",
    "        # Créer un dataframe pandas (avec hyperparams) et le sauvegarder en CSV\n",
    "        df = pd.DataFrame({'indiv_id': [self.indiv_id],\n",
    "                           'epochs': [self.epochs],\n",
    "                           'batch_size': [self.batch_size],\n",
    "                           'nb_layers': [self.nb_layers],\n",
    "                           'l1': [self.l1],\n",
    "                           'l2': [self.l2],\n",
    "                           'batch_norm': [self.batch_norm],\n",
    "                           'dropout': [self.dropout],\n",
    "                           'filters_per_layers': [self.filters_per_layers],\n",
    "                           'filters_double': [self.filters_double],\n",
    "                           'MLP_end': [self.MLP_end],\n",
    "                           'activation': [self.activation],\n",
    "                           'kernel': [self.kernel],\n",
    "                           'padding': [self.padding],\n",
    "                           'max_or_avg_pool': [self.max_or_avg_pool],\n",
    "                           'pool_frequency': [self.pool_frequency],\n",
    "                           'pool_frequency_change': [self.pool_frequency_change],\n",
    "                           'loss': [self.loss],\n",
    "                           'val_loss': [self.val_loss],\n",
    "                           'accuracy': [self.accuracy],\n",
    "                           'val_accuracy': [self.val_accuracy],\n",
    "                           'time_taken' : [self.time_taken],\n",
    "                           'learning_r' : [self.learning_r],\n",
    "                           'momentum' : [self.momentum],\n",
    "                           'optimizer' : [self.optimizer]\n",
    "                          })\n",
    "        \n",
    "        df.to_csv(path_or_buf=filename+\"recap.csv\",index=False)\n",
    "    \n",
    "    # Lance toutes les étapes\n",
    "    def exec_indiv(self, main_directory, current_time):\n",
    "        \n",
    "        # Charger les données\n",
    "        trainX, trainY, testX, testY = load_dataset()\n",
    "        \n",
    "        # Normaliser les données\n",
    "        trainX, testX = prep_pixels(trainX, testX)\n",
    "        \n",
    "        print(\"TrainX shape = \",np.shape(trainX))\n",
    "        print(\"TestX shape = \",np.shape(testX), \"\\n\")\n",
    "        # Créer et entrainer le modele\n",
    "        history, model = self.create_and_train_model(trainX, trainY, testX, testY, main_directory)\n",
    "        \n",
    "        # Sauvegarder le modèle\n",
    "        save = self.save_model(history, model, main_directory, current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buU7Dsqfs6Cv"
   },
   "source": [
    "### Classe Python qui va démarrer les tests des neural nets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Im542kkes6Cx"
   },
   "outputs": [],
   "source": [
    "# Classe générale qui va nous servir à effectuer des actions sur des individus\n",
    "class MyTraining:\n",
    "    # Prends un ID et une liste d'individus \n",
    "    def __init__(self, id_train, indiv_list):\n",
    "        \n",
    "        self.id_train = id_train\n",
    "        self.indiv_list = indiv_list\n",
    "    \n",
    "    def train(self, main_directory, current_time):\n",
    "        \n",
    "        print(\"Start training\\n\")\n",
    "        \n",
    "        for indiv in self.indiv_list:\n",
    "            print(\"indiv \", indiv.indiv_id, \"\\n\")\n",
    "            indiv.exec_indiv(main_directory, current_time)\n",
    "            print(\"-----------------------------------------------------------------\\n\")\n",
    "        \n",
    "        # Fusion des csv \n",
    "        merge_csv = pd.DataFrame(columns=['indiv_id', 'epochs', 'nb_layers', 'l1', 'l2', 'batch_norm', 'dropout',\n",
    "                                          'filters_per_layers', 'filters_double', 'MLP_end', 'activation', 'kernel',\n",
    "                                          'padding','max_or_avg_pool', 'pool_frequency', 'pool_frequency_change', 'loss',\n",
    "                                          'val_loss', 'accuracy', 'val_accuracy', 'time_taken','learning_r',\n",
    "                                          'momentum', 'optimizer'])\n",
    "        \n",
    "        for indiv in self.indiv_list:\n",
    "            merge_csv = merge_csv.append(\n",
    "                             {'indiv_id': indiv.indiv_id, 'epochs': indiv.epochs, 'batch_size': indiv.batch_size,\n",
    "                              'nb_layers' : indiv.nb_layers,'l1' : indiv.l1, 'l2' : indiv.l2, 'batch_norm': indiv.batch_norm,\n",
    "                              'dropout' : indiv.dropout,'filters_per_layers' : indiv.filters_per_layers,\n",
    "                              'filters_double' : indiv.filters_double,'MLP_end' : indiv.MLP_end,\n",
    "                              'activation' : indiv.activation,'kernel' : indiv.kernel,'padding' : indiv.padding,\n",
    "                              'max_or_avg_pool' : indiv.max_or_avg_pool,'pool_frequency' : indiv.pool_frequency,\n",
    "                              'pool_frequency_change' : indiv.pool_frequency_change,'loss' : indiv.loss,\n",
    "                              'val_loss' : indiv.val_loss,'accuracy' : indiv.accuracy, 'val_accuracy' : indiv.val_accuracy,\n",
    "                              'time_taken' : indiv.time_taken,'learning_r' : indiv.learning_r,'momentum': indiv.momentum,\n",
    "                              'optimizer' : indiv.optimizer\n",
    "                             },ignore_index=True)\n",
    "        \n",
    "        # sauvegarde\n",
    "        merge_csv.to_csv(main_directory+\"\\\\combined_recap.csv\", index=False)\n",
    "            \n",
    "    \n",
    "    def all_indiv(self):\n",
    "        \n",
    "        # Affiche les caractéristiques de l'ensemble des individus\n",
    "        for indiv in self.indiv_list:\n",
    "            print('\\n'.join(indiv.__str__()))\n",
    "            for tir in range(80): print('-', end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIGBnz_4bj74"
   },
   "source": [
    "### Traitement général (train de l'ensemble des modèles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjIV_jF6RuwF",
    "outputId": "6759a6e1-3664-4264-c87d-18266651af09",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indiv_id:1,\n",
      " \n",
      "epochs:50,\n",
      " \n",
      "batch_size:200,\n",
      " \n",
      "nb_layers:8,\n",
      " \n",
      "l1:0.0001,\n",
      " \n",
      "l2:0.0001,\n",
      " \n",
      "batch_norm:1,\n",
      " \n",
      "dropout:0.4,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:128,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:avg\n",
      "\n",
      "pool_frequency:2\n",
      "\n",
      "pool_frequency_change:(0, 0)\n",
      "\n",
      "learning_r:0.001\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "indiv_id:2,\n",
      " \n",
      "epochs:50,\n",
      " \n",
      "batch_size:500,\n",
      " \n",
      "nb_layers:8,\n",
      " \n",
      "l1:0.0001,\n",
      " \n",
      "l2:0.0001,\n",
      " \n",
      "batch_norm:1,\n",
      " \n",
      "dropout:0.4,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:128,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:avg\n",
      "\n",
      "pool_frequency:2\n",
      "\n",
      "pool_frequency_change:(0, 0)\n",
      "\n",
      "learning_r:0.001\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start training\n",
      "\n",
      "indiv  1 \n",
      "\n",
      "TrainX shape =  (50000, 32, 32, 3)\n",
      "TestX shape =  (10000, 32, 32, 3) \n",
      "\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\logs\\convnets\\logs_2020-02-09-17-27\\log_1\\tensorboard_data\\\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "i after for =  7\n",
      "Adam learning_r =  0.001  momentum =  0.9 \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "  200/50000 [..............................] - ETA: 19:58 - loss: 3.5932 - accuracy: 0.1150WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.128291). Check your callbacks.\n",
      "50000/50000 [==============================] - 26s 526us/sample - loss: 1.9107 - accuracy: 0.3362 - val_loss: 3.5465 - val_accuracy: 0.1035\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 23s 457us/sample - loss: 1.3787 - accuracy: 0.5067 - val_loss: 2.0976 - val_accuracy: 0.3368\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 26s 526us/sample - loss: 1.1041 - accuracy: 0.6080 - val_loss: 0.9410 - val_accuracy: 0.6704\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 27s 542us/sample - loss: 0.9522 - accuracy: 0.6634 - val_loss: 1.0725 - val_accuracy: 0.6386\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 29s 572us/sample - loss: 0.8549 - accuracy: 0.7018 - val_loss: 0.9812 - val_accuracy: 0.6721\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 30s 592us/sample - loss: 0.7889 - accuracy: 0.7273 - val_loss: 0.7626 - val_accuracy: 0.7429\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 29s 587us/sample - loss: 0.7342 - accuracy: 0.7452 - val_loss: 0.7446 - val_accuracy: 0.7527\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 30s 599us/sample - loss: 0.6937 - accuracy: 0.7625 - val_loss: 0.6644 - val_accuracy: 0.7733\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 32s 640us/sample - loss: 0.6613 - accuracy: 0.7736 - val_loss: 0.6269 - val_accuracy: 0.7887\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 37s 747us/sample - loss: 0.6278 - accuracy: 0.7849 - val_loss: 0.6249 - val_accuracy: 0.7956\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 36s 726us/sample - loss: 0.6029 - accuracy: 0.7957 - val_loss: 0.6591 - val_accuracy: 0.7879\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 46s 928us/sample - loss: 0.5805 - accuracy: 0.8018 - val_loss: 0.6892 - val_accuracy: 0.7732\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 60s 1ms/sample - loss: 0.5561 - accuracy: 0.8105 - val_loss: 0.8683 - val_accuracy: 0.7218\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 54s 1ms/sample - loss: 0.5473 - accuracy: 0.8133 - val_loss: 0.7322 - val_accuracy: 0.7615\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 46s 928us/sample - loss: 0.5284 - accuracy: 0.8215 - val_loss: 0.5898 - val_accuracy: 0.8090\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.5123 - accuracy: 0.8270 - val_loss: 0.5931 - val_accuracy: 0.8053\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 56s 1ms/sample - loss: 0.4968 - accuracy: 0.8312 - val_loss: 0.4902 - val_accuracy: 0.8357\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 55s 1ms/sample - loss: 0.4815 - accuracy: 0.8361 - val_loss: 0.6049 - val_accuracy: 0.8060\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 50s 992us/sample - loss: 0.4748 - accuracy: 0.8406 - val_loss: 0.5002 - val_accuracy: 0.8301\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4600 - accuracy: 0.8431 - val_loss: 0.5792 - val_accuracy: 0.8120\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 45s 905us/sample - loss: 0.4529 - accuracy: 0.8459 - val_loss: 0.5499 - val_accuracy: 0.8228\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 43s 859us/sample - loss: 0.4419 - accuracy: 0.8516 - val_loss: 0.5620 - val_accuracy: 0.8180\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 44s 883us/sample - loss: 0.4380 - accuracy: 0.8516 - val_loss: 0.5539 - val_accuracy: 0.8211\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 42s 838us/sample - loss: 0.4261 - accuracy: 0.8557 - val_loss: 0.5171 - val_accuracy: 0.8319\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 45s 903us/sample - loss: 0.4208 - accuracy: 0.8579 - val_loss: 0.5466 - val_accuracy: 0.8252\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4122 - accuracy: 0.8615 - val_loss: 0.6109 - val_accuracy: 0.8056\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 47s 947us/sample - loss: 0.4026 - accuracy: 0.8634 - val_loss: 0.5063 - val_accuracy: 0.8372\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.3954 - accuracy: 0.8655 - val_loss: 0.4820 - val_accuracy: 0.8419\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.3928 - accuracy: 0.8661 - val_loss: 0.4832 - val_accuracy: 0.8410\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 45s 894us/sample - loss: 0.3850 - accuracy: 0.8690 - val_loss: 0.6177 - val_accuracy: 0.8024\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 46s 916us/sample - loss: 0.3795 - accuracy: 0.8710 - val_loss: 0.4847 - val_accuracy: 0.8418\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 50s 1ms/sample - loss: 0.3744 - accuracy: 0.8723 - val_loss: 0.4502 - val_accuracy: 0.8545\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 65s 1ms/sample - loss: 0.3691 - accuracy: 0.8755 - val_loss: 0.4609 - val_accuracy: 0.8515\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 73s 1ms/sample - loss: 0.3633 - accuracy: 0.8758 - val_loss: 0.5262 - val_accuracy: 0.8351\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 63s 1ms/sample - loss: 0.3594 - accuracy: 0.8767 - val_loss: 0.6277 - val_accuracy: 0.8036\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 41s 829us/sample - loss: 0.3528 - accuracy: 0.8807 - val_loss: 0.4652 - val_accuracy: 0.8556\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 56s 1ms/sample - loss: 0.3538 - accuracy: 0.8799 - val_loss: 0.4794 - val_accuracy: 0.8443\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 43s 866us/sample - loss: 0.3435 - accuracy: 0.8845 - val_loss: 0.4402 - val_accuracy: 0.8559\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.3400 - accuracy: 0.8843 - val_loss: 0.4748 - val_accuracy: 0.8492\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 34s 676us/sample - loss: 0.3349 - accuracy: 0.8856 - val_loss: 0.4972 - val_accuracy: 0.8397\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 37s 747us/sample - loss: 0.3368 - accuracy: 0.8874 - val_loss: 0.5593 - val_accuracy: 0.8166\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 42s 843us/sample - loss: 0.3367 - accuracy: 0.8855 - val_loss: 0.4939 - val_accuracy: 0.8393\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 46s 927us/sample - loss: 0.3259 - accuracy: 0.8878 - val_loss: 0.4805 - val_accuracy: 0.8534\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 39s 789us/sample - loss: 0.3182 - accuracy: 0.8919 - val_loss: 0.4567 - val_accuracy: 0.8609\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 36s 720us/sample - loss: 0.3199 - accuracy: 0.8905 - val_loss: 0.4568 - val_accuracy: 0.8576\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 34s 684us/sample - loss: 0.3172 - accuracy: 0.8918 - val_loss: 0.4467 - val_accuracy: 0.8610\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 37s 749us/sample - loss: 0.3109 - accuracy: 0.8932 - val_loss: 0.4746 - val_accuracy: 0.8528\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 35s 691us/sample - loss: 0.3091 - accuracy: 0.8943 - val_loss: 0.4134 - val_accuracy: 0.8712\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 30s 601us/sample - loss: 0.3028 - accuracy: 0.8959 - val_loss: 0.4453 - val_accuracy: 0.8578\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 41s 822us/sample - loss: 0.2998 - accuracy: 0.8974 - val_loss: 0.4763 - val_accuracy: 0.8524\n",
      "\n",
      "Time for fit =  2166.66\n",
      "LOSS :  0.3\n",
      "VAL_LOSS :  0.476\n",
      "ACCURACY :  0.897\n",
      "VAL_ACCURACY :  0.852\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "indiv  2 \n",
      "\n",
      "TrainX shape =  (50000, 32, 32, 3)\n",
      "TestX shape =  (10000, 32, 32, 3) \n",
      "\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\logs\\convnets\\logs_2020-02-09-17-27\\log_2\\tensorboard_data\\\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "i after for =  7\n",
      "Adam learning_r =  0.001  momentum =  0.9 \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "  500/50000 [..............................] - ETA: 11:49 - loss: 3.4334 - accuracy: 0.0940WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.383080). Check your callbacks.\n",
      "50000/50000 [==============================] - 36s 715us/sample - loss: 2.0602 - accuracy: 0.2996 - val_loss: 2.5734 - val_accuracy: 0.1012\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 37s 744us/sample - loss: 1.5349 - accuracy: 0.4451 - val_loss: 2.9161 - val_accuracy: 0.1522\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 32s 644us/sample - loss: 1.2850 - accuracy: 0.5439 - val_loss: 2.9638 - val_accuracy: 0.1029\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 33s 653us/sample - loss: 1.1095 - accuracy: 0.6099 - val_loss: 2.7613 - val_accuracy: 0.2292\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 32s 634us/sample - loss: 0.9805 - accuracy: 0.6549 - val_loss: 1.7862 - val_accuracy: 0.3812\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 47s 940us/sample - loss: 0.8864 - accuracy: 0.6865 - val_loss: 1.2635 - val_accuracy: 0.5855\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 39s 773us/sample - loss: 0.8259 - accuracy: 0.7109 - val_loss: 0.9039 - val_accuracy: 0.6864\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.7671 - accuracy: 0.7315 - val_loss: 0.7119 - val_accuracy: 0.7533\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 38s 758us/sample - loss: 0.7185 - accuracy: 0.7507 - val_loss: 0.7365 - val_accuracy: 0.7466\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 43s 863us/sample - loss: 0.6903 - accuracy: 0.7604 - val_loss: 0.7125 - val_accuracy: 0.7615\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 45s 897us/sample - loss: 0.6648 - accuracy: 0.7681 - val_loss: 0.8209 - val_accuracy: 0.7380\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 40s 803us/sample - loss: 0.6370 - accuracy: 0.7816 - val_loss: 0.6112 - val_accuracy: 0.7937\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 43s 859us/sample - loss: 0.6157 - accuracy: 0.7886 - val_loss: 0.6824 - val_accuracy: 0.7730\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 31s 629us/sample - loss: 0.5970 - accuracy: 0.7934 - val_loss: 0.6900 - val_accuracy: 0.7693\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 38s 751us/sample - loss: 0.5753 - accuracy: 0.8037 - val_loss: 0.5848 - val_accuracy: 0.8053\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 54s 1ms/sample - loss: 0.5589 - accuracy: 0.8097 - val_loss: 0.6323 - val_accuracy: 0.7918\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 65s 1ms/sample - loss: 0.5425 - accuracy: 0.8153 - val_loss: 0.5942 - val_accuracy: 0.8031\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 48s 970us/sample - loss: 0.5256 - accuracy: 0.8192 - val_loss: 0.5252 - val_accuracy: 0.8240\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 43s 868us/sample - loss: 0.5125 - accuracy: 0.8243 - val_loss: 0.6747 - val_accuracy: 0.7920\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 57s 1ms/sample - loss: 0.4854 - accuracy: 0.8356 - val_loss: 0.6315 - val_accuracy: 0.7943\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 45s 901us/sample - loss: 0.4768 - accuracy: 0.8382 - val_loss: 0.5365 - val_accuracy: 0.8236\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 42s 845us/sample - loss: 0.4658 - accuracy: 0.8417 - val_loss: 0.5193 - val_accuracy: 0.8316\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 32s 635us/sample - loss: 0.4587 - accuracy: 0.8453 - val_loss: 0.6486 - val_accuracy: 0.8028\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 55s 1ms/sample - loss: 0.4464 - accuracy: 0.8494 - val_loss: 0.5550 - val_accuracy: 0.8211\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 48s 959us/sample - loss: 0.4369 - accuracy: 0.8503 - val_loss: 0.5587 - val_accuracy: 0.8256\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.4316 - accuracy: 0.8539 - val_loss: 0.4968 - val_accuracy: 0.8422\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 43s 854us/sample - loss: 0.4293 - accuracy: 0.8545 - val_loss: 0.5964 - val_accuracy: 0.8119\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 39s 771us/sample - loss: 0.4179 - accuracy: 0.8570 - val_loss: 0.5157 - val_accuracy: 0.8383\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 45s 901us/sample - loss: 0.4124 - accuracy: 0.8600 - val_loss: 0.5496 - val_accuracy: 0.8264\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4055 - accuracy: 0.8623 - val_loss: 0.4723 - val_accuracy: 0.8473\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 57s 1ms/sample - loss: 0.4009 - accuracy: 0.8656 - val_loss: 0.4609 - val_accuracy: 0.8493\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 39s 778us/sample - loss: 0.3958 - accuracy: 0.8640 - val_loss: 0.5246 - val_accuracy: 0.8368\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.3868 - accuracy: 0.8683 - val_loss: 0.5455 - val_accuracy: 0.8302\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 56s 1ms/sample - loss: 0.3761 - accuracy: 0.8717 - val_loss: 0.5637 - val_accuracy: 0.8266\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 45s 902us/sample - loss: 0.3751 - accuracy: 0.8721 - val_loss: 0.5009 - val_accuracy: 0.8455\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 43s 864us/sample - loss: 0.3681 - accuracy: 0.8735 - val_loss: 0.4791 - val_accuracy: 0.8507\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 47s 940us/sample - loss: 0.3582 - accuracy: 0.8776 - val_loss: 0.5087 - val_accuracy: 0.8416\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 40s 798us/sample - loss: 0.3588 - accuracy: 0.8780 - val_loss: 0.4742 - val_accuracy: 0.8545\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 38s 765us/sample - loss: 0.3568 - accuracy: 0.8781 - val_loss: 0.5093 - val_accuracy: 0.8414\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 50s 992us/sample - loss: 0.3463 - accuracy: 0.8821 - val_loss: 0.4539 - val_accuracy: 0.8577\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 49s 985us/sample - loss: 0.3502 - accuracy: 0.8810 - val_loss: 0.5584 - val_accuracy: 0.8310\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 60s 1ms/sample - loss: 0.3414 - accuracy: 0.8838 - val_loss: 0.5083 - val_accuracy: 0.8441\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 42s 836us/sample - loss: 0.3404 - accuracy: 0.8847 - val_loss: 0.4778 - val_accuracy: 0.8554\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 56s 1ms/sample - loss: 0.3335 - accuracy: 0.8857 - val_loss: 0.5222 - val_accuracy: 0.8428\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 42s 846us/sample - loss: 0.3344 - accuracy: 0.8868 - val_loss: 0.4485 - val_accuracy: 0.8633\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 40s 801us/sample - loss: 0.3248 - accuracy: 0.8892 - val_loss: 0.4869 - val_accuracy: 0.8498\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 32s 644us/sample - loss: 0.3232 - accuracy: 0.8911 - val_loss: 0.4879 - val_accuracy: 0.8534\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 37s 739us/sample - loss: 0.3238 - accuracy: 0.8882 - val_loss: 0.4777 - val_accuracy: 0.8543\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 40s 802us/sample - loss: 0.3181 - accuracy: 0.8905 - val_loss: 0.4855 - val_accuracy: 0.8522\n",
      "\n",
      "Time for fit =  2224.37\n",
      "LOSS :  0.318\n",
      "VAL_LOSS :  0.485\n",
      "ACCURACY :  0.89\n",
      "VAL_ACCURACY :  0.852\n",
      "-----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Création des individus (des neurals nets, ici convnet)\n",
    "list_indiv = []\n",
    "for num in range(len(list_indiv_id)):\n",
    "    list_indiv.append(IndividuConvnets(\n",
    "        list_indiv_id[num],\n",
    "          list_epochs[num],\n",
    "          list_batch_size[num],\n",
    "          list_nb_layers[num],\n",
    "          list_l1[num],\n",
    "          list_l2[num],\n",
    "          list_batch_norm[num],\n",
    "          list_dropout[num],\n",
    "          list_filters_per_layers[num],\n",
    "          list_filters_double[num],\n",
    "          list_MLP_end[num],\n",
    "          list_activation[num],\n",
    "          list_kernel[num],\n",
    "          list_padding[num],\n",
    "          list_max_or_avg_pool[num],\n",
    "          list_pool_frequency[num],\n",
    "          list_pool_frequency_change[num],\n",
    "          list_learning_r[num],\n",
    "          list_momentum[num],\n",
    "          list_optimizer[num]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Chargement de la classe training, affichage\n",
    "training_1 = MyTraining(1, list_indiv)\n",
    "training_1.all_indiv()\n",
    "training_1.train(main_directory, current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxpbXOw1RuwM"
   },
   "source": [
    "### Partie tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jm6vpk3MRuwQ"
   },
   "outputs": [],
   "source": [
    "# Procedure pour utiliser tensorboard\n",
    "#  1 load la première cell\n",
    "#  2 utiliser la derniere cell avec --logdir (précisez bien votre répertoire, plus sur que ça\n",
    "#    fonctionne avec une string \"mon_path\"\n",
    "#  3 Vous NE POURREZ PLUS update tensorboard sur ce port et il y aura des bugs, pour éviter ça\n",
    "#    quand vous voulez faire une update, fermez jupyter notebook (shutdown total) et réouvrez le \n",
    "#    OU, faites kernel->interrupt et changez de port + de folder de log\n",
    "\n",
    "#si vous voulez tenter de tuer des process\n",
    "#os.system(\"taskkill /im tensorboard.exe /f\") #kill tous les processus qui utilisent tensorboard\n",
    "#os.system('!kill 18776') #kill le processus X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Lj738x3Ruwa",
    "outputId": "b7c6a334-5e14-4d18-fc6e-7427bf231326"
   },
   "outputs": [],
   "source": [
    "# Liste des ports utilisés par tensorboard, attention ça se remplit vite et il faut kill jupyter pour clean\n",
    "from tensorboard import notebook\n",
    "notebook.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seEJieCtRuwq",
    "outputId": "051fb9d1-b28c-4221-c61c-a8dfbb158362",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code pour démarrer tensorboard dans le dossier souhaité [PRECISEZ BIEN LE DOSSIER ICI]\n",
    "%tensorboard --logdir \"CONVNETS_20200119-0243\\logs_20200119-093909\\tensorboard_data\" --port=6066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upoC4cmYRuwz",
    "outputId": "c0390d77-555a-49f0-934c-b2740b5e2ba0"
   },
   "outputs": [],
   "source": [
    "# Si vous avez la folie des grandeurs\n",
    "notebook.display(port=6066, height=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLuEIeDvRuw7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Fichier CSV combined_recap + Graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TO0o33QYs6FP",
    "outputId": "0ebfe25a-2965-4220-ccf7-914ae220328f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Commandes pandas utiles\n",
    "data_csv = pd.read_csv(main_directory + \"\\\\combined_recap.csv\")\n",
    "#data_csv = pd.read_csv(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\CONVNETS_20200119-2043\\\\combined_recap.csv\")\n",
    "data.head()\n",
    "#meilleure accuracy, moins pire loss par ex\n",
    "#data_csv.sort_values([\"elapsed\"], axis=0, \n",
    "                 #ascending=[False], inplace=True) \n",
    "\n",
    "# Afficher uniquement certaines colonnes\n",
    "#dataX = data_csv.filter(items=['elapsed', 'label'])\n",
    "\n",
    "#récupérer uniquement où la loss est < à X et ou kernel = (3,3) par exemple\n",
    "#dataX = data_csv.loc[(data_csv['elapsed'] > 700) & (data_csv['threadName'].str.contains('Thread Group 1-2'))]\n",
    "#dataX\n",
    "\n",
    "#pd.set_option('display.max_rows', data3.shape[0]+1) #nombre de row max à afficher\n",
    "#data_csv = pd.read_csv(main_directory+\"\\\\logs_20200116-204456\\\\recap.csv\")\n",
    "#data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGXNwkMCRuxE"
   },
   "outputs": [],
   "source": [
    "image = pyplot.imread(main_directory + \"\\\\logs_20200119-093909\\\\plot.png\")\n",
    "#image = pyplot.imread(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\logs_20200119-093909\\\\plot.png\")\n",
    "pyplot.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "colab": {
   "collapsed_sections": [],
   "name": "test_several_models_1701_modif.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
