{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7R0yO1XdBNh"
   },
   "source": [
    "### Démarrage de tensorboard et imports principaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rCCbrXXRuvi",
    "outputId": "7d91c098-39d8-4a89-adf8-3133ab8217de"
   },
   "outputs": [],
   "source": [
    "# Agrandir le notebook ?\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Commande à entrer dans le prompt anaconda si on veut éviter que notre jupyter crash sur un long entrainement\n",
    "#jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000\n",
    "\n",
    "# Démarrage de tensorboard pour notebook\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import shutil  \n",
    "from math import ceil, floor\n",
    "\n",
    "from tensorflow.python.framework import ops #pour tenter de reset tensorboard, sans grand succès\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AF9xVI2bdzuT"
   },
   "source": [
    "### Hyper paramètres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Parametres de verification : \\n\\nlist_indiv_id = [\\'1\\', \\'2\\']\\nlist_epochs = [1, 1]\\nlist_batch_size = [100, 64]\\nlist_nb_layers = [6, 2]\\nlist_l1 = [0.01, 0]\\nlist_l2 = [0.01, 0.001]\\nlist_batch_norm = [0, 1]\\nlist_dropout = [0, 0.2]\\nlist_filters_per_layers = [64, 32]\\nlist_filters_double = [2, 0]\\nlist_MLP_end = [120, 0]\\nlist_activation = [\\'relu\\', \\'relu\\']\\nlist_kernel = [(3,3), (3,3)]\\nlist_padding = [\\'same\\', \\'same\\']\\nlist_max_or_avg_pool = [\\'max\\', \\'avg\\']\\nlist_learning_r = [0.01, 0.01]\\nlist_momentum = [0.9, 0.85]\\nlist_optimizer = [\\'SGD\\', \\'Adam\\']\\n\\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\\nmain_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Parametres de verification : \n",
    "\n",
    "list_indiv_id = ['1', '2']\n",
    "list_epochs = [1, 1]\n",
    "list_batch_size = [100, 64]\n",
    "list_nb_layers = [6, 2]\n",
    "list_l1 = [0.01, 0]\n",
    "list_l2 = [0.01, 0.001]\n",
    "list_batch_norm = [0, 1]\n",
    "list_dropout = [0, 0.2]\n",
    "list_filters_per_layers = [64, 32]\n",
    "list_filters_double = [2, 0]\n",
    "list_MLP_end = [120, 0]\n",
    "list_activation = ['relu', 'relu']\n",
    "list_kernel = [(3,3), (3,3)]\n",
    "list_padding = ['same', 'same']\n",
    "list_max_or_avg_pool = ['max', 'avg']\n",
    "list_learning_r = [0.01, 0.01]\n",
    "list_momentum = [0.9, 0.85]\n",
    "list_optimizer = ['SGD', 'Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Real Test 1 -> importance des layers (refaire le test 5 mais sans la regularization)\\n# CONSTANTES : filters = 64, batch_size = 50, epochs = 30 (30 car avec beaucoup de layers ça va être long),\\n#lr = 0.001, momentum = 0.9, optimizer Adam, padding = same, maxpool, relu, kernel = (3,3), \\n#avec regu L1L2 à 0 et dropout à 0 (pas de dropout), (sans Batchnorm, sans MLP final, sans filters double)\\n\\n#6 NB layers fixes -> 8, 16, 32, 64, 96, 128\\n\\n# LEXIQUE PARAM : \\n# * filters_double permet de savoir toutes les combien de couche on double les filtres, si 0 on double pas\\n\\nlist_indiv_id = [\\'1\\', \\'2\\', \\'3\\', \\'4\\', \\'5\\', \\'6\\']\\nlist_epochs = [30, 30, 30, 30, 30, 30]\\nlist_batch_size = [50, 50, 50, 50, 50, 50]\\nlist_nb_layers = [8,16,32,64,96,128]\\nlist_l1 = [0, 0, 0, 0, 0, 0]\\nlist_l2 = [0, 0, 0, 0, 0, 0]\\nlist_batch_norm = [0, 0, 0, 0, 0, 0]\\nlist_dropout = [0, 0, 0, 0, 0, 0]\\nlist_filters_per_layers = [64, 64, 64, 64, 64, 64]\\nlist_filters_double = [0, 0, 0, 0, 0, 0]\\nlist_MLP_end = [0, 0, 0, 0, 0, 0]\\nlist_activation = [\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\']\\nlist_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\\nlist_padding = [\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\']\\nlist_max_or_avg_pool = [\\'max\\',\\'max\\',\\'max\\',\\'max\\',\\'max\\',\\'max\\']\\nlist_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001]\\nlist_momentum = [0.9,0.9,0.9,0.9,0.9,0.9]\\nlist_optimizer = [\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\']\\n\\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\\nmain_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Real Test 1 -> importance des layers (refaire le test 5 mais sans la regularization)\n",
    "# CONSTANTES : filters = 64, batch_size = 50, epochs = 30 (30 car avec beaucoup de layers ça va être long),\n",
    "#lr = 0.001, momentum = 0.9, optimizer Adam, padding = same, maxpool, relu, kernel = (3,3), \n",
    "#avec regu L1L2 à 0 et dropout à 0 (pas de dropout), (sans Batchnorm, sans MLP final, sans filters double)\n",
    "\n",
    "#6 NB layers fixes -> 8, 16, 32, 64, 96, 128\n",
    "\n",
    "# LEXIQUE PARAM : \n",
    "# * filters_double permet de savoir toutes les combien de couche on double les filtres, si 0 on double pas\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6']\n",
    "list_epochs = [30, 30, 30, 30, 30, 30]\n",
    "list_batch_size = [50, 50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,16,32,64,96,128]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [0, 0, 0, 0, 0, 0]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max']\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Real Test 2 -> importance des filters\\n# CONSTANTES : nb_layers = 8, batch_size = 50, epochs = 50, lr = 0.001, momentum = 0.9, optimizer Adam, \\n#padding = same, maxpool, relu, kernel = (3,3), MLP avec 128 neuronnes, pas de regularization\\n# \\n# * 5 convnets avec filters fixes\\n#   - 1 sans filters double avec filters (16)\\n#   - 1 sans filters double avec filters (32)\\n#   - 1 sans filters double avec filters (64)\\n#   - 1 sans filters double avec filters (96)\\n#   - 1 sans filters double avec filters (128)\\n\\n# * 5 convnets avec filters evolutifs (filter double sur mes blocs de 2 couches de convolution)\\n#   - 1 filters double (2) avec filters (16)  (va faire 16, 32, 64, 128)\\n#   - 1 filters double (2) avec filters (32)  (va faire 32, 64, 128, 256)\\n#   - 1 filters double (4) avec filters (16)  (va faire 16, 16, 32, 32)\\n#   - 1 filters double (4) avec filters (32)  (va faire 32, 32, 64, 64)\\n#   - 1 filters double (4) avec filters (64)  (va faire 64, 64, 128, 128)\\n\\nlist_indiv_id = [\\'1\\', \\'2\\', \\'3\\', \\'4\\', \\'5\\', \\'6\\', \\'7\\', \\'8\\', \\'9\\', \\'10\\']\\nlist_epochs = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\\nlist_batch_size = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\\nlist_nb_layers = [8,8,8,8,8,8,8,8,8,8]\\nlist_l1 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\\nlist_l2 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\\nlist_batch_norm = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\\nlist_dropout = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\\nlist_filters_per_layers = [16, 32, 64, 96, 128, 16, 32, 16, 32, 64]\\nlist_filters_double = [0, 0, 0, 0, 0, 2, 2, 4, 4, 4]\\nlist_MLP_end = [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]\\nlist_activation = [\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\',\\'relu\\']\\nlist_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\\nlist_padding = [\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\',\\'same\\']\\nlist_max_or_avg_pool = [\\'max\\',\\'max\\',\\'max\\',\\'max\\',\\'max\\',\\'max\\',\\'max\\',\\'max\\',\\'max\\',\\'max\\']\\nlist_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001]\\nlist_momentum = [0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9]\\nlist_optimizer = [\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\',\\'Adam\\']\\n\\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\\nmain_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Real Test 2 -> importance des filters\n",
    "# CONSTANTES : nb_layers = 8, batch_size = 50, epochs = 50, lr = 0.001, momentum = 0.9, optimizer Adam, \n",
    "#padding = same, maxpool, relu, kernel = (3,3), MLP avec 128 neuronnes, pas de regularization\n",
    "# \n",
    "# * 5 convnets avec filters fixes\n",
    "#   - 1 sans filters double avec filters (16)\n",
    "#   - 1 sans filters double avec filters (32)\n",
    "#   - 1 sans filters double avec filters (64)\n",
    "#   - 1 sans filters double avec filters (96)\n",
    "#   - 1 sans filters double avec filters (128)\n",
    "\n",
    "# * 5 convnets avec filters evolutifs (filter double sur mes blocs de 2 couches de convolution)\n",
    "#   - 1 filters double (2) avec filters (16)  (va faire 16, 32, 64, 128)\n",
    "#   - 1 filters double (2) avec filters (32)  (va faire 32, 64, 128, 256)\n",
    "#   - 1 filters double (4) avec filters (16)  (va faire 16, 16, 32, 32)\n",
    "#   - 1 filters double (4) avec filters (32)  (va faire 32, 32, 64, 64)\n",
    "#   - 1 filters double (4) avec filters (64)  (va faire 64, 64, 128, 128)\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "list_epochs = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
    "list_batch_size = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,8,8,8,8,8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [16, 32, 64, 96, 128, 16, 32, 16, 32, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 2, 2, 4, 4, 4]\n",
    "list_MLP_end = [128, 128, 128, 128, 128, 128, 128, 128, 128, 128]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max','max','max','max','max']\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Real Test 2 -> importance des filters (UNIQUEMENT FILTERS 16 FIXE)\\n# CONSTANTES : nb_layers = 8, batch_size = 50, epochs = 50, lr = 0.001, momentum = 0.9, optimizer Adam, \\n#padding = same, maxpool, relu, kernel = (3,3), MLP avec 128 neuronnes, pas de regularization\\n# \\n# * 5 convnets avec filters fixes\\n#   - 1 sans filters double avec filters (16)\\n#   - 1 sans filters double avec filters (32)\\n#   - 1 sans filters double avec filters (64)\\n#   - 1 sans filters double avec filters (96)\\n#   - 1 sans filters double avec filters (128)\\n\\n# * 5 convnets avec filters evolutifs (filter double sur mes blocs de 2 couches de convolution)\\n#   - 1 filters double (2) avec filters (16)  (va faire 16, 32, 64, 128)\\n#   - 1 filters double (2) avec filters (32)  (va faire 32, 64, 128, 256)\\n#   - 1 filters double (4) avec filters (16)  (va faire 16, 16, 32, 32)\\n#   - 1 filters double (4) avec filters (32)  (va faire 32, 32, 64, 64)\\n#   - 1 filters double (4) avec filters (64)  (va faire 64, 64, 128, 128)\\n\\nlist_indiv_id = [\\'1\\']\\nlist_epochs = [3]\\nlist_batch_size = [50]\\nlist_nb_layers = [7]\\nlist_l1 = [0]\\nlist_l2 = [0]\\nlist_batch_norm = [0]\\nlist_dropout = [0]\\nlist_filters_per_layers = [16]\\nlist_filters_double = [0]\\nlist_MLP_end = [128]\\nlist_activation = [\\'relu\\']\\nlist_kernel = [(3,3)]\\nlist_padding = [\\'same\\']\\nlist_max_or_avg_pool = [\\'max\\']\\nlist_pool_frequency = [3]\\nlist_learning_r = [0.001]\\nlist_momentum = [0.9]\\nlist_optimizer = [\\'Adam\\']\\n\\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\\nmain_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Real Test 2 -> importance des filters (UNIQUEMENT FILTERS 16 FIXE)\n",
    "# CONSTANTES : nb_layers = 8, batch_size = 50, epochs = 50, lr = 0.001, momentum = 0.9, optimizer Adam, \n",
    "#padding = same, maxpool, relu, kernel = (3,3), MLP avec 128 neuronnes, pas de regularization\n",
    "# \n",
    "# * 5 convnets avec filters fixes\n",
    "#   - 1 sans filters double avec filters (16)\n",
    "#   - 1 sans filters double avec filters (32)\n",
    "#   - 1 sans filters double avec filters (64)\n",
    "#   - 1 sans filters double avec filters (96)\n",
    "#   - 1 sans filters double avec filters (128)\n",
    "\n",
    "# * 5 convnets avec filters evolutifs (filter double sur mes blocs de 2 couches de convolution)\n",
    "#   - 1 filters double (2) avec filters (16)  (va faire 16, 32, 64, 128)\n",
    "#   - 1 filters double (2) avec filters (32)  (va faire 32, 64, 128, 256)\n",
    "#   - 1 filters double (4) avec filters (16)  (va faire 16, 16, 32, 32)\n",
    "#   - 1 filters double (4) avec filters (32)  (va faire 32, 32, 64, 64)\n",
    "#   - 1 filters double (4) avec filters (64)  (va faire 64, 64, 128, 128)\n",
    "\n",
    "list_indiv_id = ['1']\n",
    "list_epochs = [3]\n",
    "list_batch_size = [50]\n",
    "list_nb_layers = [7]\n",
    "list_l1 = [0]\n",
    "list_l2 = [0]\n",
    "list_batch_norm = [0]\n",
    "list_dropout = [0]\n",
    "list_filters_per_layers = [16]\n",
    "list_filters_double = [0]\n",
    "list_MLP_end = [128]\n",
    "list_activation = ['relu']\n",
    "list_kernel = [(3,3)]\n",
    "list_padding = ['same']\n",
    "list_max_or_avg_pool = ['max']\n",
    "list_pool_frequency = [3]\n",
    "list_learning_r = [0.001]\n",
    "list_momentum = [0.9]\n",
    "list_optimizer = ['Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real Test 3 -> importance du MLP avant la softmax\n",
    "# CONSTANTES : nb_layers = 8, batch_size = 50, epochs = 100, lr = 0.001, momentum = 0.9, optimizer Adam, \n",
    "#padding = same, maxpool, relu, kernel = (3,3), filters_per_layer = 64 filters_double = 0, pas de regularization\n",
    "# \n",
    "\n",
    "# * 1 convent sans MLP\n",
    "\n",
    "# * 5 convnets avec MLP\n",
    "#   - 1 avec (MLP 32)\n",
    "#   - 1 avec (MLP 64)\n",
    "#   - 1 avec (MLP 128)\n",
    "#   - 1 avec (MLP 256)\n",
    "#   - 1 avec (MLP 512)\n",
    "\n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6']\n",
    "list_epochs = [5, 5, 5, 5, 5, 5]\n",
    "list_batch_size = [50, 50, 50, 50, 50, 50]\n",
    "list_nb_layers = [8,8,8,8,8,8]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0, 0, 0, 0, 0, 0]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0]\n",
    "list_MLP_end = [0, 32, 64, 128, 256, 512]\n",
    "list_activation = ['relu','relu','relu','relu','relu','relu']\n",
    "list_kernel = [(3,3),(3,3),(3,3),(3,3),(3,3),(3,3)]\n",
    "list_padding = ['same','same','same','same','same','same']\n",
    "list_max_or_avg_pool = ['max','max','max','max','max','max']\n",
    "list_pool_frequency = [2,2,2,2,2,2]\n",
    "list_learning_r = [0.001,0.001,0.001,0.001,0.001,0.001]\n",
    "list_momentum = [0.9,0.9,0.9,0.9,0.9,0.9]\n",
    "list_optimizer = ['Adam','Adam','Adam','Adam','Adam','Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\convnets\\\\logs_\" + current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIIDw6pHdJbK"
   },
   "source": [
    "### Fonctions pour préparer le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tSU6Hl2Ruv0"
   },
   "outputs": [],
   "source": [
    "# Chargement des datasets de train et de validation + one hot encoding\n",
    "def load_dataset():\n",
    "    # Chargement des données cifar10\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode encoding sur les labels\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "# Normalisation pour accroître la vitesse du modèle (en redimensionnant les pixels)\n",
    "def prep_pixels(train, test):\n",
    "    # Convertion des int en float\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # Normalisation pour avoir des nombres entre 0 et 1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # Retourner les images normalisées\n",
    "    return train_norm, test_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTdXP2D-c2D5"
   },
   "source": [
    "### Classe Python pour définir les individus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jP0CrTpRuv8"
   },
   "outputs": [],
   "source": [
    "# Classe pour les convnets\n",
    "class IndividuConvnets:\n",
    "    def __init__(self, indiv_id='1', epochs=10, batch_size=1, nb_layers=2, l1=0, l2=0, batch_norm=0, dropout=0, filters_per_layers=64, filters_double=6, MLP_end=0, activation='relu', kernel=(3,3), padding='same', max_or_avg_pool=0, pool_frequency=2, learning_r=0.01, momentum=0.9, optimizer='SGD'):\n",
    "        # Initialisation de nos variables\n",
    "        self.time_fit = datetime.datetime.now()\n",
    "        self.my_reguralizer = None\n",
    "        \n",
    "        if nb_layers < 2:\n",
    "            self.nb_layers = 2\n",
    "        else:\n",
    "            self.nb_layers = nb_layers\n",
    "            \n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.indiv_id = indiv_id\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = dropout\n",
    "        self.filters_per_layers = filters_per_layers\n",
    "        \n",
    "        # Création d'un variable qui va garder la valeur de filters_per_layers (elle changera dans le modèle)\n",
    "        self.keep_filters_per_layers = filters_per_layers\n",
    "        \n",
    "        \n",
    "        # filters_double doit être supérieur ou égal à 2 OU égal à 0\n",
    "        if filters_double < 2:\n",
    "            self.filters_double = 0\n",
    "        else : \n",
    "            self.filters_double = filters_double\n",
    "        \n",
    "        if MLP_end < 0:\n",
    "            self.MLP_end = 0\n",
    "        else:\n",
    "            self.MLP_end = MLP_end\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.kernel = kernel\n",
    "        self.padding = padding\n",
    "        self.max_or_avg_pool = max_or_avg_pool\n",
    "        self.pool_frequency = pool_frequency\n",
    "        self.learning_r = learning_r\n",
    "        self.momentum = momentum\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    # ToString()\n",
    "    def __str__(self):\n",
    "        ma_liste = []\n",
    "        ma_liste.append(\"indiv_id:{},\\n \".format(self.indiv_id))\n",
    "        ma_liste.append(\"epochs:{},\\n \".format(self.epochs))\n",
    "        ma_liste.append(\"batch_size:{},\\n \".format(self.batch_size))\n",
    "        ma_liste.append(\"nb_layers:{},\\n \".format(self.nb_layers))\n",
    "        ma_liste.append(\"l1:{},\\n \".format(self.l1))\n",
    "        ma_liste.append(\"l2:{},\\n \".format(self.l2))\n",
    "        ma_liste.append(\"batch_norm:{},\\n \".format(self.batch_norm))\n",
    "        ma_liste.append(\"dropout:{},\\n \".format(self.dropout))\n",
    "        ma_liste.append(\"filters_per_layers:{},\\n \".format(self.filters_per_layers))\n",
    "        ma_liste.append(\"filters_double:{},\\n \".format(self.filters_double))\n",
    "        ma_liste.append(\"MLP_end:{},\\n \".format(self.MLP_end))\n",
    "        ma_liste.append(\"activation:{},\\n \".format(self.activation))\n",
    "        ma_liste.append(\"kernel:\\n \")\n",
    "        ma_liste.append(\"{},\\n \".format(self.kernel))\n",
    "        ma_liste.append(\"padding:{},\\n \".format(self.padding))\n",
    "        ma_liste.append(\"max_or_avg_pool:{}\\n\".format(self.max_or_avg_pool))\n",
    "        ma_liste.append(\"pool_frequency:{}\\n\".format(self.pool_frequency))\n",
    "        ma_liste.append(\"learning_r:{}\\n\".format(self.learning_r))\n",
    "        ma_liste.append(\"momentum:{}\\n\".format(self.momentum))\n",
    "        ma_liste.append(\"optimizer:{}\\n\".format(self.optimizer))\n",
    "            \n",
    "        return ma_liste\n",
    "    \n",
    "    # (Modele 2 conv + norm ? + pool) * X -> MLP -> softmax sortie 10 -> MODELE BLOC 2\n",
    "    # D'autres modeles seront crees par la suite\n",
    "    def create_and_train_model(self, trainX, trainY, testX, testY, main_directory):\n",
    "        start = datetime.datetime.now()\n",
    "        \n",
    "        # Choix d'un emplacement pour les logs\n",
    "        log_dir=main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\tensorboard_data\\\\\"\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        print(\"log dir = \",log_dir)\n",
    "        \n",
    "        # l1 et l2\n",
    "        if self.l1 > 0 and self.l2 > 0:\n",
    "            self.my_regularizer = regularizers.l1_l2(l1=self.l1 / self.nb_layers, l2=self.l2 / self.nb_layers)\n",
    "        if self.l1 > 0:\n",
    "            self.my_regularizer = regularizers.l1(self.l1 / self.nb_layers)\n",
    "        elif self.l2 > 0:\n",
    "            self.my_regularizer = regularizers.l2(self.l2 / self.nb_layers)\n",
    "        else:\n",
    "            self.my_reguralizer = None\n",
    "            \n",
    "        # Definir notre modèle basique\n",
    "        model = Sequential()\n",
    "\n",
    "        # Faire toutes les convs nécessaires (conv * 2 + max pool)\n",
    "        counter_filters_double = 0 # Var pour doubler les filtres\n",
    "        counter_pool_freq = 0 # var pour savoir où placer les couches de pooling\n",
    "        counter_pool = 0 # var pour nommer les max / avg pool\n",
    "        \n",
    "        for i in range(0, self.nb_layers):\n",
    "            \n",
    "            print(\"counter_pool_freq = \", counter_pool_freq)\n",
    "            # Traitement pour doubler les filtres\n",
    "            if counter_filters_double >= self.filters_double and self.filters_double != 0:\n",
    "                self.filters_per_layers = self.filters_per_layers * 2\n",
    "                print(\"filters = \", self.filters_per_layers)\n",
    "                counter_filters_double = 0\n",
    "            \n",
    "            # Première conv, on fixe l'input shape\n",
    "            if i == 0:\n",
    "                model.add(Conv2D(self.filters_per_layers, self.kernel, activation=self.activation, kernel_regularizer=self.my_reguralizer, padding=self.padding,input_shape=(32, 32, 3), name='conv_'+str(self.filters_per_layers)+'_'+str(i+1)))\n",
    "            else:\n",
    "                # Couche de conv + rajouts selon nos hyperparams\n",
    "                model.add(Conv2D(self.filters_per_layers, self.kernel, activation=self.activation, kernel_regularizer=self.my_reguralizer, padding=self.padding, name='conv_'+str(self.filters_per_layers)+'_'+str(i+1)))\n",
    "            \n",
    "            # Après avoir créé une conv on incrémente nos compteurs (sauf counter_pool)\n",
    "            counter_filters_double = counter_filters_double + 1\n",
    "            counter_pool_freq = counter_pool_freq + 1\n",
    "            \n",
    "            # Ajouts de la regularization / du pooling selon les hyperparamètres saisis\n",
    "            \n",
    "            if self.batch_norm == 1:\n",
    "                    model.add(BatchNormalization( name='batchnorm_'+str(i+1)))\n",
    "            \n",
    "            if self.pool_frequency == counter_pool_freq:    \n",
    "                #go max ou avg pooling\n",
    "                if self.max_or_avg_pool == 'max':\n",
    "                    model.add(MaxPooling2D((2, 2), padding='same', name='max_pool_'+str(counter_pool+1)))\n",
    "                    counter_pool = counter_pool + 1\n",
    "                else:\n",
    "                    model.add(AveragePooling2D((2, 2), padding='same', name='avg_pool_'+str(counter_pool+1)))\n",
    "                    counter_pool = counter_pool + 1\n",
    "                counter_pool_freq = 0        \n",
    "        \n",
    "        # Fin des convs -> neural network classique\n",
    "        model.add(Flatten(name='Flatten'))\n",
    "        \n",
    "        #tTrain dans un MLP avant la fin si on le souhaite\n",
    "        if self.MLP_end > 0:\n",
    "            model.add(Dense(128, activation='relu', kernel_regularizer=self.my_reguralizer, name='MLP_'+str(self.MLP_end)))\n",
    "\n",
    "            #mettre dropout sur les Dense, pas opti sur les convnets (mais on peut le faire pour le démontrer ??)\n",
    "            if self.dropout > 0:\n",
    "                model.add(Dropout(self.dropout))\n",
    "        \n",
    "        #notre output\n",
    "        model.add(Dense(10, activation='softmax', name='output')) \n",
    "\n",
    "        # Compiler le modele\n",
    "        if self.optimizer == 'SGD':\n",
    "            print(\"SGD, learning_r = \", self.learning_r, \" momentum = \", self.momentum, \"\\n\")\n",
    "            opt = SGD(lr=self.learning_r, momentum=self.momentum)\n",
    "        else:\n",
    "            print(\"Adam learning_r = \", self.learning_r, \" momentum = \", self.momentum, \"\\n\")\n",
    "            opt = Adam(lr=self.learning_r, beta_1=self.momentum) # beta_1 => notation pour momentum Adam\n",
    "        model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Entrainer le modele\n",
    "        history = model.fit(trainX, trainY, epochs=self.epochs, batch_size=self.batch_size, validation_data=(testX, testY), verbose=1, callbacks=[tensorboard_callback])\n",
    "        \n",
    "        # Garder une trace du temps nécessaire pour fit (peut être pas la meilleure méthode)\n",
    "        end = datetime.datetime.now()\n",
    "        self.time_fit = end - start\n",
    "        print(\"\\nTime for fit = \", round(self.time_fit.total_seconds(),2)) # Round avec total_seconds()\n",
    "        \n",
    "        #Arpès que le fit soit fait, remettre filters_per_layers à sa valeur initiale pour un meilleur log \n",
    "        self.filters_per_layers = self.keep_filters_per_layers\n",
    "\n",
    "        return history, model\n",
    "    \n",
    "    \n",
    "    def save_model(self, history, model, main_directory, current_time):\n",
    "        \n",
    "        # Sauvegarde du modèle\n",
    "        plot_model(model, \"model.png\")\n",
    "        \n",
    "        # Deplacement modele au bon endroit\n",
    "        shutil.move(os.getcwd()+\"\\\\model.png\", main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\model.png\")\n",
    "        \n",
    "        # Afficher nos résultats dans un graphique matplotlib sauvegardé\n",
    "        pyplot.gcf().subplots_adjust(hspace = 0.5)\n",
    "\n",
    "        # Afficher la loss\n",
    "        pyplot.subplot(211)\n",
    "        pyplot.title('Cross Entropy Loss')\n",
    "        pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "        pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "        \n",
    "        # Afficher l'accuracy\n",
    "        pyplot.subplot(212)\n",
    "        pyplot.title('Classification Accuracy')\n",
    "        pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "        pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "        \n",
    "        # Sauvegarde\n",
    "        filename = main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\\"\n",
    "        pyplot.savefig(filename + 'plot.png')\n",
    "        pyplot.close()\n",
    "       \n",
    "        \n",
    "        print(\"LOSS : \", round(history.history['loss'][self.epochs-1].item(), 3))\n",
    "        print(\"VAL_LOSS : \", round(history.history['val_loss'][self.epochs-1].item(), 3))\n",
    "        print(\"ACCURACY : \", round(history.history['accuracy'][self.epochs-1].item(), 3))\n",
    "        print(\"VAL_ACCURACY : \", round(history.history['val_accuracy'][self.epochs-1].item(), 3))\n",
    "        \n",
    "        # attributs pour créer les csv indivudels et le csv global\n",
    "        self.loss = round(history.history['loss'][self.epochs-1].item(), 3)\n",
    "        self.val_loss = round(history.history['val_loss'][self.epochs-1].item(), 3)\n",
    "        self.accuracy = round(history.history['accuracy'][self.epochs-1].item(), 3)\n",
    "        self.val_accuracy = round(history.history['val_accuracy'][self.epochs-1].item(), 3)\n",
    "        self.time_taken = round(self.time_fit.total_seconds(),2)\n",
    "        \n",
    "        # Créer un dataframe pandas (avec hyperparams) et le sauvegarder en CSV\n",
    "        df = pd.DataFrame({'indiv_id': [self.indiv_id],\n",
    "                           'epochs': [self.epochs],\n",
    "                           'batch_size': [self.batch_size],\n",
    "                           'nb_layers': [self.nb_layers],\n",
    "                           'l1': [self.l1],\n",
    "                           'l2': [self.l2],\n",
    "                           'batch_norm': [self.batch_norm],\n",
    "                           'dropout': [self.dropout],\n",
    "                           'filters_per_layers': [self.filters_per_layers],\n",
    "                           'filters_double': [self.filters_double],\n",
    "                           'MLP_end': [self.MLP_end],\n",
    "                           'activation': [self.activation],\n",
    "                           'kernel': [self.kernel],\n",
    "                           'padding': [self.padding],\n",
    "                           'max_or_avg_pool': [self.max_or_avg_pool],\n",
    "                           'pool_frequency': [self.pool_frequency],\n",
    "                           'loss': [self.loss],\n",
    "                           'val_loss': [self.val_loss],\n",
    "                           'accuracy': [self.accuracy],\n",
    "                           'val_accuracy': [self.val_accuracy],\n",
    "                           'time_taken' : [self.time_taken],\n",
    "                           'learning_r' : [self.learning_r],\n",
    "                           'momentum' : [self.momentum],\n",
    "                           'optimizer' : [self.optimizer]\n",
    "                          })\n",
    "        \n",
    "        df.to_csv(path_or_buf=filename+\"recap.csv\",index=False)\n",
    "    \n",
    "    # Lance toutes les étapes\n",
    "    def exec_indiv(self, main_directory, current_time):\n",
    "        \n",
    "        # Charger les données\n",
    "        trainX, trainY, testX, testY = load_dataset()\n",
    "        \n",
    "        # Normaliser les données\n",
    "        trainX, testX = prep_pixels(trainX, testX)\n",
    "        \n",
    "        print(\"TrainX shape = \",np.shape(trainX))\n",
    "        print(\"TestX shape = \",np.shape(testX), \"\\n\")\n",
    "        # Créer et entrainer le modele\n",
    "        history, model = self.create_and_train_model(trainX, trainY, testX, testY, main_directory)\n",
    "        \n",
    "        # Sauvegarder le modèle\n",
    "        save = self.save_model(history, model, main_directory, current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buU7Dsqfs6Cv"
   },
   "source": [
    "### Classe Python qui va démarrer les tests des neural nets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Im542kkes6Cx"
   },
   "outputs": [],
   "source": [
    "# Classe générale qui va nous servir à effectuer des actions sur des individus\n",
    "class MyTraining:\n",
    "    # Prends un ID et une liste d'individus \n",
    "    def __init__(self, id_train, indiv_list):\n",
    "        \n",
    "        self.id_train = id_train\n",
    "        self.indiv_list = indiv_list\n",
    "    \n",
    "    def train(self, main_directory, current_time):\n",
    "        \n",
    "        print(\"Start training\\n\")\n",
    "        \n",
    "        for indiv in self.indiv_list:\n",
    "            print(\"indiv \", indiv.indiv_id, \"\\n\")\n",
    "            indiv.exec_indiv(main_directory, current_time)\n",
    "            print(\"-----------------------------------------------------------------\\n\")\n",
    "        \n",
    "        # Fusion des csv \n",
    "        merge_csv = pd.DataFrame(columns=['indiv_id', 'epochs', 'nb_layers', 'l1', 'l2', 'batch_norm', 'dropout',\n",
    "                                          'filters_per_layers', 'filters_double', 'MLP_end', 'activation', 'kernel',\n",
    "                                          'padding','max_or_avg_pool', 'pool_frequency', 'loss', 'val_loss', 'accuracy', 'val_accuracy',\n",
    "                                          'time_taken','learning_r', 'momentum', 'optimizer'])\n",
    "        \n",
    "        for indiv in self.indiv_list:\n",
    "            merge_csv = merge_csv.append(\n",
    "                             {'indiv_id': indiv.indiv_id, 'epochs': indiv.epochs, 'batch_size': indiv.batch_size,\n",
    "                              'nb_layers' : indiv.nb_layers,'l1' : indiv.l1, 'l2' : indiv.l2, 'batch_norm': indiv.batch_norm,\n",
    "                              'dropout' : indiv.dropout,'filters_per_layers' : indiv.filters_per_layers,\n",
    "                              'filters_double' : indiv.filters_double,'MLP_end' : indiv.MLP_end,'activation' : indiv.activation,\n",
    "                              'kernel' : indiv.kernel,'padding' : indiv.padding, 'max_or_avg_pool' : indiv.max_or_avg_pool,\n",
    "                              'pool_frequency' : indiv.pool_frequency, 'loss' : indiv.loss,'val_loss' : indiv.val_loss,\n",
    "                              'accuracy' : indiv.accuracy, 'val_accuracy' : indiv.val_accuracy,'time_taken' : indiv.time_taken,\n",
    "                              'learning_r' : indiv.learning_r,'momentum': indiv.momentum, 'optimizer' : indiv.optimizer\n",
    "                             },ignore_index=True)\n",
    "        \n",
    "        # sauvegarde\n",
    "        merge_csv.to_csv(main_directory+\"\\\\combined_recap.csv\", index=False)\n",
    "            \n",
    "    \n",
    "    def all_indiv(self):\n",
    "        \n",
    "        # Affiche les caractéristiques de l'ensemble des individus\n",
    "        for indiv in self.indiv_list:\n",
    "            print('\\n'.join(indiv.__str__()))\n",
    "            for tir in range(80): print('-', end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIGBnz_4bj74"
   },
   "source": [
    "### Traitement général (train de l'ensemble des modèles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjIV_jF6RuwF",
    "outputId": "6759a6e1-3664-4264-c87d-18266651af09",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indiv_id:1,\n",
      " \n",
      "epochs:5,\n",
      " \n",
      "batch_size:50,\n",
      " \n",
      "nb_layers:8,\n",
      " \n",
      "l1:0,\n",
      " \n",
      "l2:0,\n",
      " \n",
      "batch_norm:0,\n",
      " \n",
      "dropout:0,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:0,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:max\n",
      "\n",
      "pool_frequency:2\n",
      "\n",
      "learning_r:0.001\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "indiv_id:2,\n",
      " \n",
      "epochs:5,\n",
      " \n",
      "batch_size:50,\n",
      " \n",
      "nb_layers:8,\n",
      " \n",
      "l1:0,\n",
      " \n",
      "l2:0,\n",
      " \n",
      "batch_norm:0,\n",
      " \n",
      "dropout:0,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:32,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:max\n",
      "\n",
      "pool_frequency:2\n",
      "\n",
      "learning_r:0.001\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "indiv_id:3,\n",
      " \n",
      "epochs:5,\n",
      " \n",
      "batch_size:50,\n",
      " \n",
      "nb_layers:8,\n",
      " \n",
      "l1:0,\n",
      " \n",
      "l2:0,\n",
      " \n",
      "batch_norm:0,\n",
      " \n",
      "dropout:0,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:64,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:max\n",
      "\n",
      "pool_frequency:2\n",
      "\n",
      "learning_r:0.001\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "indiv_id:4,\n",
      " \n",
      "epochs:5,\n",
      " \n",
      "batch_size:50,\n",
      " \n",
      "nb_layers:8,\n",
      " \n",
      "l1:0,\n",
      " \n",
      "l2:0,\n",
      " \n",
      "batch_norm:0,\n",
      " \n",
      "dropout:0,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:128,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:max\n",
      "\n",
      "pool_frequency:2\n",
      "\n",
      "learning_r:0.001\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "indiv_id:5,\n",
      " \n",
      "epochs:5,\n",
      " \n",
      "batch_size:50,\n",
      " \n",
      "nb_layers:8,\n",
      " \n",
      "l1:0,\n",
      " \n",
      "l2:0,\n",
      " \n",
      "batch_norm:0,\n",
      " \n",
      "dropout:0,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:256,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:max\n",
      "\n",
      "pool_frequency:2\n",
      "\n",
      "learning_r:0.001\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "indiv_id:6,\n",
      " \n",
      "epochs:5,\n",
      " \n",
      "batch_size:50,\n",
      " \n",
      "nb_layers:8,\n",
      " \n",
      "l1:0,\n",
      " \n",
      "l2:0,\n",
      " \n",
      "batch_norm:0,\n",
      " \n",
      "dropout:0,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:0,\n",
      " \n",
      "MLP_end:512,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:max\n",
      "\n",
      "pool_frequency:2\n",
      "\n",
      "learning_r:0.001\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start training\n",
      "\n",
      "indiv  1 \n",
      "\n",
      "TrainX shape =  (50000, 32, 32, 3)\n",
      "TestX shape =  (10000, 32, 32, 3) \n",
      "\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\logs\\convnets\\logs_2020-02-03-10-17\\log_1\\tensorboard_data\\\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "Adam learning_r =  0.001  momentum =  0.9 \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 32s 649us/sample - loss: 1.5757 - accuracy: 0.4134 - val_loss: 1.2066 - val_accuracy: 0.5716\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 27s 535us/sample - loss: 1.0783 - accuracy: 0.6166 - val_loss: 0.9457 - val_accuracy: 0.6660\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 31s 614us/sample - loss: 0.8546 - accuracy: 0.6981 - val_loss: 0.8832 - val_accuracy: 0.6914\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 30s 609us/sample - loss: 0.7128 - accuracy: 0.7507 - val_loss: 0.7820 - val_accuracy: 0.7255\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 31s 611us/sample - loss: 0.6231 - accuracy: 0.7794 - val_loss: 0.7319 - val_accuracy: 0.7528\n",
      "\n",
      "Time for fit =  152.71\n",
      "LOSS :  0.623\n",
      "VAL_LOSS :  0.732\n",
      "ACCURACY :  0.779\n",
      "VAL_ACCURACY :  0.753\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "indiv  2 \n",
      "\n",
      "TrainX shape =  (50000, 32, 32, 3)\n",
      "TestX shape =  (10000, 32, 32, 3) \n",
      "\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\logs\\convnets\\logs_2020-02-03-10-17\\log_2\\tensorboard_data\\\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "Adam learning_r =  0.001  momentum =  0.9 \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 31s 616us/sample - loss: 1.6193 - accuracy: 0.3982 - val_loss: 1.2368 - val_accuracy: 0.5469\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 29s 575us/sample - loss: 1.1200 - accuracy: 0.5986 - val_loss: 1.0476 - val_accuracy: 0.6293\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 29s 577us/sample - loss: 0.9178 - accuracy: 0.6731 - val_loss: 0.8942 - val_accuracy: 0.6889\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 29s 587us/sample - loss: 0.7765 - accuracy: 0.7283 - val_loss: 0.7847 - val_accuracy: 0.7248\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 29s 576us/sample - loss: 0.6775 - accuracy: 0.7626 - val_loss: 0.7558 - val_accuracy: 0.7373\n",
      "\n",
      "Time for fit =  148.18\n",
      "LOSS :  0.677\n",
      "VAL_LOSS :  0.756\n",
      "ACCURACY :  0.763\n",
      "VAL_ACCURACY :  0.737\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "indiv  3 \n",
      "\n",
      "TrainX shape =  (50000, 32, 32, 3)\n",
      "TestX shape =  (10000, 32, 32, 3) \n",
      "\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\logs\\convnets\\logs_2020-02-03-10-17\\log_3\\tensorboard_data\\\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "Adam learning_r =  0.001  momentum =  0.9 \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 24s 488us/sample - loss: 1.6349 - accuracy: 0.3913 - val_loss: 1.2429 - val_accuracy: 0.5446\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 24s 473us/sample - loss: 1.1051 - accuracy: 0.6022 - val_loss: 1.1772 - val_accuracy: 0.5757\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 24s 478us/sample - loss: 0.8839 - accuracy: 0.6851 - val_loss: 0.8641 - val_accuracy: 0.7011\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 24s 485us/sample - loss: 0.7444 - accuracy: 0.7385 - val_loss: 0.7908 - val_accuracy: 0.7293\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 24s 488us/sample - loss: 0.6482 - accuracy: 0.7734 - val_loss: 0.7338 - val_accuracy: 0.7473\n",
      "\n",
      "Time for fit =  122.54\n",
      "LOSS :  0.648\n",
      "VAL_LOSS :  0.734\n",
      "ACCURACY :  0.773\n",
      "VAL_ACCURACY :  0.747\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "indiv  4 \n",
      "\n",
      "TrainX shape =  (50000, 32, 32, 3)\n",
      "TestX shape =  (10000, 32, 32, 3) \n",
      "\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\logs\\convnets\\logs_2020-02-03-10-17\\log_4\\tensorboard_data\\\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "Adam learning_r =  0.001  momentum =  0.9 \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "   50/50000 [..............................] - ETA: 23:13 - loss: 2.3022 - accuracy: 0.1400WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.107228). Check your callbacks.\n",
      "50000/50000 [==============================] - 26s 522us/sample - loss: 1.6085 - accuracy: 0.4041 - val_loss: 1.2399 - val_accuracy: 0.5550\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 24s 487us/sample - loss: 1.0982 - accuracy: 0.6028 - val_loss: 0.9891 - val_accuracy: 0.6539\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 25s 500us/sample - loss: 0.8777 - accuracy: 0.6866 - val_loss: 0.8821 - val_accuracy: 0.6936\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 25s 509us/sample - loss: 0.7435 - accuracy: 0.7379 - val_loss: 0.8051 - val_accuracy: 0.7266\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 26s 520us/sample - loss: 0.6468 - accuracy: 0.7748 - val_loss: 0.7374 - val_accuracy: 0.7495\n",
      "\n",
      "Time for fit =  129.75\n",
      "LOSS :  0.647\n",
      "VAL_LOSS :  0.737\n",
      "ACCURACY :  0.775\n",
      "VAL_ACCURACY :  0.749\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "indiv  5 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainX shape =  (50000, 32, 32, 3)\n",
      "TestX shape =  (10000, 32, 32, 3) \n",
      "\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\logs\\convnets\\logs_2020-02-03-10-17\\log_5\\tensorboard_data\\\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "Adam learning_r =  0.001  momentum =  0.9 \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "   50/50000 [..............................] - ETA: 40:40 - loss: 2.3025 - accuracy: 0.1000WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.119681). Check your callbacks.\n",
      "50000/50000 [==============================] - 29s 580us/sample - loss: 1.6860 - accuracy: 0.3711 - val_loss: 1.3054 - val_accuracy: 0.5276\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 26s 528us/sample - loss: 1.1694 - accuracy: 0.5790 - val_loss: 1.0109 - val_accuracy: 0.6358\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 28s 565us/sample - loss: 0.9335 - accuracy: 0.6665 - val_loss: 0.9556 - val_accuracy: 0.6628\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 26s 521us/sample - loss: 0.7872 - accuracy: 0.7213 - val_loss: 0.8181 - val_accuracy: 0.7114\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 26s 523us/sample - loss: 0.6809 - accuracy: 0.7590 - val_loss: 0.8320 - val_accuracy: 0.7211\n",
      "\n",
      "Time for fit =  139.62\n",
      "LOSS :  0.681\n",
      "VAL_LOSS :  0.832\n",
      "ACCURACY :  0.759\n",
      "VAL_ACCURACY :  0.721\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "indiv  6 \n",
      "\n",
      "TrainX shape =  (50000, 32, 32, 3)\n",
      "TestX shape =  (10000, 32, 32, 3) \n",
      "\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\logs\\convnets\\logs_2020-02-03-10-17\\log_6\\tensorboard_data\\\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "counter_pool_freq =  0\n",
      "counter_pool_freq =  1\n",
      "Adam learning_r =  0.001  momentum =  0.9 \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "   50/50000 [..............................] - ETA: 36:13 - loss: 2.2985 - accuracy: 0.1200WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.106232). Check your callbacks.\n",
      "50000/50000 [==============================] - 28s 559us/sample - loss: 1.6799 - accuracy: 0.3677 - val_loss: 1.3103 - val_accuracy: 0.5282\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 27s 537us/sample - loss: 1.1836 - accuracy: 0.5758 - val_loss: 1.0552 - val_accuracy: 0.6261\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 27s 542us/sample - loss: 0.9614 - accuracy: 0.6565 - val_loss: 0.9419 - val_accuracy: 0.6675\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 27s 541us/sample - loss: 0.8162 - accuracy: 0.7110 - val_loss: 0.8534 - val_accuracy: 0.7000\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 27s 548us/sample - loss: 0.7108 - accuracy: 0.7505 - val_loss: 0.8020 - val_accuracy: 0.7248\n",
      "\n",
      "Time for fit =  140.32\n",
      "LOSS :  0.711\n",
      "VAL_LOSS :  0.802\n",
      "ACCURACY :  0.751\n",
      "VAL_ACCURACY :  0.725\n",
      "-----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Création des individus (des neurals nets, ici convnet)\n",
    "list_indiv = []\n",
    "for num in range(len(list_indiv_id)):\n",
    "    list_indiv.append(IndividuConvnets(\n",
    "        list_indiv_id[num],\n",
    "          list_epochs[num],\n",
    "          list_batch_size[num],\n",
    "          list_nb_layers[num],\n",
    "          list_l1[num],\n",
    "          list_l2[num],\n",
    "          list_batch_norm[num],\n",
    "          list_dropout[num],\n",
    "          list_filters_per_layers[num],\n",
    "          list_filters_double[num],\n",
    "          list_MLP_end[num],\n",
    "          list_activation[num],\n",
    "          list_kernel[num],\n",
    "          list_padding[num],\n",
    "          list_max_or_avg_pool[num],\n",
    "          list_pool_frequency[num],\n",
    "          list_learning_r[num],\n",
    "          list_momentum[num],\n",
    "          list_optimizer[num]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Chargement de la classe training, affichag\n",
    "training_1 = MyTraining(1, list_indiv)\n",
    "training_1.all_indiv()\n",
    "training_1.train(main_directory, current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxpbXOw1RuwM"
   },
   "source": [
    "### Partie tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jm6vpk3MRuwQ"
   },
   "outputs": [],
   "source": [
    "# Procedure pour utiliser tensorboard\n",
    "#  1 load la première cell\n",
    "#  2 utiliser la derniere cell avec --logdir (précisez bien votre répertoire, plus sur que ça\n",
    "#    fonctionne avec une string \"mon_path\"\n",
    "#  3 Vous NE POURREZ PLUS update tensorboard sur ce port et il y aura des bugs, pour éviter ça\n",
    "#    quand vous voulez faire une update, fermez jupyter notebook (shutdown total) et réouvrez le \n",
    "#    OU, faites kernel->interrupt et changez de port + de folder de log\n",
    "\n",
    "#si vous voulez tenter de tuer des process\n",
    "#os.system(\"taskkill /im tensorboard.exe /f\") #kill tous les processus qui utilisent tensorboard\n",
    "#os.system('!kill 18776') #kill le processus X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Lj738x3Ruwa",
    "outputId": "b7c6a334-5e14-4d18-fc6e-7427bf231326"
   },
   "outputs": [],
   "source": [
    "# Liste des ports utilisés par tensorboard, attention ça se remplit vite et il faut kill jupyter pour clean\n",
    "from tensorboard import notebook\n",
    "notebook.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seEJieCtRuwq",
    "outputId": "051fb9d1-b28c-4221-c61c-a8dfbb158362",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code pour démarrer tensorboard dans le dossier souhaité [PRECISEZ BIEN LE DOSSIER ICI]\n",
    "%tensorboard --logdir \"CONVNETS_20200119-0243\\logs_20200119-093909\\tensorboard_data\" --port=6066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upoC4cmYRuwz",
    "outputId": "c0390d77-555a-49f0-934c-b2740b5e2ba0"
   },
   "outputs": [],
   "source": [
    "# Si vous avez la folie des grandeurs\n",
    "notebook.display(port=6066, height=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLuEIeDvRuw7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Fichier CSV combined_recap + Graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TO0o33QYs6FP",
    "outputId": "0ebfe25a-2965-4220-ccf7-914ae220328f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Commandes pandas utiles\n",
    "data_csv = pd.read_csv(main_directory + \"\\\\combined_recap.csv\")\n",
    "#data_csv = pd.read_csv(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\CONVNETS_20200119-2043\\\\combined_recap.csv\")\n",
    "data.head()\n",
    "#meilleure accuracy, moins pire loss par ex\n",
    "#data_csv.sort_values([\"elapsed\"], axis=0, \n",
    "                 #ascending=[False], inplace=True) \n",
    "\n",
    "# Afficher uniquement certaines colonnes\n",
    "#dataX = data_csv.filter(items=['elapsed', 'label'])\n",
    "\n",
    "#récupérer uniquement où la loss est < à X et ou kernel = (3,3) par exemple\n",
    "#dataX = data_csv.loc[(data_csv['elapsed'] > 700) & (data_csv['threadName'].str.contains('Thread Group 1-2'))]\n",
    "#dataX\n",
    "\n",
    "#pd.set_option('display.max_rows', data3.shape[0]+1) #nombre de row max à afficher\n",
    "#data_csv = pd.read_csv(main_directory+\"\\\\logs_20200116-204456\\\\recap.csv\")\n",
    "#data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGXNwkMCRuxE"
   },
   "outputs": [],
   "source": [
    "image = pyplot.imread(main_directory + \"\\\\logs_20200119-093909\\\\plot.png\")\n",
    "#image = pyplot.imread(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\logs_20200119-093909\\\\plot.png\")\n",
    "pyplot.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "colab": {
   "collapsed_sections": [],
   "name": "test_several_models_1701_modif.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
