{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7R0yO1XdBNh"
   },
   "source": [
    "### Démarrage de tensorboard et imports principaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rCCbrXXRuvi",
    "outputId": "7d91c098-39d8-4a89-adf8-3133ab8217de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Agrandir le notebook ?\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Démarrage de tensorboard pour notebook\n",
    "%load_ext tensorboard\n",
    "\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.activations import *\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import shutil  \n",
    "from math import ceil, floor\n",
    "\n",
    "from tensorflow.python.framework import ops #pour tenter de reset tensorboard, sans grand succès\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AF9xVI2bdzuT"
   },
   "source": [
    "### Hyper paramètres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Parametres de verification : \\n\\nlist_indiv_id = [\\'1\\', \\'2\\']\\nlist_epochs = [1, 1]\\nlist_batch_size = [100, 500]\\nlist_blocks_size = [2, 2]\\nlist_blocks_nb = [5, 7]\\nlist_l1 = [0.01, 0]\\nlist_l2 = [0.01, 0.001]\\nlist_batch_norm = [0, 1]\\nlist_dropout = [0, 0]\\nlist_filters_per_layers = [64, 32]\\nlist_filters_double = [2, 0] #défini quand doubler cette valeur\\nlist_filters_pool = [3, 2] #défini tout les combien de block est effectué une couche de pooling, ne peut être nul\\nlist_activation = [\\'relu\\', \\'relu\\']\\nlist_kernel = [(3,3), (3,3)]\\nlist_first_kernel = [(7,7), (7,7)]\\nlist_padding = [\\'same\\', \\'same\\']\\nlist_max_or_avg_pool = [\\'max\\', \\'avg\\']\\nlist_learning_r = [0.01, 0.01]\\nlist_momentum = [0.9, 0.85]\\nlist_optimizer = [\\'SGD\\', \\'Adam\\']\\n\\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\\nmain_directory = os.getcwd() + \"\\\\logs\\\\resnetsConv2d\\\\logs_\" + current_time\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Parametres de verification : \n",
    "\n",
    "list_indiv_id = ['1', '2']\n",
    "list_epochs = [1, 1]\n",
    "list_batch_size = [100, 500]\n",
    "list_blocks_size = [2, 2]\n",
    "list_blocks_nb = [5, 7]\n",
    "list_l1 = [0.01, 0]\n",
    "list_l2 = [0.01, 0.001]\n",
    "list_batch_norm = [0, 1]\n",
    "list_dropout = [0, 0]\n",
    "list_filters_per_layers = [64, 32]\n",
    "list_filters_double = [2, 0] #défini quand doubler cette valeur\n",
    "list_filters_pool = [3, 2] #défini tout les combien de block est effectué une couche de pooling, ne peut être nul\n",
    "list_activation = ['relu', 'relu']\n",
    "list_kernel = [(3,3), (3,3)]\n",
    "list_first_kernel = [(7,7), (7,7)]\n",
    "list_padding = ['same', 'same']\n",
    "list_max_or_avg_pool = ['max', 'avg']\n",
    "list_learning_r = [0.01, 0.01]\n",
    "list_momentum = [0.9, 0.85]\n",
    "list_optimizer = ['SGD', 'Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\resnetsConv2d\\\\logs_\" + current_time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametres de verification : \n",
    "\n",
    "list_indiv_id = ['1', '2']\n",
    "list_epochs = [50, 50]\n",
    "list_batch_size = [100, 100]\n",
    "list_blocks_size = [2, 2]\n",
    "list_blocks_nb = [4, 4]\n",
    "list_l1 = [0.0001, 0.0001]\n",
    "list_l2 = [0.0001, 0.0001]\n",
    "list_batch_norm = [1, 1]\n",
    "list_dropout = [0.4, 0.4]\n",
    "list_filters_per_layers = [64, 64]\n",
    "list_filters_double = [0, 0] #défini quand doubler cette valeur\n",
    "list_filters_pool = [1, 1] #défini tout les combien de block est effectué une couche de pooling, ne peut être nul\n",
    "list_activation = ['relu', 'relu']\n",
    "list_kernel = [(3,3), (3,3)]\n",
    "list_first_kernel = [(3,3), (3,3)]\n",
    "list_padding = ['same', 'same']\n",
    "list_max_or_avg_pool = ['avg', 'avg']\n",
    "list_learning_r = [0.001, 0.001]\n",
    "list_momentum = [0.9, 0.9]\n",
    "list_optimizer = ['Adam', 'Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\resnetsConv2d\\\\logs_\" + current_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Parametres de verification : \\n\\nlist_indiv_id = [\\'1\\', \\'2\\', \\'3\\', \\'4\\', \\'5\\', \\'6\\', \\'7\\', \\'8\\', \\'9\\']\\nlist_epochs = [100, 100, 100, 100, 100, 100, 100, 100, 100]\\nlist_batch_size = [100, 100, 100, 100, 100, 100, 100, 100, 100]\\nlist_blocks_size = [2, 2, 2, 2, 2, 2, 2, 2, 2]\\nlist_blocks_nb = [4, 4, 4, 4, 4, 4, 4, 4, 4]\\nlist_l1 = [0, 0, 0, 0, 0, 0, 0, 0, 0]\\nlist_l2 = [0, 0, 0, 0, 0, 0, 0, 0, 0]\\nlist_batch_norm = [0, 0, 0, 0, 0, 0, 0, 0, 0]\\nlist_dropout = [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\\nlist_filters_per_layers = [64, 64, 64, 64, 64, 64, 64, 64, 64]\\nlist_filters_double = [0, 0, 0, 0, 0, 0, 0, 0, 0] #défini quand doubler cette valeur\\nlist_filters_pool = [1, 2, 3, 4, 0, 1, 2, 3, 0] #défini tout les combien de block est effectué une couche de pooling, ne peut être nul\\nlist_activation = [\\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\']\\nlist_kernel = [(3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3)]\\nlist_first_kernel = [(7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7)]\\nlist_padding = [\\'same\\', \\'same\\', \\'same\\', \\'same\\', \\'same\\', \\'same\\', \\'same\\', \\'same\\', \\'same\\']\\nlist_max_or_avg_pool = [\\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\', \\'max\\', \\'max\\', \\'max\\', \\'max\\']\\nlist_learning_r = [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\\nlist_momentum = [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\\nlist_optimizer = [\\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\']\\n\\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\\nmain_directory = os.getcwd() + \"\\\\logs\\\\resnetsConv2d\\\\logs_\" + current_time'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Parametres de verification : \n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "list_epochs = [100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
    "list_batch_size = [100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
    "list_blocks_size = [2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
    "list_blocks_nb = [4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
    "list_l1 = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_l2 = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_batch_norm = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "list_dropout = [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0, 0, 0, 0] #défini quand doubler cette valeur\n",
    "list_filters_pool = [1, 2, 3, 4, 0, 1, 2, 3, 0] #défini tout les combien de block est effectué une couche de pooling, ne peut être nul\n",
    "list_activation = ['relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu']\n",
    "list_kernel = [(3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3)]\n",
    "list_first_kernel = [(7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7)]\n",
    "list_padding = ['same', 'same', 'same', 'same', 'same', 'same', 'same', 'same', 'same']\n",
    "list_max_or_avg_pool = ['avg', 'avg', 'avg', 'avg', 'avg', 'max', 'max', 'max', 'max']\n",
    "list_learning_r = [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
    "list_momentum = [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\n",
    "list_optimizer = ['Adam', 'Adam', 'Adam', 'Adam', 'Adam', 'Adam', 'Adam', 'Adam', 'Adam']\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\resnetsConv2d\\\\logs_\" + current_time'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Parametres de verification : \\n\\nlist_indiv_id = [\\'1\\', \\'2\\', \\'3\\', \\'4\\', \\'5\\', \\'6\\']\\nlist_epochs = [50, 50, 50, 50, 50, 50]\\nlist_batch_size = [256, 256, 256, 100, 100, 100]\\nlist_blocks_size = [2, 2, 2, 2, 2, 2]\\nlist_blocks_nb = [5, 5, 5, 5, 5, 5]\\nlist_filters_per_layers = [64, 64, 64, 64, 64, 64]\\nlist_filters_double = [0, 1, 2, 3, 4, 5] #défini quand doubler cette valeur\\nlist_filters_pool = [1, 1, 1, 1, 1, 1] #défini tout les combien de block est effectué une couche de pooling, ne peut être nul\\nlist_activation = [\\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\']\\nlist_kernel = [(3,3), (3,3), (3,3), (3,3), (3,3), (3,3)]\\nlist_first_kernel = [(3,3), (3,3), (3,3), (3,3), (3,3), (3,3)]\\nlist_padding = [\\'same\\', \\'same\\', \\'same\\', \\'same\\', \\'same\\', \\'same\\']\\nlist_max_or_avg_pool = [\\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\']\\nlist_learning_r = [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\\nlist_momentum = [0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\\nlist_optimizer = [\\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\']\\n\\n#regularisation\\nlist_l1 = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001]\\nlist_l2 = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001]\\nlist_batch_norm = [1, 1, 1, 1, 1, 1]\\nlist_dropout = [0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\\n\\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\\nmain_directory = os.getcwd() + \"\\\\logs\\\\resnetsConv2d\\\\logs_\" + current_time\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Parametres de verification : \n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6']\n",
    "list_epochs = [50, 50, 50, 50, 50, 50]\n",
    "list_batch_size = [256, 256, 256, 100, 100, 100]\n",
    "list_blocks_size = [2, 2, 2, 2, 2, 2]\n",
    "list_blocks_nb = [5, 5, 5, 5, 5, 5]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 1, 2, 3, 4, 5] #défini quand doubler cette valeur\n",
    "list_filters_pool = [1, 1, 1, 1, 1, 1] #défini tout les combien de block est effectué une couche de pooling, ne peut être nul\n",
    "list_activation = ['relu', 'relu', 'relu', 'relu', 'relu', 'relu']\n",
    "list_kernel = [(3,3), (3,3), (3,3), (3,3), (3,3), (3,3)]\n",
    "list_first_kernel = [(3,3), (3,3), (3,3), (3,3), (3,3), (3,3)]\n",
    "list_padding = ['same', 'same', 'same', 'same', 'same', 'same']\n",
    "list_max_or_avg_pool = ['avg', 'avg', 'avg', 'avg', 'avg', 'avg']\n",
    "list_learning_r = [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
    "list_momentum = [0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\n",
    "list_optimizer = ['Adam', 'Adam', 'Adam', 'Adam', 'Adam', 'Adam']\n",
    "\n",
    "#regularisation\n",
    "list_l1 = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001]\n",
    "list_l2 = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001]\n",
    "list_batch_norm = [1, 1, 1, 1, 1, 1]\n",
    "list_dropout = [0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\resnetsConv2d\\\\logs_\" + current_time\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Parametres de verification : \\n\\nlist_indiv_id = [\\'1\\', \\'2\\', \\'3\\', \\'4\\', \\'5\\', \\'6\\', \\'7\\', \\'8\\', \\'9\\', \\'10\\', \\'11\\', \\'12\\', \\'13\\', \\'14\\', \\'15\\']\\nlist_epochs = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\\nlist_batch_size = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\\nlist_blocks_size = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\\nlist_blocks_nb = [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\\nlist_filters_per_layers = [64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]\\nlist_filters_double = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \\nlist_filters_pool = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] \\nlist_activation = [\\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\', \\'relu\\']\\nlist_kernel = [(3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3)]\\nlist_first_kernel = [(7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7)]\\nlist_padding = [\\'same\\', \\'same\\', \\'same\\', \\'same\\', \\'same\\', \\'same\\', \\'same\\', \\'same\\', \\'same\\', \\'same\\', \\'same\\', \\'same\\', \\'same\\', \\'same\\', \\'same\\']\\nlist_max_or_avg_pool = [\\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\', \\'avg\\']\\nlist_learning_r = [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\\nlist_momentum = [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\\nlist_optimizer = [\\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\', \\'Adam\\']\\n\\n#regularisation\\nlist_l1 = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001]\\nlist_l2 = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001]\\nlist_batch_norm = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\\nlist_dropout = [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\\n\\ncurrent_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\\nmain_directory = os.getcwd() + \"\\\\logs\\\\resnetsConv2d\\\\logs_\" + current_time\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Parametres de verification : \n",
    "\n",
    "list_indiv_id = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15']\n",
    "list_epochs = [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
    "list_batch_size = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
    "list_blocks_size = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
    "list_blocks_nb = [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
    "list_filters_per_layers = [64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]\n",
    "list_filters_double = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \n",
    "list_filters_pool = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] \n",
    "list_activation = ['relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu']\n",
    "list_kernel = [(3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3), (3,3)]\n",
    "list_first_kernel = [(7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7), (7,7)]\n",
    "list_padding = ['same', 'same', 'same', 'same', 'same', 'same', 'same', 'same', 'same', 'same', 'same', 'same', 'same', 'same', 'same']\n",
    "list_max_or_avg_pool = ['avg', 'avg', 'avg', 'avg', 'avg', 'avg', 'avg', 'avg', 'avg', 'avg', 'avg', 'avg', 'avg', 'avg', 'avg']\n",
    "list_learning_r = [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
    "list_momentum = [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\n",
    "list_optimizer = ['Adam', 'Adam', 'Adam', 'Adam', 'Adam', 'Adam', 'Adam', 'Adam', 'Adam', 'Adam', 'Adam', 'Adam', 'Adam', 'Adam', 'Adam']\n",
    "\n",
    "#regularisation\n",
    "list_l1 = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001]\n",
    "list_l2 = [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001]\n",
    "list_batch_norm = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "list_dropout = [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "main_directory = os.getcwd() + \"\\\\logs\\\\resnetsConv2d\\\\logs_\" + current_time\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIIDw6pHdJbK"
   },
   "source": [
    "### Fonctions pour préparer le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tSU6Hl2Ruv0"
   },
   "outputs": [],
   "source": [
    "# Chargement des datasets de train et de validation + one hot encoding\n",
    "def load_dataset():\n",
    "    # Chargement des données cifar10\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode encoding sur les labels\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "# Normalisation pour accroître la vitesse du modèle (en redimensionnant les pixels)\n",
    "def prep_pixels(train, test):\n",
    "    # Convertion des int en float\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # Normalisation pour avoir des nombres entre 0 et 1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # Retourner les images normalisées\n",
    "    return train_norm, test_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTdXP2D-c2D5"
   },
   "source": [
    "### Classe Python pour définir les individus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jP0CrTpRuv8"
   },
   "outputs": [],
   "source": [
    "# Classe pour les convnets\n",
    "class IndividuResnetConv2d:\n",
    "    def __init__(self, indiv_id='1', epochs=10, batch_size=1, block_size=1, block_nb=1, l1=0, l2=0, batch_norm=0, dropout=0, filters_per_layers=64, filters_double=3, filters_pool=1, activation='relu', kernel=(3,3), first_kernel=(7,7), padding='same', max_or_avg_pool=0, learning_r=0.01, momentum=0.9, optimizer='SGD'):\n",
    "        # Initialisation de nos variables\n",
    "        self.time_fit = datetime.datetime.now()\n",
    "        self.my_reguralizer = None\n",
    "        \n",
    "        if block_size < 1:\n",
    "            self.block_size = 1\n",
    "        else:\n",
    "            self.block_size = block_size\n",
    "            \n",
    "        if block_nb < 2:\n",
    "            self.block_nb = 1\n",
    "        else:\n",
    "            self.block_nb = block_nb\n",
    "            \n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.indiv_id = indiv_id\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = dropout\n",
    "        self.filters_per_layers = filters_per_layers\n",
    "        \n",
    "        # Création d'un variable qui va garder la valeur de filters_per_layers (elle changera dans le modèle)\n",
    "        self.keep_filters_per_layers = filters_per_layers\n",
    "        \n",
    "        if filters_double < 2:\n",
    "            self.filters_double = 2\n",
    "        else : \n",
    "            self.filters_double = filters_double\n",
    "            \n",
    "        if filters_pool < 1:\n",
    "            self.filters_pool = 1\n",
    "        else : \n",
    "            self.filters_pool = filters_pool\n",
    "        \n",
    "        \n",
    "        self.activation = activation\n",
    "        self.kernel = kernel\n",
    "        self.first_kernel = first_kernel\n",
    "        self.padding = padding\n",
    "        self.max_or_avg_pool = max_or_avg_pool\n",
    "        self.learning_r = learning_r\n",
    "        self.momentum = momentum\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    # ToString()\n",
    "    def __str__(self):\n",
    "        ma_liste = []\n",
    "        ma_liste.append(\"indiv_id:{},\\n \".format(self.indiv_id))\n",
    "        ma_liste.append(\"epochs:{},\\n \".format(self.epochs))\n",
    "        ma_liste.append(\"batch_size:{},\\n \".format(self.batch_size))\n",
    "        ma_liste.append(\"block_size:{},\\n \".format(self.block_size))\n",
    "        ma_liste.append(\"block_nb:{},\\n \".format(self.block_nb))\n",
    "        ma_liste.append(\"l1:{},\\n \".format(self.l1))\n",
    "        ma_liste.append(\"l2:{},\\n \".format(self.l2))\n",
    "        ma_liste.append(\"batch_norm:{},\\n \".format(self.batch_norm))\n",
    "        ma_liste.append(\"dropout:{},\\n \".format(self.dropout))\n",
    "        ma_liste.append(\"filters_per_layers:{},\\n \".format(self.filters_per_layers))\n",
    "        ma_liste.append(\"filters_double:{},\\n \".format(self.filters_double))\n",
    "        ma_liste.append(\"filters_pool:{},\\n \".format(self.filters_pool))\n",
    "        ma_liste.append(\"activation:{},\\n \".format(self.activation))\n",
    "        ma_liste.append(\"kernel:\\n \")\n",
    "        ma_liste.append(\"{},\\n \".format(self.kernel))\n",
    "        ma_liste.append(\"first_kernel:\\n \")\n",
    "        ma_liste.append(\"{},\\n \".format(self.first_kernel))\n",
    "        ma_liste.append(\"padding:{},\\n \".format(self.padding))\n",
    "        ma_liste.append(\"max_or_avg_pool:{}\\n\".format(self.max_or_avg_pool))\n",
    "        ma_liste.append(\"learning_r:{}\\n\".format(self.learning_r))\n",
    "        ma_liste.append(\"momentum:{}\\n\".format(self.momentum))\n",
    "        ma_liste.append(\"optimizer:{}\\n\".format(self.optimizer))\n",
    "            \n",
    "        return ma_liste\n",
    "    \n",
    "    # (Modele 2 conv + norm ? + pool) * X -> MLP -> softmax sortie 10 -> MODELE BLOC 2\n",
    "    # D'autres modeles seront crees par la suite\n",
    "    def create_and_train_model(self, trainX, trainY, testX, testY, main_directory):\n",
    "        start = datetime.datetime.now()\n",
    "        \n",
    "        # Choix d'un emplacement pour les logs\n",
    "        log_dir=main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\tensorboard_data\\\\\"\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        print(\"log dir = \",log_dir)\n",
    "        \n",
    "        # l1 et l2\n",
    "        if self.l1 > 0 and self.l2 > 0:\n",
    "            self.my_regularizer = regularizers.l1_l2(l1=self.l1 / (self.block_nb*self.block_size), l2=self.l2 / (self.block_nb*self.block_size))\n",
    "        if self.l1 > 0:\n",
    "            self.my_regularizer = regularizers.l1(self.l1 / (self.block_nb*self.block_size))\n",
    "        elif self.l2 > 0:\n",
    "            self.my_regularizer = regularizers.l2(self.l2 / (self.block_nb*self.block_size))\n",
    "        else:\n",
    "            self.my_reguralizer = None\n",
    "            \n",
    "        \n",
    "\n",
    "        # Faire toutes les convs nécessaires (conv * 2 + max pool)\n",
    "        double_count = 0 # Var pour doubler les filtres\n",
    "        \n",
    "        input_layer = Input((32, 32, 3))\n",
    "        count = 1\n",
    "        count2 = 1\n",
    "\n",
    "        last_output = Conv2D(self.filters_per_layers, self.first_kernel, padding=self.padding, input_shape=(32, 32, 3), name='conv_'+str(self.filters_per_layers)+'_0',\n",
    "                      kernel_regularizer=self.my_reguralizer)(input_layer)\n",
    "        last_output = Activation(activation=self.activation, name=\"Activation_0\")(last_output)\n",
    "\n",
    "        if self.max_or_avg_pool == 'max':\n",
    "            last_output = MaxPooling2D((2, 2), padding=self.padding, name='max_pool_'+str(0))(last_output)\n",
    "        else:\n",
    "            last_output = AveragePooling2D((2, 2), padding=self.padding, name='avg_pool_'+str(0))(last_output)\n",
    "\n",
    "        for i in range(self.block_nb):\n",
    "\n",
    "            block_input = last_output\n",
    "            for j in range(self.block_size):\n",
    "                last_output = Conv2D(self.filters_per_layers, self.kernel, padding=self.padding, input_shape=(32, 32, 3), name='conv_'+str(self.filters_per_layers)+'_'+str(count),\n",
    "                          kernel_regularizer=self.my_reguralizer)(last_output)\n",
    "                last_output = Activation(activation=self.activation, name=f\"Activation_{count}\")(last_output)\n",
    "                if self.batch_norm == 1:\n",
    "                    last_output = BatchNormalization(name='batchnorm_'+str(count))(last_output)\n",
    "                count += 1\n",
    "                \n",
    "            \n",
    "\n",
    "            block_input = Conv2D(self.filters_per_layers, self.kernel, padding=self.padding, input_shape=(32, 32, 3), name='conv_identity'+str(self.filters_per_layers)+'_'+str(i)\n",
    "                          )(block_input)\n",
    "\n",
    "            last_output = Add(name=f\"Add_output_{i}\")([last_output, block_input])\n",
    "            if(i%self.filters_pool == 0):\n",
    "                if self.max_or_avg_pool == 'max':\n",
    "                    last_output = MaxPooling2D((2, 2), padding=self.padding, name='max_pool_'+str(count2))(last_output)\n",
    "                else:\n",
    "                    last_output = AveragePooling2D((2, 2), padding=self.padding, name='avg_pool_'+str(count2))(last_output)\n",
    "                count2 += 1\n",
    "\n",
    "            if(i % self.filters_double == 0):\n",
    "                self.filters_per_layers = self.filters_per_layers * 2\n",
    "            if self.dropout > 0:\n",
    "                last_output = Dropout(self.dropout)(last_output)\n",
    "            \n",
    "\n",
    "        flatten_layer_output = Flatten(name=\"flatten\")(last_output)  \n",
    "        \n",
    "        last_output = Dense(128, activation='relu', kernel_regularizer=self.my_reguralizer,\n",
    "                            name='mlp_end')(flatten_layer_output)\n",
    "        if self.batch_norm == 1:\n",
    "                    last_output = BatchNormalization(name='batchnorm_end')(last_output)\n",
    "        if self.dropout > 0:\n",
    "            last_output = Dropout(self.dropout)(last_output)         \n",
    "                  \n",
    "                  \n",
    "        \n",
    "        \n",
    "        output_tensor = Dense(10, activation=softmax, name=f\"Dense_output\")(last_output)\n",
    "\n",
    "        model = Model(input_layer, output_tensor)\n",
    "\n",
    "        # Compiler le modele\n",
    "        if self.optimizer == 'SGD':\n",
    "            print(\"SGD, learning_r = \", self.learning_r, \" momentum = \", self.momentum, \"\\n\")\n",
    "            opt = SGD(lr=self.learning_r, momentum=self.momentum)\n",
    "        else:\n",
    "            print(\"Adam learning_r = \", self.learning_r, \" momentum = \", self.momentum, \"\\n\")\n",
    "            opt = Adam(lr=self.learning_r, beta_1=self.momentum) # beta_1 => notation pour momentum Adam\n",
    "        model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Entrainer le modele\n",
    "        history = model.fit(trainX, trainY, epochs=self.epochs, batch_size=self.batch_size, \n",
    "                            validation_data=(testX, testY), verbose=1, callbacks=[tensorboard_callback])\n",
    "        \n",
    "        # Garder une trace du temps nécessaire pour fit (peut être pas la meilleure méthode)\n",
    "        end = datetime.datetime.now()\n",
    "        self.time_fit = end - start\n",
    "        print(\"\\nTime for fit = \", round(self.time_fit.total_seconds(),2)) # Round avec total_seconds()\n",
    "        \n",
    "        #Arpès que le fit soit fait, remettre filters_per_layers à sa valeur initiale pour un meilleur log \n",
    "        self.filters_per_layers = self.keep_filters_per_layers\n",
    "\n",
    "        return history, model\n",
    "    \n",
    "    \n",
    "    def save_model(self, history, model, main_directory, current_time):\n",
    "        \n",
    "        # Sauvegarde du modèle\n",
    "        plot_model(model, \"model.png\")\n",
    "        \n",
    "        # Deplacement modele au bon endroit\n",
    "        shutil.move(os.getcwd()+\"\\\\model.png\", main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\model.png\")\n",
    "        \n",
    "        # Afficher nos résultats dans un graphique matplotlib sauvegardé\n",
    "        pyplot.gcf().subplots_adjust(hspace = 0.5)\n",
    "\n",
    "        # Afficher la loss\n",
    "        pyplot.subplot(211)\n",
    "        pyplot.title('Cross Entropy Loss')\n",
    "        pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "        pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "        \n",
    "        # Afficher l'accuracy\n",
    "        pyplot.subplot(212)\n",
    "        pyplot.title('Classification Accuracy')\n",
    "        pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "        pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "        \n",
    "        # Sauvegarde\n",
    "        filename = main_directory+\"\\\\log_\"+self.indiv_id+\"\\\\\"\n",
    "        pyplot.savefig(filename + 'plot.png')\n",
    "        pyplot.close()\n",
    "       \n",
    "        \n",
    "        print(\"LOSS : \", round(history.history['loss'][self.epochs-1].item(), 3))\n",
    "        print(\"VAL_LOSS : \", round(history.history['val_loss'][self.epochs-1].item(), 3))\n",
    "        print(\"ACCURACY : \", round(history.history['accuracy'][self.epochs-1].item(), 3))\n",
    "        print(\"VAL_ACCURACY : \", round(history.history['val_accuracy'][self.epochs-1].item(), 3))\n",
    "        \n",
    "        # attributs pour créer les csv indivudels et le csv global\n",
    "        self.loss = round(history.history['loss'][self.epochs-1].item(), 3)\n",
    "        self.val_loss = round(history.history['val_loss'][self.epochs-1].item(), 3)\n",
    "        self.accuracy = round(history.history['accuracy'][self.epochs-1].item(), 3)\n",
    "        self.val_accuracy = round(history.history['val_accuracy'][self.epochs-1].item(), 3)\n",
    "        self.time_taken = round(self.time_fit.total_seconds(),2)\n",
    "        \n",
    "        # Créer un dataframe pandas (avec hyperparams) et le sauvegarder en CSV\n",
    "        df = pd.DataFrame({'indiv_id': [self.indiv_id],\n",
    "                           'epochs': [self.epochs],\n",
    "                           'batch_size': [self.batch_size],\n",
    "                           'block_size': [self.block_size],\n",
    "                           'block_nb': [self.block_nb],\n",
    "                           'l1': [self.l1],\n",
    "                           'l2': [self.l2],\n",
    "                           'batch_norm': [self.batch_norm],\n",
    "                           'dropout': [self.dropout],\n",
    "                           'filters_per_layers': [self.filters_per_layers],\n",
    "                           'filters_double': [self.filters_double],\n",
    "                           'filters_pool': [self.filters_pool],\n",
    "                           'activation': [self.activation],\n",
    "                           'kernel': [self.kernel],\n",
    "                           'first_kernel': [self.first_kernel],\n",
    "                           'padding': [self.padding],\n",
    "                           'max_or_avg_pool': [self.max_or_avg_pool],\n",
    "                           'loss': [self.loss],\n",
    "                           'val_loss': [self.val_loss],\n",
    "                           'accuracy': [self.accuracy],\n",
    "                           'val_accuracy': [self.val_accuracy],\n",
    "                           'time_taken' : [self.time_taken],\n",
    "                           'learning_r' : [self.learning_r],\n",
    "                           'momentum' : [self.momentum],\n",
    "                           'optimizer' : [self.optimizer]\n",
    "                          })\n",
    "        \n",
    "        df.to_csv(path_or_buf=filename+\"recap.csv\",index=False)\n",
    "    \n",
    "    # Lance toutes les étapes\n",
    "    def exec_indiv(self, main_directory, current_time):\n",
    "        \n",
    "        # Charger les données\n",
    "        trainX, trainY, testX, testY = load_dataset()\n",
    "        \n",
    "        # Normaliser les données\n",
    "        trainX, testX = prep_pixels(trainX, testX)\n",
    "        \n",
    "        # Créer et entrainer le modele\n",
    "        history, model = self.create_and_train_model(trainX, trainY, testX, testY, main_directory)\n",
    "        \n",
    "        # Sauvegarder le modèle\n",
    "        save = self.save_model(history, model, main_directory, current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buU7Dsqfs6Cv"
   },
   "source": [
    "### Classe Python qui va démarrer les tests des neural nets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Im542kkes6Cx"
   },
   "outputs": [],
   "source": [
    "# Classe générale qui va nous servir à effectuer des actions sur des individus\n",
    "class MyTraining:\n",
    "    # Prends un ID et une liste d'individus \n",
    "    def __init__(self, id_train, indiv_list):\n",
    "        \n",
    "        self.id_train = id_train\n",
    "        self.indiv_list = indiv_list\n",
    "    \n",
    "    def train(self, main_directory, current_time):\n",
    "        \n",
    "        print(\"Start training\\n\")\n",
    "        \n",
    "        for indiv in self.indiv_list:\n",
    "            print(\"indiv \", indiv.indiv_id, \"\\n\")\n",
    "            indiv.exec_indiv(main_directory, current_time)\n",
    "            print(\"-----------------------------------------------------------------\\n\")\n",
    "        \n",
    "        # Fusion des csv \n",
    "        merge_csv = pd.DataFrame(columns=['indiv_id', 'epochs', 'block_size', 'block_nb', 'l1', 'l2', 'batch_norm', 'dropout',\n",
    "                                          'filters_per_layers', 'filters_double', 'filters_pool', 'activation', 'kernel', \n",
    "                                          'first_kernel', 'padding','max_or_avg_pool','loss', 'val_loss', 'accuracy', 'val_accuracy',\n",
    "                                          'time_taken','learning_r', 'momentum', 'optimizer'])\n",
    "        \n",
    "        for indiv in self.indiv_list:\n",
    "            merge_csv = merge_csv.append({'indiv_id': indiv.indiv_id, 'epochs': indiv.epochs, 'batch_size': indiv.batch_size, 'block_size' : indiv.block_size,\n",
    "                                          'block_nb' : indiv.block_nb, 'l1' : indiv.l1, 'l2' : indiv.l2, 'batch_norm': indiv.batch_norm, 'dropout' : indiv.dropout,\n",
    "                                          'filters_per_layers' : indiv.filters_per_layers, 'filters_double' : indiv.filters_double, 'filters_pool' : indiv.filters_pool, \n",
    "                                          'activation' : indiv.activation, 'kernel' : indiv.kernel, 'first_kernel' : indiv.first_kernel, 'padding' : indiv.padding, \n",
    "                                          'max_or_avg_pool' : indiv.max_or_avg_pool, 'loss' : indiv.loss, 'val_loss' : indiv.val_loss, 'accuracy' : indiv.accuracy, \n",
    "                                          'val_accuracy' : indiv.val_accuracy, 'time_taken' : indiv.time_taken, 'learning_r' : indiv.learning_r,\n",
    "                                          'momentum': indiv.momentum, 'optimizer' : indiv.optimizer\n",
    "                                         }, ignore_index=True)\n",
    "        \n",
    "        # sauvegarde\n",
    "        merge_csv.to_csv(main_directory+\"\\\\combined_recap.csv\", index=False)\n",
    "            \n",
    "    \n",
    "    def all_indiv(self):\n",
    "        \n",
    "        # Affiche les caractéristiques de l'ensemble des individus\n",
    "        for indiv in self.indiv_list:\n",
    "            print('\\n'.join(indiv.__str__()))\n",
    "            for tir in range(80): print('-', end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIGBnz_4bj74"
   },
   "source": [
    "### Traitement général (train de l'ensemble des modèles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjIV_jF6RuwF",
    "outputId": "6759a6e1-3664-4264-c87d-18266651af09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indiv_id:1,\n",
      " \n",
      "epochs:50,\n",
      " \n",
      "batch_size:100,\n",
      " \n",
      "block_size:2,\n",
      " \n",
      "block_nb:4,\n",
      " \n",
      "l1:0.0001,\n",
      " \n",
      "l2:0.0001,\n",
      " \n",
      "batch_norm:1,\n",
      " \n",
      "dropout:0.4,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:2,\n",
      " \n",
      "filters_pool:1,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "first_kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:avg\n",
      "\n",
      "learning_r:0.001\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "indiv_id:2,\n",
      " \n",
      "epochs:50,\n",
      " \n",
      "batch_size:100,\n",
      " \n",
      "block_size:2,\n",
      " \n",
      "block_nb:4,\n",
      " \n",
      "l1:0.0001,\n",
      " \n",
      "l2:0.0001,\n",
      " \n",
      "batch_norm:1,\n",
      " \n",
      "dropout:0.4,\n",
      " \n",
      "filters_per_layers:64,\n",
      " \n",
      "filters_double:2,\n",
      " \n",
      "filters_pool:1,\n",
      " \n",
      "activation:relu,\n",
      " \n",
      "kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "first_kernel:\n",
      " \n",
      "(3, 3),\n",
      " \n",
      "padding:same,\n",
      " \n",
      "max_or_avg_pool:avg\n",
      "\n",
      "learning_r:0.001\n",
      "\n",
      "momentum:0.9\n",
      "\n",
      "optimizer:Adam\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Start training\n",
      "\n",
      "indiv  1 \n",
      "\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\logs\\resnetsConv2d\\logs_2020-02-09-12-13\\log_1\\tensorboard_data\\\n",
      "Adam learning_r =  0.001  momentum =  0.9 \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "  100/50000 [..............................] - ETA: 26:52 - loss: 3.4454 - accuracy: 0.0600WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.164072). Check your callbacks.\n",
      "50000/50000 [==============================] - 26s 521us/sample - loss: 1.8466 - accuracy: 0.3550 - val_loss: 4.7249 - val_accuracy: 0.1212\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 26s 511us/sample - loss: 1.3142 - accuracy: 0.5255 - val_loss: 1.2065 - val_accuracy: 0.5707\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 27s 537us/sample - loss: 1.0755 - accuracy: 0.6190 - val_loss: 1.2666 - val_accuracy: 0.5549\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 26s 529us/sample - loss: 0.9321 - accuracy: 0.6756 - val_loss: 0.8278 - val_accuracy: 0.7155\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 27s 540us/sample - loss: 0.8358 - accuracy: 0.7112 - val_loss: 0.9075 - val_accuracy: 0.6940\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 26s 525us/sample - loss: 0.7744 - accuracy: 0.7344 - val_loss: 0.7574 - val_accuracy: 0.7436\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 27s 539us/sample - loss: 0.7196 - accuracy: 0.7542 - val_loss: 1.0359 - val_accuracy: 0.6831\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 28s 552us/sample - loss: 0.6757 - accuracy: 0.7679 - val_loss: 0.7872 - val_accuracy: 0.7464\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 27s 545us/sample - loss: 0.6368 - accuracy: 0.7835 - val_loss: 0.7037 - val_accuracy: 0.7715\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 29s 573us/sample - loss: 0.6017 - accuracy: 0.7958 - val_loss: 0.8612 - val_accuracy: 0.7402\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 37s 733us/sample - loss: 0.5774 - accuracy: 0.8048 - val_loss: 0.7046 - val_accuracy: 0.7712\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 36s 719us/sample - loss: 0.5512 - accuracy: 0.8144 - val_loss: 0.7329 - val_accuracy: 0.7703\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 43s 869us/sample - loss: 0.5310 - accuracy: 0.8212 - val_loss: 0.6523 - val_accuracy: 0.7823\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 38s 768us/sample - loss: 0.5062 - accuracy: 0.8295 - val_loss: 0.7131 - val_accuracy: 0.7819\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 42s 831us/sample - loss: 0.4907 - accuracy: 0.8336 - val_loss: 0.6708 - val_accuracy: 0.7806\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 43s 859us/sample - loss: 0.4745 - accuracy: 0.8389 - val_loss: 0.5854 - val_accuracy: 0.8070\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 45s 906us/sample - loss: 0.4540 - accuracy: 0.8458 - val_loss: 0.5696 - val_accuracy: 0.8143\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 46s 920us/sample - loss: 0.4459 - accuracy: 0.8486 - val_loss: 0.6675 - val_accuracy: 0.7928\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4358 - accuracy: 0.8524 - val_loss: 0.7026 - val_accuracy: 0.7729\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 44s 883us/sample - loss: 0.4228 - accuracy: 0.8566 - val_loss: 0.5368 - val_accuracy: 0.8258\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 40s 804us/sample - loss: 0.4037 - accuracy: 0.8631 - val_loss: 0.6810 - val_accuracy: 0.7876\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 45s 898us/sample - loss: 0.3924 - accuracy: 0.8675 - val_loss: 0.5581 - val_accuracy: 0.8230\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 44s 882us/sample - loss: 0.3854 - accuracy: 0.8678 - val_loss: 0.6430 - val_accuracy: 0.8108\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 36s 718us/sample - loss: 0.3748 - accuracy: 0.8730 - val_loss: 0.5828 - val_accuracy: 0.8202\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 35s 699us/sample - loss: 0.3639 - accuracy: 0.8758 - val_loss: 0.5167 - val_accuracy: 0.8392\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 38s 755us/sample - loss: 0.3557 - accuracy: 0.8797 - val_loss: 0.5834 - val_accuracy: 0.8245\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 46s 915us/sample - loss: 0.3527 - accuracy: 0.8809 - val_loss: 0.5167 - val_accuracy: 0.8441\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 47s 933us/sample - loss: 0.3351 - accuracy: 0.8874 - val_loss: 0.5659 - val_accuracy: 0.8281\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 58s 1ms/sample - loss: 0.3282 - accuracy: 0.8887 - val_loss: 0.5143 - val_accuracy: 0.8449\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 60s 1ms/sample - loss: 0.3225 - accuracy: 0.8898 - val_loss: 0.5647 - val_accuracy: 0.8259\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 60s 1ms/sample - loss: 0.3244 - accuracy: 0.8904 - val_loss: 0.5699 - val_accuracy: 0.8199\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 47s 934us/sample - loss: 0.3118 - accuracy: 0.8946 - val_loss: 0.5391 - val_accuracy: 0.8345\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 47s 936us/sample - loss: 0.3095 - accuracy: 0.8953 - val_loss: 0.5681 - val_accuracy: 0.8301\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 36s 730us/sample - loss: 0.2959 - accuracy: 0.8984 - val_loss: 0.5046 - val_accuracy: 0.8457\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 35s 698us/sample - loss: 0.2994 - accuracy: 0.8974 - val_loss: 0.5515 - val_accuracy: 0.8348\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 38s 767us/sample - loss: 0.2857 - accuracy: 0.9030 - val_loss: 0.6497 - val_accuracy: 0.8143\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 53s 1ms/sample - loss: 0.2858 - accuracy: 0.9027 - val_loss: 0.6077 - val_accuracy: 0.8266\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 43s 857us/sample - loss: 0.2777 - accuracy: 0.9055 - val_loss: 0.5747 - val_accuracy: 0.8344\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 35s 705us/sample - loss: 0.2695 - accuracy: 0.9084 - val_loss: 0.4840 - val_accuracy: 0.8598\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 43s 867us/sample - loss: 0.2685 - accuracy: 0.9084 - val_loss: 0.5244 - val_accuracy: 0.8472\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 70s 1ms/sample - loss: 0.2574 - accuracy: 0.9118 - val_loss: 0.5523 - val_accuracy: 0.8494\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 70s 1ms/sample - loss: 0.2525 - accuracy: 0.9147 - val_loss: 0.6166 - val_accuracy: 0.8191\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 45s 906us/sample - loss: 0.2514 - accuracy: 0.9146 - val_loss: 0.5376 - val_accuracy: 0.8497\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 61s 1ms/sample - loss: 0.2467 - accuracy: 0.9155 - val_loss: 0.5642 - val_accuracy: 0.8378\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 33s 664us/sample - loss: 0.2450 - accuracy: 0.9176 - val_loss: 0.5434 - val_accuracy: 0.8485\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 30s 596us/sample - loss: 0.2377 - accuracy: 0.9192 - val_loss: 0.7211 - val_accuracy: 0.8115\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 31s 624us/sample - loss: 0.2359 - accuracy: 0.9199 - val_loss: 0.5874 - val_accuracy: 0.8399\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 35s 699us/sample - loss: 0.2344 - accuracy: 0.9204 - val_loss: 0.5012 - val_accuracy: 0.8588\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 35s 707us/sample - loss: 0.2287 - accuracy: 0.9214 - val_loss: 0.5500 - val_accuracy: 0.8407\n",
      "\n",
      "Time for fit =  2049.33\n",
      "LOSS :  0.229\n",
      "VAL_LOSS :  0.55\n",
      "ACCURACY :  0.921\n",
      "VAL_ACCURACY :  0.841\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "indiv  2 \n",
      "\n",
      "log dir =  C:\\Users\\arnau\\Desktop\\quatrième_année\\Deep_Learning\\Projet_cifar-10\\logs\\resnetsConv2d\\logs_2020-02-09-12-13\\log_2\\tensorboard_data\\\n",
      "Adam learning_r =  0.001  momentum =  0.9 \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "  100/50000 [..............................] - ETA: 38:39 - loss: 3.4609 - accuracy: 0.1500WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.216922). Check your callbacks.\n",
      "50000/50000 [==============================] - 34s 675us/sample - loss: 1.7790 - accuracy: 0.3792 - val_loss: 5.4744 - val_accuracy: 0.1626\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 31s 626us/sample - loss: 1.2472 - accuracy: 0.5530 - val_loss: 1.2999 - val_accuracy: 0.5462\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 33s 653us/sample - loss: 1.0419 - accuracy: 0.6315 - val_loss: 1.1632 - val_accuracy: 0.6192\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 32s 636us/sample - loss: 0.9161 - accuracy: 0.6797 - val_loss: 0.9072 - val_accuracy: 0.7003\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 29s 587us/sample - loss: 0.8374 - accuracy: 0.7108 - val_loss: 0.9073 - val_accuracy: 0.6967\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 31s 620us/sample - loss: 0.7708 - accuracy: 0.7331 - val_loss: 0.8314 - val_accuracy: 0.7249\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 31s 618us/sample - loss: 0.7232 - accuracy: 0.7532 - val_loss: 0.7982 - val_accuracy: 0.7359\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 30s 606us/sample - loss: 0.6744 - accuracy: 0.7690 - val_loss: 0.7462 - val_accuracy: 0.7514\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 29s 582us/sample - loss: 0.6454 - accuracy: 0.7799 - val_loss: 0.6295 - val_accuracy: 0.7893\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 32s 646us/sample - loss: 0.6159 - accuracy: 0.7935 - val_loss: 0.8607 - val_accuracy: 0.7355\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 31s 613us/sample - loss: 0.5822 - accuracy: 0.8010 - val_loss: 0.6130 - val_accuracy: 0.7961\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 29s 588us/sample - loss: 0.5580 - accuracy: 0.8130 - val_loss: 0.6915 - val_accuracy: 0.7753\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 30s 607us/sample - loss: 0.5375 - accuracy: 0.8186 - val_loss: 0.6253 - val_accuracy: 0.7890\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 32s 636us/sample - loss: 0.5141 - accuracy: 0.8257 - val_loss: 0.6410 - val_accuracy: 0.7916\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 30s 595us/sample - loss: 0.4900 - accuracy: 0.8375 - val_loss: 0.5825 - val_accuracy: 0.8141\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 29s 585us/sample - loss: 0.4722 - accuracy: 0.8408 - val_loss: 0.5559 - val_accuracy: 0.8209\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 32s 634us/sample - loss: 0.4585 - accuracy: 0.8444 - val_loss: 0.5624 - val_accuracy: 0.8211\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 31s 614us/sample - loss: 0.4487 - accuracy: 0.8469 - val_loss: 0.5473 - val_accuracy: 0.8271\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 31s 621us/sample - loss: 0.4307 - accuracy: 0.8561 - val_loss: 0.6746 - val_accuracy: 0.7997\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 31s 612us/sample - loss: 0.4205 - accuracy: 0.8574 - val_loss: 0.5577 - val_accuracy: 0.8240\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 31s 625us/sample - loss: 0.4098 - accuracy: 0.8625 - val_loss: 0.5762 - val_accuracy: 0.8217\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 29s 584us/sample - loss: 0.3956 - accuracy: 0.8668 - val_loss: 0.6556 - val_accuracy: 0.7993\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 31s 615us/sample - loss: 0.3858 - accuracy: 0.8689 - val_loss: 0.5433 - val_accuracy: 0.8254- loss: 0.384\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 31s 611us/sample - loss: 0.3797 - accuracy: 0.8716 - val_loss: 0.5226 - val_accuracy: 0.8376\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 30s 594us/sample - loss: 0.3605 - accuracy: 0.8783 - val_loss: 0.5909 - val_accuracy: 0.8150\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 29s 586us/sample - loss: 0.3530 - accuracy: 0.8805 - val_loss: 0.6134 - val_accuracy: 0.8162\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 31s 627us/sample - loss: 0.3478 - accuracy: 0.8821 - val_loss: 0.5906 - val_accuracy: 0.8173\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 31s 622us/sample - loss: 0.3403 - accuracy: 0.8847 - val_loss: 0.6658 - val_accuracy: 0.8055\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 29s 589us/sample - loss: 0.3287 - accuracy: 0.8879 - val_loss: 0.5609 - val_accuracy: 0.8338 \n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 31s 611us/sample - loss: 0.3264 - accuracy: 0.8897 - val_loss: 0.5643 - val_accuracy: 0.8337\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 32s 642us/sample - loss: 0.3131 - accuracy: 0.8934 - val_loss: 0.6208 - val_accuracy: 0.8155\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 31s 614us/sample - loss: 0.3114 - accuracy: 0.8945 - val_loss: 0.5842 - val_accuracy: 0.8274\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 32s 645us/sample - loss: 0.3004 - accuracy: 0.8985 - val_loss: 0.6195 - val_accuracy: 0.8184\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 32s 637us/sample - loss: 0.2957 - accuracy: 0.9006 - val_loss: 0.5421 - val_accuracy: 0.8373\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 30s 597us/sample - loss: 0.2981 - accuracy: 0.8987 - val_loss: 0.5476 - val_accuracy: 0.8418\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 31s 612us/sample - loss: 0.2884 - accuracy: 0.9024 - val_loss: 0.6103 - val_accuracy: 0.8266\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 32s 638us/sample - loss: 0.2741 - accuracy: 0.9071 - val_loss: 0.5313 - val_accuracy: 0.8467\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 30s 597us/sample - loss: 0.2779 - accuracy: 0.9033 - val_loss: 0.5035 - val_accuracy: 0.8498\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 30s 607us/sample - loss: 0.2746 - accuracy: 0.9073 - val_loss: 0.6683 - val_accuracy: 0.8156\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 31s 626us/sample - loss: 0.2712 - accuracy: 0.9058 - val_loss: 0.4762 - val_accuracy: 0.8553\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 30s 605us/sample - loss: 0.2641 - accuracy: 0.9106 - val_loss: 0.5378 - val_accuracy: 0.8447\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 32s 633us/sample - loss: 0.2593 - accuracy: 0.9114 - val_loss: 0.5874 - val_accuracy: 0.8361\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 32s 640us/sample - loss: 0.2610 - accuracy: 0.9117 - val_loss: 0.5806 - val_accuracy: 0.8317\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 31s 623us/sample - loss: 0.2469 - accuracy: 0.9174 - val_loss: 0.6238 - val_accuracy: 0.8299\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 30s 593us/sample - loss: 0.2408 - accuracy: 0.9172 - val_loss: 0.5952 - val_accuracy: 0.8269\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 33s 665us/sample - loss: 0.2441 - accuracy: 0.9167 - val_loss: 0.5153 - val_accuracy: 0.8501\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 32s 633us/sample - loss: 0.2410 - accuracy: 0.9193 - val_loss: 0.5041 - val_accuracy: 0.8535\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 29s 584us/sample - loss: 0.2316 - accuracy: 0.9208 - val_loss: 0.6255 - val_accuracy: 0.8272\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 31s 622us/sample - loss: 0.2323 - accuracy: 0.9216 - val_loss: 0.5419 - val_accuracy: 0.8461\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 31s 620us/sample - loss: 0.2287 - accuracy: 0.9222 - val_loss: 0.5155 - val_accuracy: 0.8543\n",
      "\n",
      "Time for fit =  1548.19\n",
      "LOSS :  0.229\n",
      "VAL_LOSS :  0.515\n",
      "ACCURACY :  0.922\n",
      "VAL_ACCURACY :  0.854\n",
      "-----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Création des individus (des neurals nets, ici convnet)\n",
    "list_indiv = []\n",
    "for num in range(len(list_indiv_id)):\n",
    "    list_indiv.append(IndividuResnetConv2d(\n",
    "        list_indiv_id[num],\n",
    "          list_epochs[num],\n",
    "          list_batch_size[num],\n",
    "          list_blocks_size[num],\n",
    "          list_blocks_nb[num],\n",
    "          list_l1[num],\n",
    "          list_l2[num],\n",
    "          list_batch_norm[num],\n",
    "          list_dropout[num],\n",
    "          list_filters_per_layers[num],\n",
    "          list_filters_double[num],\n",
    "          list_filters_pool[num],\n",
    "          list_activation[num],\n",
    "          list_kernel[num],\n",
    "          list_first_kernel[num],\n",
    "          list_padding[num],\n",
    "          list_max_or_avg_pool[num],\n",
    "          list_learning_r[num],\n",
    "          list_momentum[num],\n",
    "          list_optimizer[num]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Chargement de la classe training, affichag\n",
    "training_1 = MyTraining(1, list_indiv)\n",
    "training_1.all_indiv()\n",
    "training_1.train(main_directory, current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxpbXOw1RuwM"
   },
   "source": [
    "### Partie tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jm6vpk3MRuwQ"
   },
   "outputs": [],
   "source": [
    "# Procedure pour utiliser tensorboard\n",
    "#  1 load la première cell\n",
    "#  2 utiliser la derniere cell avec --logdir (précisez bien votre répertoire, plus sur que ça\n",
    "#    fonctionne avec une string \"mon_path\"\n",
    "#  3 Vous NE POURREZ PLUS update tensorboard sur ce port et il y aura des bugs, pour éviter ça\n",
    "#    quand vous voulez faire une update, fermez jupyter notebook (shutdown total) et réouvrez le \n",
    "#    OU, faites kernel->interrupt et changez de port + de folder de log\n",
    "\n",
    "#si vous voulez tenter de tuer des process\n",
    "#os.system(\"taskkill /im tensorboard.exe /f\") #kill tous les processus qui utilisent tensorboard\n",
    "#os.system('!kill 18776') #kill le processus X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Lj738x3Ruwa",
    "outputId": "b7c6a334-5e14-4d18-fc6e-7427bf231326"
   },
   "outputs": [],
   "source": [
    "# Liste des ports utilisés par tensorboard, attention ça se remplit vite et il faut kill jupyter pour clean\n",
    "from tensorboard import notebook\n",
    "notebook.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seEJieCtRuwq",
    "outputId": "051fb9d1-b28c-4221-c61c-a8dfbb158362",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code pour démarrer tensorboard dans le dossier souhaité [PRECISEZ BIEN LE DOSSIER ICI]\n",
    "%tensorboard --logdir \"CONVNETS_20200119-0243\\logs_20200119-093909\\tensorboard_data\" --port=6066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upoC4cmYRuwz",
    "outputId": "c0390d77-555a-49f0-934c-b2740b5e2ba0"
   },
   "outputs": [],
   "source": [
    "# Si vous avez la folie des grandeurs\n",
    "notebook.display(port=6066, height=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLuEIeDvRuw7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Fichier CSV combined_recap + Graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TO0o33QYs6FP",
    "outputId": "0ebfe25a-2965-4220-ccf7-914ae220328f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Commandes pandas utiles\n",
    "data_csv = pd.read_csv(main_directory + \"\\\\combined_recap.csv\")\n",
    "#data_csv = pd.read_csv(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\CONVNETS_20200119-2043\\\\combined_recap.csv\")\n",
    "data.head()\n",
    "#meilleure accuracy, moins pire loss par ex\n",
    "#data_csv.sort_values([\"elapsed\"], axis=0, \n",
    "                 #ascending=[False], inplace=True) \n",
    "\n",
    "# Afficher uniquement certaines colonnes\n",
    "#dataX = data_csv.filter(items=['elapsed', 'label'])\n",
    "\n",
    "#récupérer uniquement où la loss est < à X et ou kernel = (3,3) par exemple\n",
    "#dataX = data_csv.loc[(data_csv['elapsed'] > 700) & (data_csv['threadName'].str.contains('Thread Group 1-2'))]\n",
    "#dataX\n",
    "\n",
    "#pd.set_option('display.max_rows', data3.shape[0]+1) #nombre de row max à afficher\n",
    "#data_csv = pd.read_csv(main_directory+\"\\\\logs_20200116-204456\\\\recap.csv\")\n",
    "#data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGXNwkMCRuxE"
   },
   "outputs": [],
   "source": [
    "image = pyplot.imread(main_directory + \"\\\\logs_20200119-093909\\\\plot.png\")\n",
    "#image = pyplot.imread(\"C:\\\\Users\\\\arnau\\\\Desktop\\\\quatrième_année\\\\Deep_Learning\\\\Projet_cifar-10\\\\logs_20200119-093909\\\\plot.png\")\n",
    "pyplot.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "colab": {
   "collapsed_sections": [],
   "name": "test_several_models_1701_modif.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
