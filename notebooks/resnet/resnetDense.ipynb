{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7R0yO1XdBNh"
   },
   "source": [
    "### Démarrage de tensorboard et imports principaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rCCbrXXRuvi",
    "outputId": "7d91c098-39d8-4a89-adf8-3133ab8217de"
   },
   "outputs": [],
   "source": [
    "# Agrandir le notebook ?\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Démarrage de tensorboard pour notebook\n",
    "%load_ext tensorboard\n",
    "\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.activations import *\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import shutil  \n",
    "from math import ceil, floor\n",
    "\n",
    "from tensorflow.python.framework import ops #pour tenter de reset tensorboard, sans grand succès\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametres de verification : ==> adjust\n",
    "\n",
    "list_indiv_id = ['1', '2']\n",
    "list_epochs = [10, 10]\n",
    "list_depth = [6, 2]\n",
    "list_l1 = [0.001, 0]\n",
    "list_l2 = [0, 0.001]\n",
    "list_dropout = [0, 0.2]\n",
    "list_filters_per_layers = [1024, 1024]\n",
    "list_activations = [\"relu\", \"relu\"]\n",
    "\n",
    "# Chemin dossier courant\n",
    "LOGS_DIRECTORY = os.getcwd() + \"\\\\logs\\\\resnets\\\\logs_\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\") #dossier de la session\n",
    "print(LOGS_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIIDw6pHdJbK"
   },
   "source": [
    "### Fonctions pour préparer le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tSU6Hl2Ruv0"
   },
   "outputs": [],
   "source": [
    "# Chargement des datasets de train et de validation + one hot encoding\n",
    "def load_dataset():\n",
    "    # Chargement des données cifar10\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode encoding sur les labels\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)  \n",
    "    \n",
    "    Xtrain = np.reshape(np.asarray([np.mean(im, axis=2, keepdims=True) for im in trainX]),(50000,32,32))\n",
    "    Xtest = np.reshape(np.asarray([np.mean(im, axis=2, keepdims=True) for im in testX]),(10000,32,32))\n",
    "        \n",
    "    \n",
    "    return Xtrain, trainY, Xtest, testY\n",
    "\n",
    "# Normalisation pour accroître la vitesse du modèle (en redimensionnant les pixels)\n",
    "def prep_pixels(train, test):\n",
    "    # Convertion des int en float\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # Normalisation pour avoir des nombres entre 0 et 1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # Retourner les images normalisées\n",
    "    return train_norm, test_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTdXP2D-c2D5"
   },
   "source": [
    "### Classe Python pour définir les individus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jP0CrTpRuv8"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-2-676fea37dcf5>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-676fea37dcf5>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    def __init__(self, indiv_id='1', epochs=10, depth=34, l1=0, l2=0, dropout=0, filters_per_layers=64, activation):\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "# Classe pour les convnets\n",
    "class IndividuConvnets:\n",
    "    def __init__(self, indiv_id='1', epochs=10, depth=34, l1=0, l2=0, dropout=0, filters_per_layers=64, activation):\n",
    "        # Initialisation de nos variables\n",
    "        self.time_fit = datetime.datetime.now()\n",
    "        self.my_reguralizer = None\n",
    "\n",
    "        if depth <= 0:\n",
    "            self.depth = 1\n",
    "        else:\n",
    "            self.depth = depth\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.indiv_id = indiv_id\n",
    "        self.epochs = epochs\n",
    "        self.activation = activation\n",
    "\n",
    "        # On peut mettre l1 et l2 en même temps mais pour l'instant on le gère pas\n",
    "        if (l1 != 0 and l2 != 0):\n",
    "            self.l1 = 0\n",
    "            self.l2 = 0\n",
    "        else:\n",
    "            self.l1 = l1\n",
    "            self.l2 = l2\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.filters_per_layers = filters_per_layers\n",
    "    \n",
    "    # ToString()\n",
    "    def __str__(self):  # ToString\n",
    "        ma_liste = []\n",
    "        ma_liste.append(\"indiv_id:{},\\n \".format(self.indiv_id))\n",
    "        ma_liste.append(\"epochs:{},\\n \".format(self.epochs))\n",
    "        ma_liste.append(\"nb_layers:{},\\n \".format(self.depth))\n",
    "        ma_liste.append(\"l1:{},\\n \".format(self.l1))\n",
    "        ma_liste.append(\"l2:{},\\n \".format(self.l2))\n",
    "        ma_liste.append(\"dropout:{},\\n \".format(self.dropout))\n",
    "        ma_liste.append(\"filters_per_layers:{},\\n \".format(self.filters_per_layers))\n",
    "        ma_liste.append(\"activation:{},\\n \".format(self.activation))\n",
    "        \n",
    "        return ma_liste\n",
    "    \n",
    "   \n",
    "    def create_and_train_model(self, trainX, trainY, testX, testY, use_skip_connections: bool = True):     \n",
    "        start = datetime.datetime.now()\n",
    "\n",
    "        # Update indiv_id pour avoir un vrai ID unique\n",
    "        #self.indiv_id = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        # Choix d'un emplacement pour les logs\n",
    "        log_dir = LOGS_DIRECTORY + \"\\\\logs_\" + self.indiv_id + \"\\\\tensorboard_data\\\\\"\n",
    "        \n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        \n",
    "        print(\"log dir = \", log_dir)\n",
    "        \n",
    "        #A VERIFIER!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        #A VERIFIER!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        #A VERIFIER!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        if self.l1 > 0 and self.l2 > 0:\n",
    "            self.my_regularizer = regularizers.l1_l2(l1=self.l1 / self.nb_layers, l2=self.l2 / self.nb_layers)\n",
    "        if self.l1 > 0:\n",
    "            self.my_regularizer = regularizers.l1(self.l1 / self.nb_layers)\n",
    "        elif self.l2 > 0:\n",
    "            self.my_regularizer = regularizers.l2(self.l2 / self.nb_layers)\n",
    "        else:\n",
    "            self.my_reguralizer = None\n",
    "           \n",
    "        \n",
    "        input_layer = Input((32, 32))\n",
    "        flatten_layer_output = Flatten(name=\"flatten\")(input_layer)\n",
    "\n",
    "        penultimate_output = None\n",
    "        last_output = flatten_layer_output        \n",
    "        \n",
    "        for i in range(self.depth):\n",
    "            \n",
    "            if penultimate_output is not None and use_skip_connections:\n",
    "                add_output = Add(name=f\"Add_{i}\")([last_output, penultimate_output])\n",
    "                penultimate_output = add_output\n",
    "                last_output = Dense(self.filters_per_layers, activation=linear, name=f\"Dense_{i}\", kernel_regularizer=self.my_reguralizer)(add_output)\n",
    "                last_output = Activation(activation=self.activation, name=f\"Activation_{i}\")(last_output)\n",
    "            else:\n",
    "                penultimate_output = last_output\n",
    "                last_output = Dense(self.filters_per_layers, activation=linear, name=f\"Dense_{i}\", kernel_regularizer=self.my_reguralizer)(last_output)\n",
    "                last_output = Activation(activation=self.activation, name=f\"Activation_{i}\")(last_output)\n",
    "            if(self.dropout > 0)\n",
    "\n",
    "        if use_skip_connections:\n",
    "            last_output = Add(name=f\"Add_output\")([last_output, penultimate_output])\n",
    "        \n",
    "        output_tensor = Dense(10, activation=softmax, name=f\"Dense_output\")(last_output)\n",
    "        model = tf.keras.Model(input_layer, output_tensor)\n",
    "\n",
    "        # Compiler le modèle\n",
    "        model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "              \n",
    "        # Entrainer le modele\n",
    "        history = model.fit(trainX, trainY, epochs=self.epochs, batch_size=64, validation_data=(testX, testY), verbose=1, callbacks=[tensorboard_callback])\n",
    "       \n",
    "        end = datetime.datetime.now()\n",
    "        self.time_fit = end - start\n",
    "        print(\"Time for fit = \", self.time_fit)\n",
    "\n",
    "        return history, model\n",
    "    \n",
    "    \n",
    "    def save_model(self, history, model):\n",
    "        \n",
    "        # Sauvegarde du modèle\n",
    "        plot_model(model, \"model.png\")\n",
    "        \n",
    "         # Deplacement modele au bon endroit\n",
    "        main_dir = os.getcwd() + \"\\\\model.png\"\n",
    "        dest = LOGS_DIRECTORY + \"\\\\logs_\" + self.indiv_id + \"\\\\model.png\"\n",
    "        shutil.move(main_dir, dest)\n",
    "        \n",
    "        # Afficher nos résultats dans un graphique matplotlib sauvegardé\n",
    "        pyplot.gcf().subplots_adjust(hspace = 0.5)\n",
    "\n",
    "        # Afficher la loss\n",
    "        pyplot.subplot(211)\n",
    "        pyplot.title('Cross Entropy Loss')\n",
    "        pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "        pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "        \n",
    "        # Afficher l'accuracy\n",
    "        pyplot.subplot(212)\n",
    "        pyplot.title('Classification Accuracy')\n",
    "        pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "        pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "        \n",
    "        # Sauvegarde\n",
    "        filename = LOGS_DIRECTORY + \"\\\\logs_\" + self.indiv_id + \"\\\\\"\n",
    "        pyplot.savefig( filename + 'plot.png')\n",
    "        pyplot.close()\n",
    "       \n",
    "        \n",
    "        print(\"LOSS : \", round(history.history['loss'][self.epochs].item(), 3))\n",
    "        print(\"VAL_LOSS : \", round(history.history['val_loss'][self.epochs].item(), 3))\n",
    "        print(\"ACCURACY : \", round(history.history['accuracy'][self.epochs].item(), 3))\n",
    "        print(\"VAL_ACCURACY : \", round(history.history['val_accuracy'][self.epochs].item(), 3))\n",
    "        \n",
    "        # attributs pour créer les csv indivudels et le csv global\n",
    "        self.loss = round(history.history['loss'][0].item(), 3)\n",
    "        self.val_loss = round(history.history['val_loss'][0].item(), 3)\n",
    "        self.accuracy = round(history.history['accuracy'][0].item(), 3)\n",
    "        self.val_accuracy = round(history.history['val_accuracy'][0].item(), 3)\n",
    "        self.time_taken = round(self.time_fit.total_seconds(),2)\n",
    "        \n",
    "        # Créer un dataframe pandas (avec hyperparams) et le sauvegarder en CSV\n",
    "        df = pd.DataFrame({'indiv_id': [self.indiv_id],\n",
    "                           'epochs': [self.epochs],\n",
    "                           'depth': [self.depth],\n",
    "                           'l1': [self.l1],\n",
    "                           'l2': [self.l2],\n",
    "                           'dropout': [self.dropout],\n",
    "                           'filters_per_layers': [self.filters_per_layers],\n",
    "                           'activation': [self.activation],\n",
    "                           'loss': [self.loss],\n",
    "                           'val_loss': [self.val_loss],\n",
    "                           'accuracy': [self.accuracy],\n",
    "                           'val_accuracy': [self.val_accuracy],\n",
    "                           'time_taken' : [self.time_taken]\n",
    "                          })\n",
    "        \n",
    "        df.to_csv(path_or_buf=filename+\"recap.csv\",index=False)\n",
    "    \n",
    "    # Lance toutes les étapes\n",
    "    def exec_indiv(self, trainX, testX, trainY, testY):    \n",
    "        # Créer et entrainer le modele    \n",
    "        \n",
    "        history, model = self.create_and_train_model(trainX, trainY, testX, testY)\n",
    "        \n",
    "        # Sauvegarder le modèle\n",
    "        save = self.save_model(history, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullPath = os.getcwd() + \"\\\\model.png\", LOGS_DIRECTORY + \"\\\\model.png\"\n",
    "print(fullPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buU7Dsqfs6Cv"
   },
   "source": [
    "### Classe Python qui va démarrer les tests des neural nets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Im542kkes6Cx"
   },
   "outputs": [],
   "source": [
    "# Classe générale qui va nous servir à effectuer des actions sur des individus\n",
    "class MyTraining:\n",
    "    # Prends un ID et une liste d'individus \n",
    "    def __init__(self, id_train, indiv_list):\n",
    "        \n",
    "        self.id_train = id_train\n",
    "        self.indiv_list = indiv_list\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        # Charger les données\n",
    "        trainX, trainY, testX, testY = load_dataset()\n",
    "        \n",
    "        # Normaliser les données\n",
    "        trainX, testX = prep_pixels(trainX, testX)\n",
    "        \n",
    "        print(\"Start training\\n\")\n",
    "        \n",
    "        for indiv in self.indiv_list:\n",
    "            print(\"indiv \", indiv.indiv_id, \"\\n\")\n",
    "            indiv.exec_indiv(trainX, testX, trainY, testY)\n",
    "            print(\"-----------------------------------------------------------------\\n\")\n",
    "        # self, indiv_id='1', epochs=10, depth=34, l1=0, l2=0, dropout=0, filters_per_layers=64\n",
    "        # Fusion des csv  \n",
    "        merge_csv = pd.DataFrame(columns=['indiv_id', 'epochs', 'depth', 'l1', 'l2', 'dropout', 'filters_per_layers','activation'\n",
    "                                          'loss', 'val_loss', 'accuracy', 'val_accuracy', 'time_taken'])\n",
    "        for indiv in self.indiv_list:\n",
    "            merge_csv = merge_csv.append({'indiv_id': indiv.indiv_id, 'epochs': indiv.epochs,'depth' : indiv.depth,'l1' : indiv.l1,\n",
    "                                          'l2' : indiv.l2, 'dropout' : indiv.dropout,'filters_per_layers' : indiv.filters_per_layers,\n",
    "                                          'activation' : self.activation, 'loss' : indiv.loss,'val_loss' : indiv.val_loss, \n",
    "                                          'accuracy' : indiv.accuracy, 'val_accuracy' : indiv.val_accuracy,'time_taken' : indiv.time_taken},\n",
    "                                         ignore_index=True)\n",
    "        \n",
    "        # sauvegarde\n",
    "        merge_csv.to_csv(LOGS_DIRECTORY + \"\\\\combined_recap.csv\", index=False)\n",
    "            \n",
    "    \n",
    "    def all_indiv(self):\n",
    "        \n",
    "        # Affiche les caractéristiques de l'ensemble des individus\n",
    "        for indiv in self.indiv_list:\n",
    "            print('\\n'.join(indiv.__str__()))\n",
    "            for tir in range(80): print('-', end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AF9xVI2bdzuT"
   },
   "source": [
    "### Hyper paramètres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Premier Test : ==> adjust\n",
    "# CONSTANTES : nb_layers = 8, batch_size = 50, epochs 100, lr = 0.01, momentum = 0.9, optimizer Adam, \n",
    "#padding = same, maxpool, relu, kernel = (3,3)\n",
    "# \n",
    "# * 3 convnets sans regularization et MLP à 128\n",
    "#   - 2 filters double(2) avec filters (16, 32) \n",
    "#   - 1 sans filters double avec filters (32)\n",
    "\n",
    "# 2 convnets sans regu ou on test le MLP_end et filters 64\n",
    "#  * 1 sans double, filter 64 avec MLP_end(128)\n",
    "#  * 1 sans double, filter 64 avec MLP_end(0)\n",
    "\n",
    "# 5 convnets avec regu (+ MLP à 128) et filters 32 sans double\n",
    "#  * 1 convnet avec l1 à 0.01\n",
    "#  * 1 convnet avec l1 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec l2 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec L1 et L2 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec L1 et L2 à 0.01 + batchnorm + dropout à 0.2\n",
    "\n",
    "# LEXIQUE PARAM : \n",
    "# * filters_double permet de savoir toutes les combien de couche on double les filtres, si 0 on double pas\n",
    "\n",
    "#list_indiv_id = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "#list_epochs = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
    "#list_depth = [8,8,8,8,8,8,8,8,8,8]\n",
    "#list_l1 = [0, 0, 0, 0, 0, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "#list_l2 = [0, 0, 0, 0, 0, 0, 0, 0.01, 0.01, 0.01]\n",
    "#list_dropout = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2]\n",
    "#list_filters_per_layers = [16, 32, 32, 64, 64, 32, 32, 32, 32, 32]\n",
    "\n",
    "#main_directory =(\"D:\\\\Projets\\\\TestsDL\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIGBnz_4bj74"
   },
   "source": [
    "### Traitement général (train de l'ensemble des modèles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjIV_jF6RuwF",
    "outputId": "6759a6e1-3664-4264-c87d-18266651af09"
   },
   "outputs": [],
   "source": [
    "# Création des individus (des neurals nets, ici convnet) ==> adjust\n",
    "list_indiv = []\n",
    "for num in range(len(list_indiv_id)):\n",
    "    list_indiv.append(IndividuConvnets(\n",
    "        list_indiv_id[num],\n",
    "          list_epochs[num],\n",
    "          list_depth[num],\n",
    "          list_l1[num],\n",
    "          list_l2[num],\n",
    "          list_dropout[num],\n",
    "          list_filters_per_layers[num],\n",
    "          list_activations[num]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Chargement de la classe training, affichage des individus et train de tous les convnets\n",
    "training_1 = MyTraining(1, list_indiv)\n",
    "training_1.all_indiv()\n",
    "training_1.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxpbXOw1RuwM"
   },
   "source": [
    "### Partie tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jm6vpk3MRuwQ"
   },
   "outputs": [],
   "source": [
    "# Procedure pour utiliser tensorboard\n",
    "#  1 load la première cell\n",
    "#  2 utiliser la derniere cell avec --logdir (précisez bien votre répertoire, plus sur que ça\n",
    "#    fonctionne avec une string \"mon_path\"\n",
    "#  3 Vous NE POURREZ PLUS update tensorboard sur ce port et il y aura des bugs, pour éviter ça\n",
    "#    quand vous voulez faire une update, fermez jupyter notebook (shutdown total) et réouvrez le \n",
    "#    OU, faites kernel->interrupt et changez de port + de folder de log\n",
    "\n",
    "#si vous voulez tenter de tuer des process\n",
    "#os.system(\"taskkill /im tensorboard.exe /f\") #kill tous les processus qui utilisent tensorboard\n",
    "#os.system('!kill 18776') #kill le processus X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Lj738x3Ruwa",
    "outputId": "b7c6a334-5e14-4d18-fc6e-7427bf231326"
   },
   "outputs": [],
   "source": [
    "# Liste des ports utilisés par tensorboard, attention ça se remplit vite et il faut kill jupyter pour clean\n",
    "from tensorboard import notebook\n",
    "notebook.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seEJieCtRuwq",
    "outputId": "051fb9d1-b28c-4221-c61c-a8dfbb158362"
   },
   "outputs": [],
   "source": [
    "# Code pour démarrer tensorboard dans le dossier souhaité [PRECISEZ BIEN LE DOSSIER ICI]\n",
    "%tensorboard --logdir \"CONVNETS_20200116-2354\\\\logs_20200116-235500\\\\tensorboard_data\" --port=6152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upoC4cmYRuwz",
    "outputId": "c0390d77-555a-49f0-934c-b2740b5e2ba0"
   },
   "outputs": [],
   "source": [
    "# Si vous avez la folie des grandeurs\n",
    "notebook.display(port=6066, height=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLuEIeDvRuw7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Fichier CSV combined_recap + Graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TO0o33QYs6FP",
    "outputId": "0ebfe25a-2965-4220-ccf7-914ae220328f"
   },
   "outputs": [],
   "source": [
    "# Commandes pandas utiles\n",
    "\n",
    "#data_csv = pd.read_csv(main_directory+\"\\\\combined_recap.csv\")\n",
    "\n",
    "#meilleure accuracy, moins pire loss par ex\n",
    "#data_csv.sort_values([\"elapsed\"], axis=0, \n",
    "                 #ascending=[False], inplace=True) \n",
    "\n",
    "# Afficher uniquement certaines colonnes\n",
    "#dataX = data_csv.filter(items=['elapsed', 'label'])\n",
    "\n",
    "#récupérer uniquement où la loss est < à X et ou kernel = (3,3) par exemple\n",
    "#dataX = data_csv.loc[(data_csv['elapsed'] > 700) & (data_csv['threadName'].str.contains('Thread Group 1-2'))]\n",
    "#dataX\n",
    "\n",
    "#pd.set_option('display.max_rows', data3.shape[0]+1) #nombre de row max à afficher\n",
    "#data_csv = pd.read_csv(main_directory+\"\\\\\n",
    "_20200116-204456\\\\recap.csv\")\n",
    "#data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGXNwkMCRuxE"
   },
   "outputs": [],
   "source": [
    "image = pyplot.imread(\"CONVNETS_20200113-1951\\\\logs1\\\\indiv1_plot.png\")\n",
    "pyplot.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "colab": {
   "collapsed_sections": [],
   "name": "test_several_models_1701_modif.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
