{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_7R0yO1XdBNh"
   },
   "source": [
    "### Démarrage de tensorboard et imports principaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rCCbrXXRuvi",
    "outputId": "7d91c098-39d8-4a89-adf8-3133ab8217de"
   },
   "outputs": [],
   "source": [
    "# Agrandir le notebook ?\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Démarrage de tensorboard pour notebook\n",
    "%load_ext tensorboard\n",
    "\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.activations import *\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import shutil  \n",
    "from math import ceil, floor\n",
    "\n",
    "from tensorflow.python.framework import ops #pour tenter de reset tensorboard, sans grand succès\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Projets\\TestsDL\\logs\\resnets\\logs_2020-01-28_18-43\n"
     ]
    }
   ],
   "source": [
    "# Parametres de verification : ==> adjust\n",
    "\n",
    "list_indiv_id = ['1', '2']\n",
    "list_epochs = [10, 10]\n",
    "list_depth = [6, 2]\n",
    "list_l1 = [0.001, 0]\n",
    "list_l2 = [0, 0.001]\n",
    "list_dropout = [0, 0.2]\n",
    "list_filters_per_layers = [1024, 1024]\n",
    "list_activations = [\"relu\", \"relu\"]\n",
    "\n",
    "# Chemin dossier courant\n",
    "LOGS_DIRECTORY = os.getcwd() + \"\\\\logs\\\\resnets\\\\logs_\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\") #dossier de la session\n",
    "print(LOGS_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIIDw6pHdJbK"
   },
   "source": [
    "### Fonctions pour préparer le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tSU6Hl2Ruv0"
   },
   "outputs": [],
   "source": [
    "# Chargement des datasets de train et de validation + one hot encoding\n",
    "def load_dataset():\n",
    "    # Chargement des données cifar10\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode encoding sur les labels\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)  \n",
    "    \n",
    "    Xtrain = np.reshape(np.asarray([np.mean(im, axis=2, keepdims=True) for im in trainX]),(50000,32,32))\n",
    "    Xtest = np.reshape(np.asarray([np.mean(im, axis=2, keepdims=True) for im in testX]),(10000,32,32))\n",
    "        \n",
    "    \n",
    "    return Xtrain, trainY, Xtest, testY\n",
    "\n",
    "# Normalisation pour accroître la vitesse du modèle (en redimensionnant les pixels)\n",
    "def prep_pixels(train, test):\n",
    "    # Convertion des int en float\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # Normalisation pour avoir des nombres entre 0 et 1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # Retourner les images normalisées\n",
    "    return train_norm, test_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTdXP2D-c2D5"
   },
   "source": [
    "### Classe Python pour définir les individus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jP0CrTpRuv8"
   },
   "outputs": [],
   "source": [
    "# Classe pour les convnets\n",
    "class IndividuConvnets:\n",
    "    def __init__(self, indiv_id='1', epochs=10, depth=34, l1=0, l2=0, dropout=0, filters_per_layers=64, activation):\n",
    "        # Initialisation de nos variables\n",
    "        self.time_fit = datetime.datetime.now()\n",
    "        self.my_reguralizer = None\n",
    "\n",
    "        if depth <= 0:\n",
    "            self.depth = 1\n",
    "        else:\n",
    "            self.depth = depth\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.indiv_id = indiv_id\n",
    "        self.epochs = epochs\n",
    "        self.activation = activation\n",
    "\n",
    "        # On peut mettre l1 et l2 en même temps mais pour l'instant on le gère pas\n",
    "        if (l1 != 0 and l2 != 0):\n",
    "            self.l1 = 0\n",
    "            self.l2 = 0\n",
    "        else:\n",
    "            self.l1 = l1\n",
    "            self.l2 = l2\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.filters_per_layers = filters_per_layers\n",
    "    \n",
    "    # ToString()\n",
    "    def __str__(self):  # ToString\n",
    "        ma_liste = []\n",
    "        ma_liste.append(\"indiv_id:{},\\n \".format(self.indiv_id))\n",
    "        ma_liste.append(\"epochs:{},\\n \".format(self.epochs))\n",
    "        ma_liste.append(\"nb_layers:{},\\n \".format(self.depth))\n",
    "        ma_liste.append(\"l1:{},\\n \".format(self.l1))\n",
    "        ma_liste.append(\"l2:{},\\n \".format(self.l2))\n",
    "        ma_liste.append(\"dropout:{},\\n \".format(self.dropout))\n",
    "        ma_liste.append(\"filters_per_layers:{},\\n \".format(self.filters_per_layers))\n",
    "        ma_liste.append(\"activation:{},\\n \".format(self.activation))\n",
    "        \n",
    "        return ma_liste\n",
    "    \n",
    "   \n",
    "    def create_and_train_model(self, trainX, trainY, testX, testY, use_skip_connections: bool = True):     \n",
    "        start = datetime.datetime.now()\n",
    "\n",
    "        # Update indiv_id pour avoir un vrai ID unique\n",
    "        #self.indiv_id = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        # Choix d'un emplacement pour les logs\n",
    "        log_dir = LOGS_DIRECTORY + \"\\\\logs_\" + self.indiv_id + \"\\\\tensorboard_data\\\\\"\n",
    "        \n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        \n",
    "        print(\"log dir = \", log_dir)\n",
    "        \n",
    "        # l1 et l2 en même temps dans le modèle pas encore géré\n",
    "        if self.l1 > 0:\n",
    "            self.my_regularizer = regularizers.l1(self.l1 / self.depth)\n",
    "        elif self.l2 > 0:\n",
    "            self.my_regularizer = regularizers.l2(self.l2 / self.depth)\n",
    "           \n",
    "        \n",
    "        input_layer = Input((32, 32))\n",
    "        flatten_layer_output = Flatten(name=\"flatten\")(input_layer)\n",
    "\n",
    "        penultimate_output = None\n",
    "        last_output = flatten_layer_output        \n",
    "        \n",
    "        for i in range(self.depth):\n",
    "            \n",
    "            if penultimate_output is not None and use_skip_connections:\n",
    "                add_output = Add(name=f\"Add_{i}\")([last_output, penultimate_output])\n",
    "                penultimate_output = add_output\n",
    "                last_output = Dense(self.filters_per_layers, activation=linear, name=f\"Dense_{i}\", kernel_regularizer=self.my_reguralizer)(add_output)\n",
    "                last_output = Activation(activation=self.activation, name=f\"Activation_{i}\")(last_output)\n",
    "            else:\n",
    "                penultimate_output = last_output\n",
    "                last_output = Dense(self.filters_per_layers, activation=linear, name=f\"Dense_{i}\", kernel_regularizer=self.my_reguralizer)(last_output)\n",
    "                last_output = Activation(activation=self.activation, name=f\"Activation_{i}\")(last_output)\n",
    "\n",
    "        if use_skip_connections:\n",
    "            last_output = Add(name=f\"Add_output\")([last_output, penultimate_output])\n",
    "        \n",
    "        output_tensor = Dense(10, activation=softmax, name=f\"Dense_output\")(last_output)\n",
    "        model = tf.keras.Model(input_layer, output_tensor)\n",
    "\n",
    "        # Compiler le modèle\n",
    "        model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "              \n",
    "        # Entrainer le modele\n",
    "        history = model.fit(trainX, trainY, epochs=self.epochs, batch_size=64, validation_data=(testX, testY), verbose=1, callbacks=[tensorboard_callback])\n",
    "       \n",
    "        end = datetime.datetime.now()\n",
    "        self.time_fit = end - start\n",
    "        print(\"Time for fit = \", self.time_fit)\n",
    "\n",
    "        return history, model\n",
    "    \n",
    "    \n",
    "    def save_model(self, history, model):\n",
    "        \n",
    "        # Sauvegarde du modèle\n",
    "        plot_model(model, \"model.png\")\n",
    "        \n",
    "         # Deplacement modele au bon endroit\n",
    "        main_dir = os.getcwd() + \"\\\\model.png\"\n",
    "        dest = LOGS_DIRECTORY + \"\\\\logs_\" + self.indiv_id + \"\\\\model.png\"\n",
    "        shutil.move(main_dir, dest)\n",
    "        \n",
    "        # Afficher nos résultats dans un graphique matplotlib sauvegardé\n",
    "        pyplot.gcf().subplots_adjust(hspace = 0.5)\n",
    "\n",
    "        # Afficher la loss\n",
    "        pyplot.subplot(211)\n",
    "        pyplot.title('Cross Entropy Loss')\n",
    "        pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "        pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "        \n",
    "        # Afficher l'accuracy\n",
    "        pyplot.subplot(212)\n",
    "        pyplot.title('Classification Accuracy')\n",
    "        pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "        pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "        \n",
    "        # Sauvegarde\n",
    "        filename = LOGS_DIRECTORY + \"\\\\logs_\" + self.indiv_id + \"\\\\\"\n",
    "        pyplot.savefig( filename + 'plot.png')\n",
    "        pyplot.close()\n",
    "       \n",
    "        \n",
    "        print(\"LOSS : \", round(history.history['loss'][0].item(), 3))\n",
    "        print(\"VAL_LOSS : \", round(history.history['val_loss'][0].item(), 3))\n",
    "        print(\"ACCURACY : \", round(history.history['accuracy'][0].item(), 3))\n",
    "        print(\"VAL_ACCURACY : \", round(history.history['val_accuracy'][0].item(), 3))\n",
    "        \n",
    "        # attributs pour créer les csv indivudels et le csv global\n",
    "        self.loss = round(history.history['loss'][0].item(), 3)\n",
    "        self.val_loss = round(history.history['val_loss'][0].item(), 3)\n",
    "        self.accuracy = round(history.history['accuracy'][0].item(), 3)\n",
    "        self.val_accuracy = round(history.history['val_accuracy'][0].item(), 3)\n",
    "        self.time_taken = round(self.time_fit.total_seconds(),2)\n",
    "        \n",
    "        # Créer un dataframe pandas (avec hyperparams) et le sauvegarder en CSV\n",
    "        df = pd.DataFrame({'indiv_id': [self.indiv_id],\n",
    "                           'epochs': [self.epochs],\n",
    "                           'depth': [self.depth],\n",
    "                           'l1': [self.l1],\n",
    "                           'l2': [self.l2],\n",
    "                           'dropout': [self.dropout],\n",
    "                           'filters_per_layers': [self.filters_per_layers],\n",
    "                           'activation': [self.activation],\n",
    "                           'loss': [self.loss],\n",
    "                           'val_loss': [self.val_loss],\n",
    "                           'accuracy': [self.accuracy],\n",
    "                           'val_accuracy': [self.val_accuracy],\n",
    "                           'time_taken' : [self.time_taken]\n",
    "                          })\n",
    "        \n",
    "        df.to_csv(path_or_buf=filename+\"recap.csv\",index=False)\n",
    "    \n",
    "    # Lance toutes les étapes\n",
    "    def exec_indiv(self, trainX, testX, trainY, testY):    \n",
    "        # Créer et entrainer le modele    \n",
    "        \n",
    "        history, model = self.create_and_train_model(trainX, trainY, testX, testY)\n",
    "        \n",
    "        # Sauvegarder le modèle\n",
    "        save = self.save_model(history, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullPath = os.getcwd() + \"\\\\model.png\", LOGS_DIRECTORY + \"\\\\model.png\"\n",
    "print(fullPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buU7Dsqfs6Cv"
   },
   "source": [
    "### Classe Python qui va démarrer les tests des neural nets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Im542kkes6Cx"
   },
   "outputs": [],
   "source": [
    "# Classe générale qui va nous servir à effectuer des actions sur des individus\n",
    "class MyTraining:\n",
    "    # Prends un ID et une liste d'individus \n",
    "    def __init__(self, id_train, indiv_list):\n",
    "        \n",
    "        self.id_train = id_train\n",
    "        self.indiv_list = indiv_list\n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        # Charger les données\n",
    "        trainX, trainY, testX, testY = load_dataset()\n",
    "        \n",
    "        # Normaliser les données\n",
    "        trainX, testX = prep_pixels(trainX, testX)\n",
    "        \n",
    "        print(\"Start training\\n\")\n",
    "        \n",
    "        for indiv in self.indiv_list:\n",
    "            print(\"indiv \", indiv.indiv_id, \"\\n\")\n",
    "            indiv.exec_indiv(trainX, testX, trainY, testY)\n",
    "            print(\"-----------------------------------------------------------------\\n\")\n",
    "        # self, indiv_id='1', epochs=10, depth=34, l1=0, l2=0, dropout=0, filters_per_layers=64\n",
    "        # Fusion des csv  \n",
    "        merge_csv = pd.DataFrame(columns=['indiv_id', 'epochs', 'depth', 'l1', 'l2', 'dropout', 'filters_per_layers','activation'\n",
    "                                          'loss', 'val_loss', 'accuracy', 'val_accuracy', 'time_taken'])\n",
    "        for indiv in self.indiv_list:\n",
    "            merge_csv = merge_csv.append({'indiv_id': indiv.indiv_id, 'epochs': indiv.epochs,'depth' : indiv.depth,'l1' : indiv.l1,\n",
    "                                          'l2' : indiv.l2, 'dropout' : indiv.dropout,'filters_per_layers' : indiv.filters_per_layers,\n",
    "                                          'activation' : self.activation, 'loss' : indiv.loss,'val_loss' : indiv.val_loss, \n",
    "                                          'accuracy' : indiv.accuracy, 'val_accuracy' : indiv.val_accuracy,'time_taken' : indiv.time_taken},\n",
    "                                         ignore_index=True)\n",
    "        \n",
    "        # sauvegarde\n",
    "        merge_csv.to_csv(LOGS_DIRECTORY + \"\\\\combined_recap.csv\", index=False)\n",
    "            \n",
    "    \n",
    "    def all_indiv(self):\n",
    "        \n",
    "        # Affiche les caractéristiques de l'ensemble des individus\n",
    "        for indiv in self.indiv_list:\n",
    "            print('\\n'.join(indiv.__str__()))\n",
    "            for tir in range(80): print('-', end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AF9xVI2bdzuT"
   },
   "source": [
    "### Hyper paramètres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Premier Test : ==> adjust\n",
    "# CONSTANTES : nb_layers = 8, batch_size = 50, epochs 100, lr = 0.01, momentum = 0.9, optimizer Adam, \n",
    "#padding = same, maxpool, relu, kernel = (3,3)\n",
    "# \n",
    "# * 3 convnets sans regularization et MLP à 128\n",
    "#   - 2 filters double(2) avec filters (16, 32) \n",
    "#   - 1 sans filters double avec filters (32)\n",
    "\n",
    "# 2 convnets sans regu ou on test le MLP_end et filters 64\n",
    "#  * 1 sans double, filter 64 avec MLP_end(128)\n",
    "#  * 1 sans double, filter 64 avec MLP_end(0)\n",
    "\n",
    "# 5 convnets avec regu (+ MLP à 128) et filters 32 sans double\n",
    "#  * 1 convnet avec l1 à 0.01\n",
    "#  * 1 convnet avec l1 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec l2 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec L1 et L2 à 0.01 et batchnorm\n",
    "#  * 1 convnet avec L1 et L2 à 0.01 + batchnorm + dropout à 0.2\n",
    "\n",
    "# LEXIQUE PARAM : \n",
    "# * filters_double permet de savoir toutes les combien de couche on double les filtres, si 0 on double pas\n",
    "\n",
    "#list_indiv_id = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "#list_epochs = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
    "#list_depth = [8,8,8,8,8,8,8,8,8,8]\n",
    "#list_l1 = [0, 0, 0, 0, 0, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "#list_l2 = [0, 0, 0, 0, 0, 0, 0, 0.01, 0.01, 0.01]\n",
    "#list_dropout = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2]\n",
    "#list_filters_per_layers = [16, 32, 32, 64, 64, 32, 32, 32, 32, 32]\n",
    "\n",
    "#main_directory =(\"D:\\\\Projets\\\\TestsDL\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIGBnz_4bj74"
   },
   "source": [
    "### Traitement général (train de l'ensemble des modèles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bjIV_jF6RuwF",
    "outputId": "6759a6e1-3664-4264-c87d-18266651af09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indiv_id:1,\n",
      " \n",
      "epochs:10,\n",
      " \n",
      "nb_layers:6,\n",
      " \n",
      "l1:0.001,\n",
      " \n",
      "l2:0,\n",
      " \n",
      "dropout:0,\n",
      " \n",
      "filters_per_layers:1024,\n",
      " \n",
      "--------------------------------------------------------------------------------\n",
      "indiv_id:2,\n",
      " \n",
      "epochs:10,\n",
      " \n",
      "nb_layers:2,\n",
      " \n",
      "l1:0,\n",
      " \n",
      "l2:0.001,\n",
      " \n",
      "dropout:0.2,\n",
      " \n",
      "filters_per_layers:1024,\n",
      " \n",
      "--------------------------------------------------------------------------------\n",
      "Start training\n",
      "\n",
      "indiv  1 \n",
      "\n",
      "log dir =  D:\\Projets\\TestsDL\\logs\\resnets\\logs_2020-01-28_18-43\\logs_1\\tensorboard_data\\\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 11s 217us/sample - loss: 2.7133 - accuracy: 0.2424 - val_loss: 1.9533 - val_accuracy: 0.2897\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 9s 189us/sample - loss: 1.9309 - accuracy: 0.3085 - val_loss: 1.9191 - val_accuracy: 0.3257\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 8s 158us/sample - loss: 1.8641 - accuracy: 0.3345 - val_loss: 1.8227 - val_accuracy: 0.3451\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 7s 139us/sample - loss: 1.8175 - accuracy: 0.3537 - val_loss: 1.8305 - val_accuracy: 0.3524\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 7s 145us/sample - loss: 1.7708 - accuracy: 0.3708 - val_loss: 1.7906 - val_accuracy: 0.3607\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 7s 147us/sample - loss: 1.7315 - accuracy: 0.3831 - val_loss: 1.7598 - val_accuracy: 0.3646\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 9s 172us/sample - loss: 1.7051 - accuracy: 0.3970 - val_loss: 1.7424 - val_accuracy: 0.3803\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 7s 141us/sample - loss: 1.6706 - accuracy: 0.4074 - val_loss: 1.6914 - val_accuracy: 0.3931\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 10s 191us/sample - loss: 1.6516 - accuracy: 0.4132 - val_loss: 1.7218 - val_accuracy: 0.3839\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 8s 157us/sample - loss: 1.6242 - accuracy: 0.4238 - val_loss: 1.7069 - val_accuracy: 0.3918\n",
      "Time for fit =  0:01:23.306322\n",
      "LOSS :  2.713\n",
      "VAL_LOSS :  1.953\n",
      "ACCURACY :  0.242\n",
      "VAL_ACCURACY :  0.29\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "indiv  2 \n",
      "\n",
      "log dir =  D:\\Projets\\TestsDL\\logs\\resnets\\logs_2020-01-28_18-43\\logs_2\\tensorboard_data\\\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "   64/50000 [..............................] - ETA: 6:57 - loss: 4.2513 - accuracy: 0.1719WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.434245). Check your callbacks.\n",
      "50000/50000 [==============================] - 10s 193us/sample - loss: 2.1600 - accuracy: 0.2482 - val_loss: 1.9916 - val_accuracy: 0.2858\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 9s 189us/sample - loss: 1.9338 - accuracy: 0.3089 - val_loss: 1.8794 - val_accuracy: 0.3294\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 10s 207us/sample - loss: 1.8695 - accuracy: 0.3349 - val_loss: 1.9527 - val_accuracy: 0.3067\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 11s 230us/sample - loss: 1.8212 - accuracy: 0.3527 - val_loss: 1.7841 - val_accuracy: 0.3680\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 13s 270us/sample - loss: 1.7765 - accuracy: 0.3690 - val_loss: 1.7814 - val_accuracy: 0.3674\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 14s 278us/sample - loss: 1.7433 - accuracy: 0.3834 - val_loss: 1.7470 - val_accuracy: 0.3821\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 12s 245us/sample - loss: 1.7071 - accuracy: 0.3950 - val_loss: 1.7477 - val_accuracy: 0.3866\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 11s 227us/sample - loss: 1.6857 - accuracy: 0.4044 - val_loss: 1.7364 - val_accuracy: 0.3847\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 10s 202us/sample - loss: 1.6630 - accuracy: 0.4113 - val_loss: 1.7022 - val_accuracy: 0.4050\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 12s 230us/sample - loss: 1.6404 - accuracy: 0.4180 - val_loss: 1.7545 - val_accuracy: 0.3751\n",
      "Time for fit =  0:01:54.973929\n",
      "LOSS :  2.16\n",
      "VAL_LOSS :  1.992\n",
      "ACCURACY :  0.248\n",
      "VAL_ACCURACY :  0.286\n",
      "-----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Création des individus (des neurals nets, ici convnet) ==> adjust\n",
    "list_indiv = []\n",
    "for num in range(len(list_indiv_id)):\n",
    "    list_indiv.append(IndividuConvnets(\n",
    "        list_indiv_id[num],\n",
    "          list_epochs[num],\n",
    "          list_depth[num],\n",
    "          list_l1[num],\n",
    "          list_l2[num],\n",
    "          list_dropout[num],\n",
    "          list_filters_per_layers[num],\n",
    "          list_activations[num]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Chargement de la classe training, affichage des individus et train de tous les convnets\n",
    "training_1 = MyTraining(1, list_indiv)\n",
    "training_1.all_indiv()\n",
    "training_1.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxpbXOw1RuwM"
   },
   "source": [
    "### Partie tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jm6vpk3MRuwQ"
   },
   "outputs": [],
   "source": [
    "# Procedure pour utiliser tensorboard\n",
    "#  1 load la première cell\n",
    "#  2 utiliser la derniere cell avec --logdir (précisez bien votre répertoire, plus sur que ça\n",
    "#    fonctionne avec une string \"mon_path\"\n",
    "#  3 Vous NE POURREZ PLUS update tensorboard sur ce port et il y aura des bugs, pour éviter ça\n",
    "#    quand vous voulez faire une update, fermez jupyter notebook (shutdown total) et réouvrez le \n",
    "#    OU, faites kernel->interrupt et changez de port + de folder de log\n",
    "\n",
    "#si vous voulez tenter de tuer des process\n",
    "#os.system(\"taskkill /im tensorboard.exe /f\") #kill tous les processus qui utilisent tensorboard\n",
    "#os.system('!kill 18776') #kill le processus X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Lj738x3Ruwa",
    "outputId": "b7c6a334-5e14-4d18-fc6e-7427bf231326"
   },
   "outputs": [],
   "source": [
    "# Liste des ports utilisés par tensorboard, attention ça se remplit vite et il faut kill jupyter pour clean\n",
    "from tensorboard import notebook\n",
    "notebook.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seEJieCtRuwq",
    "outputId": "051fb9d1-b28c-4221-c61c-a8dfbb158362"
   },
   "outputs": [],
   "source": [
    "# Code pour démarrer tensorboard dans le dossier souhaité [PRECISEZ BIEN LE DOSSIER ICI]\n",
    "%tensorboard --logdir \"CONVNETS_20200116-2354\\\\logs_20200116-235500\\\\tensorboard_data\" --port=6152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upoC4cmYRuwz",
    "outputId": "c0390d77-555a-49f0-934c-b2740b5e2ba0"
   },
   "outputs": [],
   "source": [
    "# Si vous avez la folie des grandeurs\n",
    "notebook.display(port=6066, height=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLuEIeDvRuw7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Fichier CSV combined_recap + Graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TO0o33QYs6FP",
    "outputId": "0ebfe25a-2965-4220-ccf7-914ae220328f"
   },
   "outputs": [],
   "source": [
    "# Commandes pandas utiles\n",
    "\n",
    "#data_csv = pd.read_csv(main_directory+\"\\\\combined_recap.csv\")\n",
    "\n",
    "#meilleure accuracy, moins pire loss par ex\n",
    "#data_csv.sort_values([\"elapsed\"], axis=0, \n",
    "                 #ascending=[False], inplace=True) \n",
    "\n",
    "# Afficher uniquement certaines colonnes\n",
    "#dataX = data_csv.filter(items=['elapsed', 'label'])\n",
    "\n",
    "#récupérer uniquement où la loss est < à X et ou kernel = (3,3) par exemple\n",
    "#dataX = data_csv.loc[(data_csv['elapsed'] > 700) & (data_csv['threadName'].str.contains('Thread Group 1-2'))]\n",
    "#dataX\n",
    "\n",
    "#pd.set_option('display.max_rows', data3.shape[0]+1) #nombre de row max à afficher\n",
    "#data_csv = pd.read_csv(main_directory+\"\\\\\n",
    "_20200116-204456\\\\recap.csv\")\n",
    "#data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGXNwkMCRuxE"
   },
   "outputs": [],
   "source": [
    "image = pyplot.imread(\"CONVNETS_20200113-1951\\\\logs1\\\\indiv1_plot.png\")\n",
    "pyplot.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "def show_first_samples(x_train, y_train):\n",
    "    plt.imshow(x_train[0])\n",
    "    print(y_train[0])\n",
    "    plt.show()\n",
    "    plt.imshow(x_train[1])\n",
    "    print(y_train[1])\n",
    "    plt.show()\n",
    "    plt.imshow(x_train[2])\n",
    "    print(y_train[2])\n",
    "    plt.show()\n",
    "    plt.imshow(x_train[3])\n",
    "    print(y_train[3])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class PlotRealLossAndAccuracyOnTrainCallback(Callback):\n",
    "\n",
    "    def __init__(self, x_train, y_train):\n",
    "        super().__init__()\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        (loss, acc) = self.model.evaluate(self.x_train, self.y_train, batch_size = 8192)\n",
    "        print(f\"Real Loss on train : {loss}\")\n",
    "        print(f\"Real Acc on train : {acc}\")\n",
    "\n",
    "\n",
    "def create_model(depth: int = 2, use_skip_connections: bool = True):\n",
    "    input_layer = Input((28, 28))\n",
    "    flatten_layer_output = Flatten(name=\"flatten\")(input_layer)\n",
    "\n",
    "    penultimate_output = None\n",
    "    last_output = flatten_layer_output\n",
    "\n",
    "    for i in range(depth):\n",
    "        if penultimate_output is not None and use_skip_connections:\n",
    "            add_output = Add(name=f\"Add_{i}\")([last_output, penultimate_output])\n",
    "            penultimate_output = add_output\n",
    "            last_output = Dense(784, activation=linear, name=f\"Dense_{i}\", kernel_regularizer=L1L2(l2=0.001 / depth))(\n",
    "                add_output)\n",
    "            # last_output = BatchNormalization()(last_output)\n",
    "            last_output = Activation(activation=relu, name=f\"Activation_{i}\")(last_output)\n",
    "        else:\n",
    "            penultimate_output = last_output\n",
    "            last_output = Dense(784, activation=linear, name=f\"Dense_{i}\", kernel_regularizer=L1L2(l2=0.001 / depth))(\n",
    "                last_output)\n",
    "            # last_output = BatchNormalization()(last_output)\n",
    "            last_output = Activation(activation=relu, name=f\"Activation_{i}\")(last_output)\n",
    "\n",
    "    if use_skip_connections:\n",
    "        last_output = Add(name=f\"Add_output\")([last_output, penultimate_output])\n",
    "\n",
    "    output_tensor = Dense(10, activation=softmax, name=f\"Dense_output\", kernel_regularizer=L1L2(l2=0.001 / depth))(\n",
    "        last_output)\n",
    "    model = Model(input_layer, output_tensor)\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    (x_train, y_train), (x_val, y_val) = cifar10.load_data()\n",
    "    x_train = x_train / 255.0\n",
    "    x_val = x_val / 255.0\n",
    "    #show_first_samples(x_train, y_train)\n",
    "    model = create_model(use_skip_connections=False)\n",
    "    print(model.summary())\n",
    "    plot_model(model, \"residual_dense.png\")\n",
    "    model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "              epochs=100,\n",
    "              batch_size=8192,\n",
    "              callbacks=[PlotRealLossAndAccuracyOnTrainCallback(x_train, y_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "colab": {
   "collapsed_sections": [],
   "name": "test_several_models_1701_modif.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
